{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 22618,
     "status": "ok",
     "timestamp": 1628269473144,
     "user": {
      "displayName": "Nir Weingarten",
      "photoUrl": "",
      "userId": "08827275300476553234"
     },
     "user_tz": -180
    },
    "id": "tXKby-KyjGS_",
    "outputId": "ea70a224-d6a5-490a-9008-657b96a5c0a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.18.3)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.4)\n",
      "Requirement already satisfied: Pillow<=8.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (8.0.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.19.4)\n",
      "Requirement already satisfied: pyglet<=1.5.15,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.15)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.2.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  freeglut3 libbsd0 libdrm-amdgpu1 libdrm-common libdrm-intel1 libdrm-nouveau2\n",
      "  libdrm-radeon1 libdrm2 libedit2 libelf1 libgl1 libgl1-mesa-dri\n",
      "  libgl1-mesa-glx libglapi-mesa libglu1-mesa libglvnd0 libglx-mesa0 libglx0\n",
      "  libllvm10 libpciaccess0 libpython-stdlib libpython2.7-minimal\n",
      "  libpython2.7-stdlib libsensors4 libx11-6 libx11-data libx11-xcb1 libxau6\n",
      "  libxcb-dri2-0 libxcb-dri3-0 libxcb-glx0 libxcb-present0 libxcb-sync1 libxcb1\n",
      "  libxdamage1 libxdmcp6 libxext6 libxfixes3 libxi6 libxshmfence1 libxxf86vm1\n",
      "  python python-minimal python2.7 python2.7-minimal\n",
      "Suggested packages:\n",
      "  pciutils lm-sensors python-doc python-tk python-numpy libgle3 python2.7-doc\n",
      "  binfmt-support\n",
      "The following NEW packages will be installed:\n",
      "  freeglut3 libbsd0 libdrm-amdgpu1 libdrm-common libdrm-intel1 libdrm-nouveau2\n",
      "  libdrm-radeon1 libdrm2 libedit2 libelf1 libgl1 libgl1-mesa-dri\n",
      "  libgl1-mesa-glx libglapi-mesa libglu1-mesa libglvnd0 libglx-mesa0 libglx0\n",
      "  libllvm10 libpciaccess0 libpython-stdlib libpython2.7-minimal\n",
      "  libpython2.7-stdlib libsensors4 libx11-6 libx11-data libx11-xcb1 libxau6\n",
      "  libxcb-dri2-0 libxcb-dri3-0 libxcb-glx0 libxcb-present0 libxcb-sync1 libxcb1\n",
      "  libxdamage1 libxdmcp6 libxext6 libxfixes3 libxi6 libxshmfence1 libxxf86vm1\n",
      "  python python-minimal python-opengl python2.7 python2.7-minimal\n",
      "0 upgraded, 46 newly installed, 0 to remove and 13 not upgraded.\n",
      "Need to get 4484 kB/31.0 MB of archives.\n",
      "After this operation, 354 MB of additional disk space will be used.\n",
      "Ign:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libx11-data all 2:1.6.4-3ubuntu0.3\n",
      "Ign:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libx11-6 amd64 2:1.6.4-3ubuntu0.3\n",
      "Ign:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpython2.7-minimal amd64 2.7.17-1~18.04ubuntu1.2\n",
      "Ign:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python2.7-minimal amd64 2.7.17-1~18.04ubuntu1.2\n",
      "Ign:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpython2.7-stdlib amd64 2.7.17-1~18.04ubuntu1.2\n",
      "Ign:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python2.7 amd64 2.7.17-1~18.04ubuntu1.2\n",
      "Err:1 http://security.ubuntu.com/ubuntu bionic-updates/main amd64 libx11-data all 2:1.6.4-3ubuntu0.3\n",
      "  404  Not Found [IP: 91.189.88.152 80]\n",
      "Ign:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libx11-xcb1 amd64 2:1.6.4-3ubuntu0.3\n",
      "Err:2 http://security.ubuntu.com/ubuntu bionic-updates/main amd64 libx11-6 amd64 2:1.6.4-3ubuntu0.3\n",
      "  404  Not Found [IP: 91.189.88.152 80]\n",
      "Err:3 http://security.ubuntu.com/ubuntu bionic-updates/main amd64 libpython2.7-minimal amd64 2.7.17-1~18.04ubuntu1.2\n",
      "  404  Not Found [IP: 91.189.88.152 80]\n",
      "Err:4 http://security.ubuntu.com/ubuntu bionic-updates/main amd64 python2.7-minimal amd64 2.7.17-1~18.04ubuntu1.2\n",
      "  404  Not Found [IP: 91.189.88.152 80]\n",
      "Err:5 http://security.ubuntu.com/ubuntu bionic-updates/main amd64 libpython2.7-stdlib amd64 2.7.17-1~18.04ubuntu1.2\n",
      "  404  Not Found [IP: 91.189.88.152 80]\n",
      "Err:6 http://security.ubuntu.com/ubuntu bionic-updates/main amd64 python2.7 amd64 2.7.17-1~18.04ubuntu1.2\n",
      "  404  Not Found [IP: 91.189.88.152 80]\n",
      "Err:7 http://security.ubuntu.com/ubuntu bionic-updates/main amd64 libx11-xcb1 amd64 2:1.6.4-3ubuntu0.3\n",
      "  404  Not Found [IP: 91.189.88.152 80]\n",
      "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/libx/libx11/libx11-data_1.6.4-3ubuntu0.3_all.deb  404  Not Found [IP: 91.189.88.152 80]\n",
      "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/libx/libx11/libx11-6_1.6.4-3ubuntu0.3_amd64.deb  404  Not Found [IP: 91.189.88.152 80]\n",
      "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/p/python2.7/libpython2.7-minimal_2.7.17-1~18.04ubuntu1.2_amd64.deb  404  Not Found [IP: 91.189.88.152 80]\n",
      "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/p/python2.7/python2.7-minimal_2.7.17-1~18.04ubuntu1.2_amd64.deb  404  Not Found [IP: 91.189.88.152 80]\n",
      "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/p/python2.7/libpython2.7-stdlib_2.7.17-1~18.04ubuntu1.2_amd64.deb  404  Not Found [IP: 91.189.88.152 80]\n",
      "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/p/python2.7/python2.7_2.7.17-1~18.04ubuntu1.2_amd64.deb  404  Not Found [IP: 91.189.88.152 80]\n",
      "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/libx/libx11/libx11-xcb1_1.6.4-3ubuntu0.3_amd64.deb  404  Not Found [IP: 91.189.88.152 80]\n",
      "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libbsd0 libdrm-amdgpu1 libdrm-common libdrm-intel1 libdrm-nouveau2\n",
      "  libdrm-radeon1 libdrm2 libedit2 libelf1 libfontenc1 libgl1 libgl1-mesa-dri\n",
      "  libglapi-mesa libglvnd0 libglx-mesa0 libglx0 libice6 libllvm10 libpciaccess0\n",
      "  libpixman-1-0 libsensors4 libsm6 libx11-6 libx11-data libx11-xcb1 libxau6\n",
      "  libxaw7 libxcb-dri2-0 libxcb-dri3-0 libxcb-glx0 libxcb-present0 libxcb-sync1\n",
      "  libxcb1 libxdamage1 libxdmcp6 libxext6 libxfixes3 libxfont2 libxkbfile1\n",
      "  libxmu6 libxmuu1 libxpm4 libxshmfence1 libxt6 libxxf86vm1 x11-common\n",
      "  x11-xkb-utils xauth xfonts-base xfonts-encodings xfonts-utils xkb-data\n",
      "  xserver-common\n",
      "Suggested packages:\n",
      "  pciutils lm-sensors\n",
      "The following NEW packages will be installed:\n",
      "  libbsd0 libdrm-amdgpu1 libdrm-common libdrm-intel1 libdrm-nouveau2\n",
      "  libdrm-radeon1 libdrm2 libedit2 libelf1 libfontenc1 libgl1 libgl1-mesa-dri\n",
      "  libglapi-mesa libglvnd0 libglx-mesa0 libglx0 libice6 libllvm10 libpciaccess0\n",
      "  libpixman-1-0 libsensors4 libsm6 libx11-6 libx11-data libx11-xcb1 libxau6\n",
      "  libxaw7 libxcb-dri2-0 libxcb-dri3-0 libxcb-glx0 libxcb-present0 libxcb-sync1\n",
      "  libxcb1 libxdamage1 libxdmcp6 libxext6 libxfixes3 libxfont2 libxkbfile1\n",
      "  libxmu6 libxmuu1 libxpm4 libxshmfence1 libxt6 libxxf86vm1 x11-common\n",
      "  x11-xkb-utils xauth xfonts-base xfonts-encodings xfonts-utils xkb-data\n",
      "  xserver-common xvfb\n",
      "0 upgraded, 54 newly installed, 0 to remove and 13 not upgraded.\n",
      "Need to get 1505 kB/35.1 MB of archives.\n",
      "After this operation, 348 MB of additional disk space will be used.\n",
      "Ign:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libx11-data all 2:1.6.4-3ubuntu0.3\n",
      "Ign:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libx11-6 amd64 2:1.6.4-3ubuntu0.3\n",
      "Ign:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libx11-xcb1 amd64 2:1.6.4-3ubuntu0.3\n",
      "Ign:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 xserver-common all 2:1.19.6-1ubuntu4.8\n",
      "Ign:5 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8\n",
      "Err:1 http://security.ubuntu.com/ubuntu bionic-updates/main amd64 libx11-data all 2:1.6.4-3ubuntu0.3\n",
      "  404  Not Found [IP: 91.189.88.142 80]\n",
      "Err:2 http://security.ubuntu.com/ubuntu bionic-updates/main amd64 libx11-6 amd64 2:1.6.4-3ubuntu0.3\n",
      "  404  Not Found [IP: 91.189.88.142 80]\n",
      "Err:3 http://security.ubuntu.com/ubuntu bionic-updates/main amd64 libx11-xcb1 amd64 2:1.6.4-3ubuntu0.3\n",
      "  404  Not Found [IP: 91.189.88.142 80]\n",
      "Err:4 http://security.ubuntu.com/ubuntu bionic-updates/main amd64 xserver-common all 2:1.19.6-1ubuntu4.8\n",
      "  404  Not Found [IP: 91.189.88.142 80]\n",
      "Err:5 http://security.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8\n",
      "  404  Not Found [IP: 91.189.88.142 80]\n",
      "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/libx/libx11/libx11-data_1.6.4-3ubuntu0.3_all.deb  404  Not Found [IP: 91.189.88.142 80]\n",
      "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/libx/libx11/libx11-6_1.6.4-3ubuntu0.3_amd64.deb  404  Not Found [IP: 91.189.88.142 80]\n",
      "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/libx/libx11/libx11-xcb1_1.6.4-3ubuntu0.3_amd64.deb  404  Not Found [IP: 91.189.88.142 80]\n",
      "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/x/xorg-server/xserver-common_1.19.6-1ubuntu4.8_all.deb  404  Not Found [IP: 91.189.88.142 80]\n",
      "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/universe/x/xorg-server/xvfb_1.19.6-1ubuntu4.8_amd64.deb  404  Not Found [IP: 91.189.88.142 80]\n",
      "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n",
      "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (2.2)\n",
      "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.3)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.2.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: piglet in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
      "Requirement already satisfied: piglet-templates in /usr/local/lib/python3.6/dist-packages (from piglet) (1.2.0)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (2.4.7)\n",
      "Requirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (20.3.0)\n",
      "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.1.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (0.36.2)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.2.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "ename": "EasyProcessError",
     "evalue": "start error <EasyProcess cmd_param=['Xvfb', '-help'] cmd=['Xvfb', '-help'] oserror=[Errno 2] No such file or directory: 'Xvfb': 'Xvfb' return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/easyprocess/__init__.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m             self.popen = subprocess.Popen(\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             )\n",
      "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    728\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1363\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Xvfb': 'Xvfb'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEasyProcessError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9acf97954ebe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install piglet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyvirtualdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m900\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyvirtualdisplay/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, backend, visible, size, color_depth, bgcolor, use_xauth, retries, extra_args, manage_global_env, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mmanage_global_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanage_global_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         )\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyvirtualdisplay/xvfb.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size, color_depth, bgcolor, use_xauth, fbdir, dpi, retries, extra_args, manage_global_env)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mextra_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mmanage_global_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanage_global_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         )\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyvirtualdisplay/abstractdisplay.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, program, use_xauth, retries, extra_args, manage_global_env)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retries_current\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mhelptext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_helptext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_displayfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"-displayfd\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhelptext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_displayfd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyvirtualdisplay/util.py\u001b[0m in \u001b[0;36mget_helptext\u001b[0;34m(program)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_stdout_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_stderr_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mhelptext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhelptext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/easyprocess/__init__.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \"\"\"\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/easyprocess/__init__.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OSError exception: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moserror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moserror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moserror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEasyProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"start error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"process was started (pid=%s)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEasyProcessError\u001b[0m: start error <EasyProcess cmd_param=['Xvfb', '-help'] cmd=['Xvfb', '-help'] oserror=[Errno 2] No such file or directory: 'Xvfb': 'Xvfb' return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!apt-get install python-opengl -y\n",
    "!apt install xvfb -y\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install piglet\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import Monitor\n",
    "gymlogger.set_level(40) # error only\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "!pip install gym[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4727,
     "status": "ok",
     "timestamp": 1628269477846,
     "user": {
      "displayName": "Nir Weingarten",
      "photoUrl": "",
      "userId": "08827275300476553234"
     },
     "user_tz": -180
    },
    "id": "IrE3hLIENcpt",
    "outputId": "5a7be766-07e3-4a4e-e4f8-f60e7bd4aa03"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from agent import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune discrete agents\n",
    "For sake of time the hyperparameters here will be used for the other models as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 69426552,
     "status": "error",
     "timestamp": 1628350776575,
     "user": {
      "displayName": "Nir Weingarten",
      "photoUrl": "",
      "userId": "08827275300476553234"
     },
     "user_tz": -180
    },
    "id": "1dhj8a3PNca-",
    "outputId": "34520642-4e2b-44a3-d517-154949054f91",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### SplitDiscreteActorCritic_6_0.99_1000_0.22_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -433.927\tavg length: 64.290\t avg time: 0.105\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -432.850\tavg length: 63.210\t avg time: 0.104\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -436.855\tavg length: 65.120\t avg time: 0.107\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -445.498\tavg length: 65.280\t avg time: 0.108\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -438.511\tavg length: 65.240\t avg time: 0.107\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -425.361\tavg length: 62.200\t avg time: 0.102\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -426.800\tavg length: 63.810\t avg time: 0.104\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -449.900\tavg length: 64.600\t avg time: 0.106\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -437.554\tavg length: 63.510\t avg time: 0.104\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -415.133\tavg length: 61.860\t avg time: 0.102\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -447.076\tavg length: 65.800\t avg time: 0.108\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -441.453\tavg length: 65.580\t avg time: 0.108\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -426.483\tavg length: 63.540\t avg time: 0.105\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -428.145\tavg length: 62.600\t avg time: 0.104\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.664 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_1000_0.22_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -431.962\tavg length: 63.690\t avg time: 0.105\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -430.639\tavg length: 63.580\t avg time: 0.105\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -438.611\tavg length: 64.760\t avg time: 0.107\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -430.342\tavg length: 64.640\t avg time: 0.106\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -419.561\tavg length: 62.610\t avg time: 0.103\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -432.698\tavg length: 64.900\t avg time: 0.107\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -421.488\tavg length: 62.900\t avg time: 0.103\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -436.643\tavg length: 64.110\t avg time: 0.106\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -438.686\tavg length: 64.300\t avg time: 0.106\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -441.411\tavg length: 63.980\t avg time: 0.105\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -443.844\tavg length: 64.640\t avg time: 0.106\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -428.648\tavg length: 63.010\t avg time: 0.103\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -468.149\tavg length: 65.920\t avg time: 0.108\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -442.834\tavg length: 63.160\t avg time: 0.103\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.629 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_1000_0.22_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -512.544\tavg length: 63.990\t avg time: 0.105\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -524.648\tavg length: 64.990\t avg time: 0.107\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -519.360\tavg length: 65.350\t avg time: 0.107\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -502.332\tavg length: 63.300\t avg time: 0.104\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -498.059\tavg length: 62.820\t avg time: 0.103\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -495.952\tavg length: 63.290\t avg time: 0.105\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -484.875\tavg length: 63.480\t avg time: 0.105\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -482.266\tavg length: 61.320\t avg time: 0.101\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -503.485\tavg length: 61.990\t avg time: 0.102\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -512.516\tavg length: 62.930\t avg time: 0.104\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -483.989\tavg length: 63.490\t avg time: 0.105\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -501.618\tavg length: 64.070\t avg time: 0.106\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -512.330\tavg length: 65.130\t avg time: 0.107\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -506.419\tavg length: 62.930\t avg time: 0.104\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.617 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_1000_0.0018_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -378.076\tavg length: 68.830\t avg time: 0.116\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -187.257\tavg length: 74.520\t avg time: 0.125\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -178.248\tavg length: 79.840\t avg time: 0.134\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -161.857\tavg length: 77.840\t avg time: 0.131\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -136.964\tavg length: 86.450\t avg time: 0.148\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -124.952\tavg length: 89.320\t avg time: 0.153\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -139.962\tavg length: 92.370\t avg time: 0.160\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -206.531\tavg length: 101.540\t avg time: 0.178\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -157.702\tavg length: 70.980\t avg time: 0.120\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -220.241\tavg length: 81.330\t avg time: 0.139\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -190.650\tavg length: 91.430\t avg time: 0.160\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -215.146\tavg length: 183.750\t avg time: 0.388\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -252.752\tavg length: 180.040\t avg time: 0.366\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -137.184\tavg length: 185.790\t avg time: 0.395\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 5.112 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_1000_0.0018_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -209.448\tavg length: 73.250\t avg time: 0.121\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -169.204\tavg length: 74.390\t avg time: 0.121\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -161.386\tavg length: 74.710\t avg time: 0.122\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -159.755\tavg length: 82.350\t avg time: 0.133\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -180.427\tavg length: 92.140\t avg time: 0.147\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -149.210\tavg length: 75.590\t avg time: 0.120\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -149.814\tavg length: 66.210\t avg time: 0.101\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -152.563\tavg length: 66.350\t avg time: 0.103\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -161.979\tavg length: 69.190\t avg time: 0.108\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -195.636\tavg length: 72.350\t avg time: 0.116\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -97.328\tavg length: 77.590\t avg time: 0.130\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -61.468\tavg length: 128.870\t avg time: 0.280\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -19.646\tavg length: 494.160\t avg time: 1.697\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -26.156\tavg length: 784.310\t avg time: 2.765\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 15.453 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_1000_0.0018_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -238.238\tavg length: 85.380\t avg time: 0.146\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -185.118\tavg length: 86.190\t avg time: 0.152\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -172.284\tavg length: 89.270\t avg time: 0.156\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -163.907\tavg length: 71.320\t avg time: 0.121\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -161.657\tavg length: 68.100\t avg time: 0.110\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -162.369\tavg length: 66.070\t avg time: 0.102\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -159.983\tavg length: 66.920\t avg time: 0.102\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -161.591\tavg length: 65.200\t avg time: 0.100\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -177.499\tavg length: 70.410\t avg time: 0.109\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -173.917\tavg length: 72.400\t avg time: 0.113\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -156.839\tavg length: 70.600\t avg time: 0.110\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -161.483\tavg length: 72.270\t avg time: 0.114\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -163.781\tavg length: 75.620\t avg time: 0.121\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -163.887\tavg length: 83.430\t avg time: 0.134\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -430.762\tavg length: 62.300\t avg time: 0.103\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -423.774\tavg length: 62.160\t avg time: 0.103\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -445.740\tavg length: 63.500\t avg time: 0.106\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -438.519\tavg length: 64.170\t avg time: 0.109\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.714 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_1000_0.0014_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -238.898\tavg length: 90.420\t avg time: 0.155\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -221.001\tavg length: 94.780\t avg time: 0.163\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -190.955\tavg length: 88.440\t avg time: 0.150\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -156.326\tavg length: 87.950\t avg time: 0.148\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -152.066\tavg length: 86.990\t avg time: 0.147\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -126.400\tavg length: 116.090\t avg time: 0.210\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -160.986\tavg length: 223.840\t avg time: 0.490\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -72.868\tavg length: 198.310\t avg time: 0.407\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -25.156\tavg length: 244.920\t avg time: 0.569\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -30.476\tavg length: 386.910\t avg time: 1.097\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -70.954\tavg length: 683.520\t avg time: 2.299\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -52.726\tavg length: 835.260\t avg time: 2.909\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -18.392\tavg length: 832.500\t avg time: 3.097\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -12.739\tavg length: 899.130\t avg time: 3.403\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 31.494 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_1000_0.0014_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -208.880\tavg length: 81.710\t avg time: 0.140\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -193.991\tavg length: 81.460\t avg time: 0.144\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -187.394\tavg length: 92.970\t avg time: 0.168\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -172.578\tavg length: 99.600\t avg time: 0.180\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -162.443\tavg length: 125.260\t avg time: 0.232\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -157.404\tavg length: 159.260\t avg time: 0.312\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -141.659\tavg length: 169.330\t avg time: 0.328\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -71.796\tavg length: 220.630\t avg time: 0.502\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -119.188\tavg length: 299.090\t avg time: 0.738\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -59.932\tavg length: 326.630\t avg time: 0.844\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -20.298\tavg length: 444.980\t avg time: 1.330\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -45.943\tavg length: 659.370\t avg time: 2.272\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -30.063\tavg length: 803.590\t avg time: 2.943\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -20.343\tavg length: 722.830\t avg time: 2.658\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 25.092 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_1000_0.001_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -331.184\tavg length: 105.270\t avg time: 0.192\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -245.052\tavg length: 103.650\t avg time: 0.190\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -197.419\tavg length: 71.420\t avg time: 0.124\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -179.051\tavg length: 67.780\t avg time: 0.116\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -154.349\tavg length: 64.590\t avg time: 0.110\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -181.488\tavg length: 68.470\t avg time: 0.116\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -175.325\tavg length: 66.900\t avg time: 0.113\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -155.142\tavg length: 67.490\t avg time: 0.112\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -133.441\tavg length: 78.310\t avg time: 0.132\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -116.937\tavg length: 92.300\t avg time: 0.178\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -118.228\tavg length: 106.330\t avg time: 0.189\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -135.336\tavg length: 267.740\t avg time: 0.615\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -31.713\tavg length: 524.300\t avg time: 1.679\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -27.393\tavg length: 816.320\t avg time: 2.837\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 16.278 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_1000_0.001_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -267.593\tavg length: 82.730\t avg time: 0.142\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -286.541\tavg length: 98.390\t avg time: 0.175\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -265.485\tavg length: 115.260\t avg time: 0.207\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -233.966\tavg length: 124.850\t avg time: 0.227\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -274.339\tavg length: 154.300\t avg time: 0.285\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -223.517\tavg length: 150.200\t avg time: 0.302\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -181.702\tavg length: 131.240\t avg time: 0.236\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -158.372\tavg length: 124.090\t avg time: 0.227\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -197.368\tavg length: 156.670\t avg time: 0.292\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -310.344\tavg length: 237.130\t avg time: 0.487\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -209.524\tavg length: 171.600\t avg time: 0.320\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -143.865\tavg length: 196.400\t avg time: 0.394\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -163.943\tavg length: 249.030\t avg time: 0.611\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -52.174\tavg length: 355.060\t avg time: 0.970\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 11.845 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_1000_0.001_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -233.576\tavg length: 76.190\t avg time: 0.131\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -224.181\tavg length: 79.450\t avg time: 0.137\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -203.495\tavg length: 86.800\t avg time: 0.160\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -197.530\tavg length: 82.950\t avg time: 0.142\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -203.979\tavg length: 86.050\t avg time: 0.149\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -195.461\tavg length: 82.040\t avg time: 0.141\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -163.249\tavg length: 82.540\t avg time: 0.141\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -176.036\tavg length: 82.050\t avg time: 0.138\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -170.722\tavg length: 83.270\t avg time: 0.140\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -170.183\tavg length: 82.680\t avg time: 0.140\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -164.213\tavg length: 89.570\t avg time: 0.152\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -151.351\tavg length: 84.100\t avg time: 0.144\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -178.500\tavg length: 91.800\t avg time: 0.158\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -146.584\tavg length: 109.650\t avg time: 0.205\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.765 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_1000_0.0006_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -230.151\tavg length: 85.800\t avg time: 0.148\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -184.189\tavg length: 85.540\t avg time: 0.148\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -172.083\tavg length: 88.210\t avg time: 0.153\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -148.753\tavg length: 106.040\t avg time: 0.190\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -161.705\tavg length: 110.310\t avg time: 0.199\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -125.122\tavg length: 112.760\t avg time: 0.201\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -126.632\tavg length: 109.130\t avg time: 0.193\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -118.614\tavg length: 116.110\t avg time: 0.207\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -77.888\tavg length: 158.710\t avg time: 0.302\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -47.416\tavg length: 221.280\t avg time: 0.498\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -66.427\tavg length: 395.510\t avg time: 1.186\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -13.255\tavg length: 516.420\t avg time: 1.782\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -12.423\tavg length: 632.520\t avg time: 2.349\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -12.538\tavg length: 746.570\t avg time: 3.003\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 23.103 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_1000_0.0006_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -243.271\tavg length: 78.610\t avg time: 0.134\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -181.548\tavg length: 81.150\t avg time: 0.144\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -184.693\tavg length: 84.380\t avg time: 0.149\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -162.069\tavg length: 93.030\t avg time: 0.180\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -178.495\tavg length: 89.610\t avg time: 0.159\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -155.889\tavg length: 94.430\t avg time: 0.166\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -177.008\tavg length: 98.270\t avg time: 0.181\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -157.455\tavg length: 95.580\t avg time: 0.167\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -137.412\tavg length: 101.070\t avg time: 0.177\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -173.632\tavg length: 127.270\t avg time: 0.241\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -122.432\tavg length: 141.630\t avg time: 0.265\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -149.823\tavg length: 168.870\t avg time: 0.339\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -147.786\tavg length: 195.060\t avg time: 0.383\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -145.914\tavg length: 252.260\t avg time: 0.598\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 6.649 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_1000_0.0006_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -263.522\tavg length: 81.180\t avg time: 0.138\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -223.401\tavg length: 77.200\t avg time: 0.131\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -227.766\tavg length: 78.240\t avg time: 0.132\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -198.297\tavg length: 82.410\t avg time: 0.138\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -170.976\tavg length: 79.980\t avg time: 0.134\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -173.105\tavg length: 80.370\t avg time: 0.135\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -173.000\tavg length: 81.330\t avg time: 0.137\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -160.906\tavg length: 84.740\t avg time: 0.143\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -173.004\tavg length: 85.300\t avg time: 0.144\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -169.304\tavg length: 82.360\t avg time: 0.138\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -150.068\tavg length: 83.530\t avg time: 0.142\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -164.436\tavg length: 84.600\t avg time: 0.143\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -148.841\tavg length: 84.350\t avg time: 0.143\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -157.307\tavg length: 82.130\t avg time: 0.139\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.473 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_250_0.22_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -493.032\tavg length: 64.360\t avg time: 0.108\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -507.026\tavg length: 62.720\t avg time: 0.105\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -516.522\tavg length: 64.640\t avg time: 0.108\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -510.183\tavg length: 62.730\t avg time: 0.105\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -515.976\tavg length: 64.040\t avg time: 0.107\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -499.601\tavg length: 62.570\t avg time: 0.105\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -509.445\tavg length: 62.810\t avg time: 0.105\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -518.233\tavg length: 64.020\t avg time: 0.108\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -502.156\tavg length: 62.100\t avg time: 0.104\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -494.888\tavg length: 62.610\t avg time: 0.105\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -519.504\tavg length: 62.930\t avg time: 0.105\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -489.047\tavg length: 63.080\t avg time: 0.106\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -497.132\tavg length: 61.310\t avg time: 0.102\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -513.165\tavg length: 63.890\t avg time: 0.108\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.649 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_250_0.22_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -559.868\tavg length: 62.160\t avg time: 0.104\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -573.922\tavg length: 62.520\t avg time: 0.104\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -556.135\tavg length: 61.280\t avg time: 0.102\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -576.075\tavg length: 63.270\t avg time: 0.105\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -582.755\tavg length: 63.290\t avg time: 0.104\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -538.550\tavg length: 60.900\t avg time: 0.100\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -582.225\tavg length: 63.180\t avg time: 0.104\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -563.799\tavg length: 60.540\t avg time: 0.100\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -563.393\tavg length: 61.890\t avg time: 0.102\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -577.293\tavg length: 64.750\t avg time: 0.107\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -563.030\tavg length: 63.470\t avg time: 0.105\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -530.991\tavg length: 60.160\t avg time: 0.100\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -588.203\tavg length: 63.560\t avg time: 0.106\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -571.549\tavg length: 62.640\t avg time: 0.104\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.586 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_250_0.22_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -498.989\tavg length: 61.500\t avg time: 0.102\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -506.480\tavg length: 62.350\t avg time: 0.104\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -494.333\tavg length: 63.690\t avg time: 0.106\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -517.169\tavg length: 63.350\t avg time: 0.105\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -511.128\tavg length: 64.710\t avg time: 0.108\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -503.622\tavg length: 63.530\t avg time: 0.105\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -497.690\tavg length: 62.690\t avg time: 0.104\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -498.379\tavg length: 62.870\t avg time: 0.105\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -488.259\tavg length: 62.510\t avg time: 0.104\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -516.821\tavg length: 62.780\t avg time: 0.104\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -502.897\tavg length: 63.110\t avg time: 0.105\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -520.527\tavg length: 64.240\t avg time: 0.106\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -480.821\tavg length: 62.200\t avg time: 0.103\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -503.461\tavg length: 62.880\t avg time: 0.104\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.616 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_250_0.0018_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -366.106\tavg length: 86.640\t avg time: 0.149\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -333.804\tavg length: 167.190\t avg time: 0.362\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -149.659\tavg length: 159.990\t avg time: 0.306\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -56.152\tavg length: 134.190\t avg time: 0.246\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -79.894\tavg length: 380.810\t avg time: 1.171\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -28.145\tavg length: 431.770\t avg time: 1.536\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -79.956\tavg length: 150.070\t avg time: 0.377\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -122.272\tavg length: 118.990\t avg time: 0.217\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -27.398\tavg length: 578.030\t avg time: 1.823\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: 33.710\tavg length: 929.880\t avg time: 2.860\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: 27.654\tavg length: 319.760\t avg time: 0.835\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: 23.965\tavg length: 554.480\t avg time: 1.803\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: 140.639\tavg length: 372.690\t avg time: 0.895\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 91.315\tavg length: 619.010\t avg time: 1.870\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 24.850 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_250_0.0018_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -258.024\tavg length: 74.130\t avg time: 0.126\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -238.586\tavg length: 82.250\t avg time: 0.138\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -237.752\tavg length: 80.280\t avg time: 0.136\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -211.665\tavg length: 80.550\t avg time: 0.137\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -199.297\tavg length: 90.070\t avg time: 0.154\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -183.539\tavg length: 90.160\t avg time: 0.154\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -181.577\tavg length: 96.590\t avg time: 0.167\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -131.738\tavg length: 98.510\t avg time: 0.170\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -140.257\tavg length: 101.210\t avg time: 0.174\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -125.265\tavg length: 116.510\t avg time: 0.204\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -157.612\tavg length: 115.240\t avg time: 0.204\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -129.398\tavg length: 149.830\t avg time: 0.293\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -106.910\tavg length: 162.970\t avg time: 0.310\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -164.706\tavg length: 208.170\t avg time: 0.465\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 5.220 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_250_0.0018_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -221.210\tavg length: 85.540\t avg time: 0.147\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -207.387\tavg length: 95.120\t avg time: 0.164\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -186.406\tavg length: 91.990\t avg time: 0.158\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -162.466\tavg length: 87.120\t avg time: 0.148\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -149.622\tavg length: 83.270\t avg time: 0.141\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -142.842\tavg length: 84.910\t avg time: 0.143\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -132.487\tavg length: 90.470\t avg time: 0.154\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -145.289\tavg length: 123.620\t avg time: 0.215\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -146.273\tavg length: 158.540\t avg time: 0.303\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -297.562\tavg length: 168.770\t avg time: 0.341\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -170.419\tavg length: 167.660\t avg time: 0.331\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -247.216\tavg length: 138.660\t avg time: 0.279\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -336.285\tavg length: 115.520\t avg time: 0.214\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -428.048\tavg length: 150.850\t avg time: 0.317\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 5.704 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_250_0.0014_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -533.005\tavg length: 74.930\t avg time: 0.128\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -445.216\tavg length: 63.820\t avg time: 0.106\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -439.168\tavg length: 64.050\t avg time: 0.107\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -437.035\tavg length: 63.710\t avg time: 0.106\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -417.435\tavg length: 67.400\t avg time: 0.112\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -254.816\tavg length: 115.500\t avg time: 0.207\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -258.493\tavg length: 150.420\t avg time: 0.290\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -396.902\tavg length: 118.360\t avg time: 0.216\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -827.404\tavg length: 182.680\t avg time: 0.376\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -457.687\tavg length: 117.120\t avg time: 0.215\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -432.399\tavg length: 127.290\t avg time: 0.236\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -513.310\tavg length: 253.100\t avg time: 0.609\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -75.842\tavg length: 294.700\t avg time: 0.775\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 7.125\tavg length: 562.680\t avg time: 1.782\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 11.446 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_250_0.0014_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -322.601\tavg length: 79.540\t avg time: 0.138\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -198.533\tavg length: 75.560\t avg time: 0.129\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -186.876\tavg length: 88.660\t avg time: 0.165\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -168.103\tavg length: 74.250\t avg time: 0.124\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -168.074\tavg length: 75.350\t avg time: 0.125\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -160.768\tavg length: 73.590\t avg time: 0.123\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -216.930\tavg length: 70.140\t avg time: 0.116\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -149.785\tavg length: 67.680\t avg time: 0.112\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -136.868\tavg length: 67.140\t avg time: 0.112\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -129.301\tavg length: 72.850\t avg time: 0.122\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -150.646\tavg length: 69.550\t avg time: 0.115\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -152.225\tavg length: 67.340\t avg time: 0.111\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -221.393\tavg length: 67.720\t avg time: 0.111\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -119.248\tavg length: 70.520\t avg time: 0.115\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.076 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_250_0.0014_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -232.708\tavg length: 77.130\t avg time: 0.129\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -190.835\tavg length: 78.420\t avg time: 0.130\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -167.645\tavg length: 78.910\t avg time: 0.130\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -179.071\tavg length: 89.570\t avg time: 0.148\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -185.690\tavg length: 103.560\t avg time: 0.175\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -197.854\tavg length: 102.780\t avg time: 0.175\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -147.453\tavg length: 114.260\t avg time: 0.196\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -172.684\tavg length: 188.970\t avg time: 0.363\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -62.205\tavg length: 172.000\t avg time: 0.326\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -97.067\tavg length: 239.900\t avg time: 0.525\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -110.419\tavg length: 240.930\t avg time: 0.540\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -44.961\tavg length: 302.790\t avg time: 0.748\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -20.711\tavg length: 220.730\t avg time: 0.460\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -14.859\tavg length: 308.600\t avg time: 0.759\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 9.880 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_250_0.001_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -235.397\tavg length: 80.640\t avg time: 0.137\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -189.116\tavg length: 74.390\t avg time: 0.126\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -150.247\tavg length: 72.900\t avg time: 0.123\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -123.261\tavg length: 74.260\t avg time: 0.126\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -135.488\tavg length: 74.280\t avg time: 0.127\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -133.199\tavg length: 86.370\t avg time: 0.160\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -185.721\tavg length: 118.430\t avg time: 0.260\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -137.723\tavg length: 524.700\t avg time: 1.915\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -17.626\tavg length: 797.680\t avg time: 3.368\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: 45.654\tavg length: 620.010\t avg time: 2.129\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -667.513\tavg length: 332.560\t avg time: 0.894\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -452.805\tavg length: 205.510\t avg time: 0.471\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -750.620\tavg length: 379.450\t avg time: 1.050\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -45.223\tavg length: 580.180\t avg time: 1.930\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 24.162 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_250_0.001_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -195.976\tavg length: 85.910\t avg time: 0.146\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -213.813\tavg length: 99.030\t avg time: 0.172\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -174.251\tavg length: 106.140\t avg time: 0.185\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -184.762\tavg length: 116.290\t avg time: 0.207\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -203.418\tavg length: 116.160\t avg time: 0.206\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -123.078\tavg length: 152.500\t avg time: 0.300\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -455.897\tavg length: 202.180\t avg time: 0.416\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -486.888\tavg length: 94.500\t avg time: 0.166\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -450.787\tavg length: 257.570\t avg time: 0.564\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -140.043\tavg length: 233.250\t avg time: 0.507\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -64.891\tavg length: 178.200\t avg time: 0.363\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -62.123\tavg length: 193.780\t avg time: 0.415\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -64.051\tavg length: 219.540\t avg time: 0.489\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -31.326\tavg length: 270.800\t avg time: 0.671\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 10.566 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_250_0.001_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -341.220\tavg length: 80.050\t avg time: 0.136\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -254.695\tavg length: 75.620\t avg time: 0.128\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -209.469\tavg length: 84.170\t avg time: 0.149\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -194.063\tavg length: 78.570\t avg time: 0.131\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -179.268\tavg length: 74.770\t avg time: 0.124\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -169.152\tavg length: 79.330\t avg time: 0.132\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -189.901\tavg length: 77.860\t avg time: 0.129\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -166.806\tavg length: 71.820\t avg time: 0.118\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -151.970\tavg length: 73.170\t avg time: 0.121\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -173.853\tavg length: 77.280\t avg time: 0.128\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -215.909\tavg length: 75.660\t avg time: 0.125\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -215.264\tavg length: 79.970\t avg time: 0.133\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -166.389\tavg length: 80.270\t avg time: 0.134\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -170.923\tavg length: 81.340\t avg time: 0.136\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.263 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_250_0.0006_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -224.048\tavg length: 86.400\t avg time: 0.146\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -214.149\tavg length: 86.920\t avg time: 0.148\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -190.456\tavg length: 89.200\t avg time: 0.152\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -163.291\tavg length: 87.710\t avg time: 0.151\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -154.404\tavg length: 97.870\t avg time: 0.170\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -148.398\tavg length: 98.170\t avg time: 0.171\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -230.306\tavg length: 164.230\t avg time: 0.326\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -171.217\tavg length: 104.520\t avg time: 0.186\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -114.515\tavg length: 117.400\t avg time: 0.209\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -449.693\tavg length: 146.660\t avg time: 0.302\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -469.868\tavg length: 188.400\t avg time: 0.394\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -366.202\tavg length: 145.350\t avg time: 0.273\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -163.416\tavg length: 473.020\t avg time: 1.445\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -14.548\tavg length: 858.070\t avg time: 3.013\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 14.509 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_250_0.0006_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -261.049\tavg length: 88.710\t avg time: 0.152\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -200.253\tavg length: 87.040\t avg time: 0.148\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -183.864\tavg length: 81.380\t avg time: 0.138\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -169.616\tavg length: 80.820\t avg time: 0.137\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -150.542\tavg length: 87.790\t avg time: 0.150\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -177.139\tavg length: 93.860\t avg time: 0.161\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -210.505\tavg length: 92.390\t avg time: 0.157\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -156.630\tavg length: 93.320\t avg time: 0.159\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -157.422\tavg length: 95.840\t avg time: 0.163\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -169.489\tavg length: 87.640\t avg time: 0.149\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -161.365\tavg length: 93.890\t avg time: 0.159\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -155.337\tavg length: 96.110\t avg time: 0.164\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -135.171\tavg length: 98.460\t avg time: 0.169\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -139.886\tavg length: 100.170\t avg time: 0.172\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.950 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.99_250_0.0006_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -266.180\tavg length: 80.610\t avg time: 0.135\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -195.192\tavg length: 77.510\t avg time: 0.131\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -204.559\tavg length: 86.710\t avg time: 0.145\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -187.068\tavg length: 91.360\t avg time: 0.154\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -195.067\tavg length: 88.130\t avg time: 0.148\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -188.389\tavg length: 92.380\t avg time: 0.156\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -176.572\tavg length: 102.110\t avg time: 0.175\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -177.797\tavg length: 103.700\t avg time: 0.179\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -194.690\tavg length: 112.870\t avg time: 0.198\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -168.563\tavg length: 109.820\t avg time: 0.192\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -155.910\tavg length: 114.490\t avg time: 0.200\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -174.153\tavg length: 129.000\t avg time: 0.231\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -216.472\tavg length: 184.050\t avg time: 0.361\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -311.788\tavg length: 215.810\t avg time: 0.453\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 5.307 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_1000_0.22_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -163.783\tavg length: 63.060\t avg time: 0.096\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -161.485\tavg length: 64.560\t avg time: 0.098\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -164.183\tavg length: 65.300\t avg time: 0.100\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -147.295\tavg length: 66.450\t avg time: 0.101\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -163.240\tavg length: 63.580\t avg time: 0.097\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -157.154\tavg length: 66.150\t avg time: 0.101\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -146.799\tavg length: 66.090\t avg time: 0.101\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -166.860\tavg length: 64.840\t avg time: 0.099\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -159.122\tavg length: 67.340\t avg time: 0.103\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -163.256\tavg length: 64.520\t avg time: 0.098\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -163.572\tavg length: 66.770\t avg time: 0.102\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -160.667\tavg length: 64.930\t avg time: 0.099\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -156.004\tavg length: 66.430\t avg time: 0.101\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -160.009\tavg length: 66.180\t avg time: 0.101\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.498 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_1000_0.22_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -564.134\tavg length: 62.450\t avg time: 0.103\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -550.973\tavg length: 63.330\t avg time: 0.105\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -574.584\tavg length: 64.080\t avg time: 0.106\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -572.178\tavg length: 63.370\t avg time: 0.104\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -574.354\tavg length: 62.790\t avg time: 0.105\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -589.480\tavg length: 63.730\t avg time: 0.106\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -545.037\tavg length: 60.810\t avg time: 0.101\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -568.621\tavg length: 63.320\t avg time: 0.105\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -562.525\tavg length: 63.650\t avg time: 0.106\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -577.595\tavg length: 63.310\t avg time: 0.105\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -543.293\tavg length: 61.220\t avg time: 0.102\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -561.572\tavg length: 62.000\t avg time: 0.103\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -572.723\tavg length: 63.160\t avg time: 0.104\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -554.216\tavg length: 62.200\t avg time: 0.102\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.599 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_1000_0.22_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -1006.355\tavg length: 79.620\t avg time: 0.142\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -999.210\tavg length: 78.020\t avg time: 0.139\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -1019.425\tavg length: 79.240\t avg time: 0.142\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -994.031\tavg length: 77.550\t avg time: 0.138\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -1051.699\tavg length: 82.250\t avg time: 0.147\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -1041.122\tavg length: 81.140\t avg time: 0.145\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -959.737\tavg length: 75.380\t avg time: 0.135\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -1010.663\tavg length: 78.510\t avg time: 0.141\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -1003.554\tavg length: 78.270\t avg time: 0.140\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -977.818\tavg length: 77.420\t avg time: 0.139\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -983.928\tavg length: 76.460\t avg time: 0.137\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -972.368\tavg length: 76.090\t avg time: 0.137\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -1020.123\tavg length: 79.960\t avg time: 0.143\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -1046.479\tavg length: 80.790\t avg time: 0.145\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.515 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_1000_0.0018_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -273.860\tavg length: 104.870\t avg time: 0.186\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -715.306\tavg length: 92.920\t avg time: 0.168\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -851.688\tavg length: 80.930\t avg time: 0.147\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -881.401\tavg length: 84.110\t avg time: 0.152\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -843.456\tavg length: 81.570\t avg time: 0.148\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -859.089\tavg length: 82.650\t avg time: 0.150\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -845.263\tavg length: 81.260\t avg time: 0.147\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -868.651\tavg length: 84.250\t avg time: 0.152\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -909.455\tavg length: 86.150\t avg time: 0.156\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -886.289\tavg length: 85.270\t avg time: 0.155\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -906.162\tavg length: 86.330\t avg time: 0.157\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -905.573\tavg length: 86.050\t avg time: 0.156\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -873.869\tavg length: 84.010\t avg time: 0.152\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -880.013\tavg length: 84.880\t avg time: 0.153\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.886 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_1000_0.0018_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -198.121\tavg length: 82.200\t avg time: 0.143\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -410.341\tavg length: 110.660\t avg time: 0.208\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -499.870\tavg length: 63.050\t avg time: 0.108\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -515.412\tavg length: 63.870\t avg time: 0.109\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -503.801\tavg length: 62.440\t avg time: 0.104\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -492.257\tavg length: 63.880\t avg time: 0.106\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -510.967\tavg length: 63.080\t avg time: 0.105\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -490.059\tavg length: 61.990\t avg time: 0.103\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -494.949\tavg length: 63.240\t avg time: 0.105\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -498.421\tavg length: 64.540\t avg time: 0.107\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -559.023\tavg length: 65.240\t avg time: 0.109\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -950.603\tavg length: 82.290\t avg time: 0.147\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -930.860\tavg length: 80.720\t avg time: 0.144\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -967.267\tavg length: 82.420\t avg time: 0.148\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.146 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_1000_0.0018_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -270.946\tavg length: 94.420\t avg time: 0.161\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -212.940\tavg length: 92.500\t avg time: 0.158\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -225.763\tavg length: 119.670\t avg time: 0.212\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -265.601\tavg length: 170.060\t avg time: 0.338\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -260.880\tavg length: 304.690\t avg time: 0.740\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -185.255\tavg length: 369.020\t avg time: 1.064\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -114.192\tavg length: 436.860\t avg time: 1.423\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -53.683\tavg length: 699.810\t avg time: 2.643\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -18.819\tavg length: 606.700\t avg time: 2.265\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -25.026\tavg length: 827.540\t avg time: 3.343\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -39.684\tavg length: 825.060\t avg time: 3.244\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: 3.786\tavg length: 774.380\t avg time: 3.021\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: 38.567\tavg length: 800.220\t avg time: 2.871\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 70.980\tavg length: 843.780\t avg time: 2.858\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 46.278 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_1000_0.0014_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -202.294\tavg length: 96.490\t avg time: 0.177\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -256.230\tavg length: 122.740\t avg time: 0.230\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -233.778\tavg length: 158.880\t avg time: 0.311\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -255.413\tavg length: 248.430\t avg time: 0.592\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -280.309\tavg length: 285.910\t avg time: 0.714\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -418.984\tavg length: 208.980\t avg time: 0.505\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -337.475\tavg length: 73.500\t avg time: 0.129\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -443.127\tavg length: 66.140\t avg time: 0.116\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -433.970\tavg length: 63.420\t avg time: 0.109\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -422.803\tavg length: 62.230\t avg time: 0.105\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -442.690\tavg length: 62.780\t avg time: 0.105\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -448.512\tavg length: 68.590\t avg time: 0.116\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -472.524\tavg length: 71.480\t avg time: 0.122\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -403.192\tavg length: 79.060\t avg time: 0.136\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 6.048 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_1000_0.0014_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -310.658\tavg length: 93.310\t avg time: 0.160\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -237.129\tavg length: 102.280\t avg time: 0.173\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -216.968\tavg length: 100.360\t avg time: 0.167\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -288.145\tavg length: 190.280\t avg time: 0.387\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -432.669\tavg length: 322.320\t avg time: 0.820\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -141.466\tavg length: 155.170\t avg time: 0.300\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -283.119\tavg length: 400.900\t avg time: 1.130\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -101.079\tavg length: 500.580\t avg time: 1.596\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -36.717\tavg length: 495.510\t avg time: 1.652\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: 32.536\tavg length: 620.480\t avg time: 2.098\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: 37.815\tavg length: 882.980\t avg time: 3.200\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: 80.029\tavg length: 906.580\t avg time: 3.293\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: 75.826\tavg length: 851.190\t avg time: 2.910\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 66.368\tavg length: 866.160\t avg time: 3.061\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 40.149 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_1000_0.0014_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -244.869\tavg length: 82.540\t avg time: 0.142\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -183.416\tavg length: 81.710\t avg time: 0.144\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -196.053\tavg length: 82.820\t avg time: 0.147\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -175.502\tavg length: 91.190\t avg time: 0.162\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -184.390\tavg length: 141.110\t avg time: 0.263\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -201.466\tavg length: 128.650\t avg time: 0.242\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -201.984\tavg length: 128.850\t avg time: 0.239\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -168.026\tavg length: 169.490\t avg time: 0.326\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -218.247\tavg length: 194.670\t avg time: 0.395\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -135.909\tavg length: 222.670\t avg time: 0.457\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -114.331\tavg length: 333.340\t avg time: 0.804\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -90.497\tavg length: 391.610\t avg time: 1.060\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -38.143\tavg length: 575.960\t avg time: 1.807\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -38.356\tavg length: 614.930\t avg time: 2.020\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 17.417 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_1000_0.001_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -195.255\tavg length: 78.870\t avg time: 0.136\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -349.792\tavg length: 130.290\t avg time: 0.244\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -179.443\tavg length: 115.200\t avg time: 0.209\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -100.318\tavg length: 224.460\t avg time: 0.488\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -78.247\tavg length: 605.590\t avg time: 1.971\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -51.028\tavg length: 657.310\t avg time: 2.201\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -48.107\tavg length: 883.370\t avg time: 3.222\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -54.654\tavg length: 879.040\t avg time: 3.178\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -11.664\tavg length: 917.430\t avg time: 3.346\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -7.429\tavg length: 915.650\t avg time: 3.331\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: 18.623\tavg length: 971.350\t avg time: 3.484\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: 48.572\tavg length: 888.340\t avg time: 2.942\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: 57.592\tavg length: 947.070\t avg time: 3.246\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 79.309\tavg length: 967.480\t avg time: 3.285\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 57.421 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_1000_0.001_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -298.838\tavg length: 81.260\t avg time: 0.141\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -202.038\tavg length: 78.560\t avg time: 0.140\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -187.205\tavg length: 80.240\t avg time: 0.142\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -165.829\tavg length: 90.400\t avg time: 0.161\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -188.815\tavg length: 93.680\t avg time: 0.165\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -180.971\tavg length: 102.340\t avg time: 0.180\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -202.588\tavg length: 139.700\t avg time: 0.264\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -177.056\tavg length: 128.380\t avg time: 0.232\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -313.352\tavg length: 203.040\t avg time: 0.407\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -233.168\tavg length: 237.040\t avg time: 0.501\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -139.299\tavg length: 325.350\t avg time: 0.786\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -77.360\tavg length: 431.120\t avg time: 1.201\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -62.468\tavg length: 461.630\t avg time: 1.341\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -81.423\tavg length: 685.370\t avg time: 2.245\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 17.271 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_1000_0.001_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -256.755\tavg length: 84.470\t avg time: 0.146\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -203.131\tavg length: 88.430\t avg time: 0.154\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -193.460\tavg length: 89.980\t avg time: 0.158\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -173.046\tavg length: 92.960\t avg time: 0.162\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -168.442\tavg length: 93.270\t avg time: 0.163\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -168.133\tavg length: 96.750\t avg time: 0.168\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -135.846\tavg length: 115.480\t avg time: 0.211\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -110.178\tavg length: 118.010\t avg time: 0.210\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -194.030\tavg length: 185.660\t avg time: 0.352\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -164.439\tavg length: 185.250\t avg time: 0.365\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -212.317\tavg length: 202.950\t avg time: 0.404\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -240.479\tavg length: 263.320\t avg time: 0.555\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -139.582\tavg length: 262.680\t avg time: 0.558\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -110.925\tavg length: 369.420\t avg time: 0.947\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 9.461 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_1000_0.0006_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -277.862\tavg length: 92.070\t avg time: 0.173\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -305.745\tavg length: 96.310\t avg time: 0.171\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -209.089\tavg length: 109.600\t avg time: 0.197\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -183.716\tavg length: 122.380\t avg time: 0.224\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -257.000\tavg length: 176.840\t avg time: 0.357\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -327.201\tavg length: 245.410\t avg time: 0.550\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -300.192\tavg length: 231.950\t avg time: 0.544\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -456.934\tavg length: 275.950\t avg time: 0.698\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -504.210\tavg length: 343.310\t avg time: 0.900\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -421.640\tavg length: 331.910\t avg time: 0.877\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -269.196\tavg length: 360.380\t avg time: 1.036\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -517.988\tavg length: 392.470\t avg time: 1.088\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -388.039\tavg length: 478.750\t avg time: 1.596\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -194.864\tavg length: 450.700\t avg time: 1.493\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 20.855 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_1000_0.0006_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -256.795\tavg length: 81.160\t avg time: 0.140\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -234.425\tavg length: 82.300\t avg time: 0.144\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -199.958\tavg length: 88.450\t avg time: 0.155\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -213.976\tavg length: 86.870\t avg time: 0.151\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -212.481\tavg length: 92.500\t avg time: 0.160\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -202.130\tavg length: 102.650\t avg time: 0.178\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -187.155\tavg length: 105.020\t avg time: 0.182\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -193.503\tavg length: 128.680\t avg time: 0.229\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -179.599\tavg length: 129.580\t avg time: 0.230\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -181.293\tavg length: 145.770\t avg time: 0.265\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -365.115\tavg length: 229.150\t avg time: 0.465\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -225.167\tavg length: 203.020\t avg time: 0.385\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -137.033\tavg length: 175.860\t avg time: 0.331\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -127.824\tavg length: 224.630\t avg time: 0.456\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 6.759 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_1000_0.0006_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -274.229\tavg length: 75.020\t avg time: 0.128\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -219.658\tavg length: 78.770\t avg time: 0.134\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -189.911\tavg length: 87.040\t avg time: 0.150\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -205.023\tavg length: 87.420\t avg time: 0.150\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -172.639\tavg length: 83.920\t avg time: 0.143\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -163.980\tavg length: 82.200\t avg time: 0.139\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -159.726\tavg length: 84.690\t avg time: 0.143\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -154.192\tavg length: 82.840\t avg time: 0.139\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -162.043\tavg length: 81.160\t avg time: 0.137\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -166.550\tavg length: 86.460\t avg time: 0.146\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -176.710\tavg length: 90.500\t avg time: 0.155\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -160.329\tavg length: 101.470\t avg time: 0.175\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -132.145\tavg length: 108.540\t avg time: 0.190\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -138.736\tavg length: 116.620\t avg time: 0.207\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.961 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_250_0.22_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -877.456\tavg length: 82.210\t avg time: 0.149\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -860.869\tavg length: 82.130\t avg time: 0.148\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -855.667\tavg length: 80.500\t avg time: 0.145\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -865.703\tavg length: 80.960\t avg time: 0.146\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -879.145\tavg length: 83.230\t avg time: 0.150\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -885.513\tavg length: 83.070\t avg time: 0.149\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -877.949\tavg length: 84.600\t avg time: 0.153\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -854.858\tavg length: 82.600\t avg time: 0.149\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -864.010\tavg length: 84.170\t avg time: 0.152\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -867.585\tavg length: 83.850\t avg time: 0.151\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -864.149\tavg length: 83.330\t avg time: 0.150\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -872.149\tavg length: 83.540\t avg time: 0.150\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -917.640\tavg length: 85.570\t avg time: 0.154\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -872.906\tavg length: 83.310\t avg time: 0.150\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.756 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_250_0.22_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -857.033\tavg length: 81.300\t avg time: 0.145\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -889.078\tavg length: 85.360\t avg time: 0.152\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -884.377\tavg length: 83.240\t avg time: 0.149\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -880.658\tavg length: 82.390\t avg time: 0.147\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -896.574\tavg length: 84.960\t avg time: 0.152\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -867.383\tavg length: 83.330\t avg time: 0.149\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -854.378\tavg length: 82.590\t avg time: 0.148\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -909.962\tavg length: 88.630\t avg time: 0.159\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -866.079\tavg length: 82.340\t avg time: 0.148\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -856.842\tavg length: 80.860\t avg time: 0.145\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -837.251\tavg length: 81.430\t avg time: 0.146\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -917.349\tavg length: 85.840\t avg time: 0.155\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -879.075\tavg length: 83.650\t avg time: 0.151\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -872.329\tavg length: 82.750\t avg time: 0.149\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.745 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_250_0.22_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -890.415\tavg length: 85.350\t avg time: 0.154\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -882.313\tavg length: 83.340\t avg time: 0.150\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -885.407\tavg length: 84.590\t avg time: 0.152\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -873.576\tavg length: 87.250\t avg time: 0.156\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -855.554\tavg length: 79.740\t avg time: 0.142\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -911.986\tavg length: 86.090\t avg time: 0.154\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -874.380\tavg length: 84.230\t avg time: 0.151\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -892.381\tavg length: 84.810\t avg time: 0.152\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -873.825\tavg length: 82.360\t avg time: 0.147\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -891.948\tavg length: 84.250\t avg time: 0.151\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -864.074\tavg length: 81.220\t avg time: 0.145\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -862.025\tavg length: 82.060\t avg time: 0.147\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -839.619\tavg length: 81.530\t avg time: 0.146\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -878.708\tavg length: 83.500\t avg time: 0.150\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.755 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_250_0.0018_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -241.158\tavg length: 77.350\t avg time: 0.129\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -193.868\tavg length: 78.450\t avg time: 0.129\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -201.742\tavg length: 82.730\t avg time: 0.134\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -287.910\tavg length: 111.060\t avg time: 0.193\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -314.563\tavg length: 113.960\t avg time: 0.199\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -246.204\tavg length: 94.750\t avg time: 0.162\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -243.778\tavg length: 91.050\t avg time: 0.152\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -444.867\tavg length: 132.800\t avg time: 0.241\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -464.489\tavg length: 140.560\t avg time: 0.257\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -525.629\tavg length: 138.600\t avg time: 0.250\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -517.066\tavg length: 124.570\t avg time: 0.220\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -529.964\tavg length: 135.440\t avg time: 0.245\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -652.113\tavg length: 95.820\t avg time: 0.170\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -781.987\tavg length: 80.500\t avg time: 0.147\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.627 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_250_0.0018_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -221.128\tavg length: 79.600\t avg time: 0.135\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -188.721\tavg length: 104.490\t avg time: 0.199\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -150.498\tavg length: 115.070\t avg time: 0.204\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -271.512\tavg length: 115.020\t avg time: 0.210\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -118.716\tavg length: 103.970\t avg time: 0.180\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -102.513\tavg length: 226.990\t avg time: 0.570\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -144.692\tavg length: 250.040\t avg time: 0.670\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -71.094\tavg length: 755.130\t avg time: 2.478\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: 29.031\tavg length: 417.930\t avg time: 1.232\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -47.675\tavg length: 568.340\t avg time: 1.771\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -132.945\tavg length: 407.510\t avg time: 1.206\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -17.709\tavg length: 779.790\t avg time: 2.514\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -14.750\tavg length: 878.280\t avg time: 3.111\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 46.403\tavg length: 893.700\t avg time: 2.939\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 32.754 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_250_0.0018_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -268.632\tavg length: 76.990\t avg time: 0.132\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -222.988\tavg length: 81.830\t avg time: 0.139\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -175.257\tavg length: 86.040\t avg time: 0.151\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -159.390\tavg length: 87.430\t avg time: 0.150\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -192.433\tavg length: 95.540\t avg time: 0.164\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -203.147\tavg length: 104.190\t avg time: 0.182\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -111.853\tavg length: 121.990\t avg time: 0.216\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -94.194\tavg length: 144.570\t avg time: 0.270\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -106.312\tavg length: 171.860\t avg time: 0.343\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -126.164\tavg length: 190.750\t avg time: 0.421\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -57.636\tavg length: 137.060\t avg time: 0.253\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -82.026\tavg length: 175.120\t avg time: 0.352\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -447.964\tavg length: 129.250\t avg time: 0.247\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -115.960\tavg length: 151.300\t avg time: 0.278\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 6.048 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_250_0.0014_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -217.073\tavg length: 69.710\t avg time: 0.115\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -202.465\tavg length: 65.190\t avg time: 0.107\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -157.748\tavg length: 67.510\t avg time: 0.112\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -169.703\tavg length: 70.380\t avg time: 0.116\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -177.240\tavg length: 85.000\t avg time: 0.143\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -129.608\tavg length: 107.910\t avg time: 0.186\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -280.852\tavg length: 133.280\t avg time: 0.245\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -118.734\tavg length: 255.930\t avg time: 0.657\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -44.541\tavg length: 310.170\t avg time: 0.885\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: 28.573\tavg length: 220.090\t avg time: 0.457\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -31.483\tavg length: 670.220\t avg time: 2.117\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: 87.511\tavg length: 558.460\t avg time: 1.610\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: 37.686\tavg length: 726.740\t avg time: 2.267\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -36.876\tavg length: 976.310\t avg time: 3.612\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 24.673 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_250_0.0014_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -233.889\tavg length: 95.600\t avg time: 0.180\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -191.152\tavg length: 91.510\t avg time: 0.158\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -197.772\tavg length: 97.850\t avg time: 0.170\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -186.620\tavg length: 99.690\t avg time: 0.175\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -154.535\tavg length: 124.060\t avg time: 0.222\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -212.373\tavg length: 171.260\t avg time: 0.338\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -260.199\tavg length: 147.950\t avg time: 0.293\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -251.080\tavg length: 112.390\t avg time: 0.220\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -172.738\tavg length: 136.430\t avg time: 0.251\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -144.545\tavg length: 198.280\t avg time: 0.416\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -301.460\tavg length: 236.680\t avg time: 0.517\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -180.915\tavg length: 201.180\t avg time: 0.433\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -174.378\tavg length: 250.750\t avg time: 0.599\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -227.293\tavg length: 209.410\t avg time: 0.476\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 8.133 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_250_0.0014_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -269.712\tavg length: 85.510\t avg time: 0.146\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -205.149\tavg length: 92.380\t avg time: 0.158\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -204.248\tavg length: 101.770\t avg time: 0.176\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -177.350\tavg length: 116.660\t avg time: 0.206\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -248.476\tavg length: 133.590\t avg time: 0.245\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -142.095\tavg length: 139.490\t avg time: 0.256\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -98.050\tavg length: 161.480\t avg time: 0.311\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -206.764\tavg length: 182.350\t avg time: 0.365\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -71.368\tavg length: 161.630\t avg time: 0.316\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -48.299\tavg length: 228.020\t avg time: 0.492\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -120.994\tavg length: 204.450\t avg time: 0.427\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -61.214\tavg length: 157.240\t avg time: 0.289\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -42.190\tavg length: 198.110\t avg time: 0.400\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -64.965\tavg length: 445.690\t avg time: 1.234\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 11.733 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_250_0.001_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -281.136\tavg length: 93.250\t avg time: 0.164\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -171.004\tavg length: 92.460\t avg time: 0.160\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -181.640\tavg length: 145.880\t avg time: 0.282\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -411.185\tavg length: 194.470\t avg time: 0.395\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -372.714\tavg length: 264.280\t avg time: 0.608\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -44.763\tavg length: 609.080\t avg time: 1.805\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -28.865\tavg length: 373.780\t avg time: 0.997\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: 19.774\tavg length: 486.460\t avg time: 1.433\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: 67.831\tavg length: 908.180\t avg time: 2.834\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -15.862\tavg length: 692.370\t avg time: 2.238\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: 73.587\tavg length: 949.310\t avg time: 3.141\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: 63.527\tavg length: 956.190\t avg time: 3.286\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: 39.616\tavg length: 984.160\t avg time: 3.495\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 77.797\tavg length: 997.090\t avg time: 3.402\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 45.361 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_250_0.001_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -253.679\tavg length: 88.880\t avg time: 0.152\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -220.842\tavg length: 89.340\t avg time: 0.152\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -192.585\tavg length: 94.620\t avg time: 0.162\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -177.026\tavg length: 93.230\t avg time: 0.160\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -112.503\tavg length: 132.520\t avg time: 0.255\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -104.552\tavg length: 129.390\t avg time: 0.231\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -222.073\tavg length: 176.340\t avg time: 0.357\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -66.801\tavg length: 199.280\t avg time: 0.443\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -162.537\tavg length: 138.320\t avg time: 0.264\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -180.487\tavg length: 178.450\t avg time: 0.377\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -207.161\tavg length: 307.040\t avg time: 0.797\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -116.905\tavg length: 430.200\t avg time: 1.277\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -89.187\tavg length: 686.810\t avg time: 2.406\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -61.807\tavg length: 851.970\t avg time: 3.063\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 19.586 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_250_0.001_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -293.104\tavg length: 80.230\t avg time: 0.137\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -201.923\tavg length: 87.820\t avg time: 0.150\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -196.409\tavg length: 86.990\t avg time: 0.148\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -198.980\tavg length: 91.320\t avg time: 0.155\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -178.880\tavg length: 88.310\t avg time: 0.150\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -171.266\tavg length: 95.790\t avg time: 0.164\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -155.503\tavg length: 108.920\t avg time: 0.189\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -146.052\tavg length: 121.140\t avg time: 0.217\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -137.266\tavg length: 139.370\t avg time: 0.257\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -111.865\tavg length: 125.550\t avg time: 0.224\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -159.893\tavg length: 168.520\t avg time: 0.326\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -158.039\tavg length: 167.960\t avg time: 0.324\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -98.395\tavg length: 253.580\t avg time: 0.608\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -143.657\tavg length: 233.320\t avg time: 0.534\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 7.139 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_250_0.0006_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -230.234\tavg length: 87.630\t avg time: 0.150\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -180.172\tavg length: 87.380\t avg time: 0.148\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -242.107\tavg length: 85.930\t avg time: 0.145\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -169.164\tavg length: 73.200\t avg time: 0.121\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -160.255\tavg length: 79.040\t avg time: 0.133\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -175.773\tavg length: 85.480\t avg time: 0.144\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -153.456\tavg length: 96.860\t avg time: 0.166\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -139.211\tavg length: 105.080\t avg time: 0.184\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -259.653\tavg length: 151.730\t avg time: 0.285\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -362.062\tavg length: 180.840\t avg time: 0.356\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -486.340\tavg length: 226.850\t avg time: 0.459\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -239.737\tavg length: 202.600\t avg time: 0.407\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -122.107\tavg length: 194.690\t avg time: 0.391\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -80.060\tavg length: 209.120\t avg time: 0.439\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 6.544 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_250_0.0006_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -220.008\tavg length: 84.740\t avg time: 0.143\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -258.650\tavg length: 101.060\t avg time: 0.174\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -398.508\tavg length: 120.100\t avg time: 0.216\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -261.060\tavg length: 145.160\t avg time: 0.277\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -243.009\tavg length: 154.920\t avg time: 0.291\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -409.692\tavg length: 158.910\t avg time: 0.305\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -476.439\tavg length: 208.380\t avg time: 0.420\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -459.270\tavg length: 262.620\t avg time: 0.570\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -331.691\tavg length: 223.960\t avg time: 0.466\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -344.030\tavg length: 278.540\t avg time: 0.643\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -98.027\tavg length: 150.820\t avg time: 0.281\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -66.951\tavg length: 221.510\t avg time: 0.495\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -63.284\tavg length: 227.800\t avg time: 0.496\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -162.077\tavg length: 256.670\t avg time: 0.588\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 9.738 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_6_0.97_250_0.0006_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -261.026\tavg length: 81.960\t avg time: 0.140\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -236.856\tavg length: 84.180\t avg time: 0.143\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -221.924\tavg length: 86.690\t avg time: 0.148\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -225.397\tavg length: 83.960\t avg time: 0.143\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -200.533\tavg length: 93.740\t avg time: 0.161\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -182.289\tavg length: 98.970\t avg time: 0.171\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -170.357\tavg length: 103.780\t avg time: 0.180\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -154.188\tavg length: 107.040\t avg time: 0.187\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -139.804\tavg length: 130.280\t avg time: 0.245\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -157.748\tavg length: 150.220\t avg time: 0.281\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -158.246\tavg length: 132.860\t avg time: 0.241\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -130.389\tavg length: 153.470\t avg time: 0.291\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -134.848\tavg length: 152.740\t avg time: 0.283\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -165.413\tavg length: 163.450\t avg time: 0.319\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 5.468 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_1000_0.22_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -566.673\tavg length: 63.070\t avg time: 0.106\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -580.572\tavg length: 65.010\t avg time: 0.109\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -555.819\tavg length: 61.420\t avg time: 0.102\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -578.873\tavg length: 65.410\t avg time: 0.109\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -565.359\tavg length: 62.660\t avg time: 0.104\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -589.390\tavg length: 64.660\t avg time: 0.107\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -560.426\tavg length: 62.790\t avg time: 0.104\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -581.499\tavg length: 63.650\t avg time: 0.106\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -558.220\tavg length: 62.600\t avg time: 0.103\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -576.936\tavg length: 63.920\t avg time: 0.106\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -565.320\tavg length: 62.890\t avg time: 0.104\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -567.288\tavg length: 62.040\t avg time: 0.103\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -587.545\tavg length: 65.540\t avg time: 0.109\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -552.933\tavg length: 61.830\t avg time: 0.102\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.633 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_1000_0.22_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -854.138\tavg length: 80.770\t avg time: 0.144\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -880.167\tavg length: 86.110\t avg time: 0.154\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -896.654\tavg length: 86.540\t avg time: 0.155\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -867.070\tavg length: 82.810\t avg time: 0.148\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -859.367\tavg length: 84.400\t avg time: 0.150\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -886.809\tavg length: 85.100\t avg time: 0.152\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -902.806\tavg length: 86.230\t avg time: 0.155\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -873.677\tavg length: 84.410\t avg time: 0.152\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -877.372\tavg length: 84.960\t avg time: 0.153\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -882.703\tavg length: 84.170\t avg time: 0.151\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -881.667\tavg length: 84.920\t avg time: 0.153\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -890.267\tavg length: 86.720\t avg time: 0.156\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -905.026\tavg length: 88.160\t avg time: 0.159\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -908.711\tavg length: 88.390\t avg time: 0.159\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.818 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_1000_0.22_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -529.265\tavg length: 63.420\t avg time: 0.105\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -500.985\tavg length: 62.490\t avg time: 0.104\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -515.048\tavg length: 62.470\t avg time: 0.103\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -531.287\tavg length: 64.290\t avg time: 0.106\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -509.538\tavg length: 61.950\t avg time: 0.102\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -532.816\tavg length: 64.780\t avg time: 0.106\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -536.325\tavg length: 65.700\t avg time: 0.108\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -513.473\tavg length: 63.000\t avg time: 0.103\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -536.437\tavg length: 63.590\t avg time: 0.105\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -498.430\tavg length: 61.560\t avg time: 0.102\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -532.533\tavg length: 64.190\t avg time: 0.106\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -518.602\tavg length: 63.100\t avg time: 0.105\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -510.230\tavg length: 63.910\t avg time: 0.106\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -505.364\tavg length: 62.410\t avg time: 0.103\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.621 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_1000_0.0018_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -316.948\tavg length: 84.380\t avg time: 0.145\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -240.849\tavg length: 180.120\t avg time: 0.410\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -305.726\tavg length: 215.600\t avg time: 0.453\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -308.287\tavg length: 196.530\t avg time: 0.439\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -130.591\tavg length: 456.970\t avg time: 1.516\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -64.637\tavg length: 602.390\t avg time: 2.090\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -78.186\tavg length: 786.970\t avg time: 3.007\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -6.816\tavg length: 863.820\t avg time: 3.327\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: 16.143\tavg length: 854.610\t avg time: 3.182\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: 91.218\tavg length: 909.970\t avg time: 3.017\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: 117.415\tavg length: 717.480\t avg time: 2.182\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: 118.676\tavg length: 663.460\t avg time: 2.040\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: 123.291\tavg length: 538.870\t avg time: 1.469\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 124.601\tavg length: 828.660\t avg time: 3.022\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 47.083 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_1000_0.0018_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -251.188\tavg length: 78.150\t avg time: 0.135\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -199.492\tavg length: 74.680\t avg time: 0.129\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -188.725\tavg length: 75.150\t avg time: 0.130\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -177.818\tavg length: 95.190\t avg time: 0.169\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -243.761\tavg length: 99.570\t avg time: 0.178\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -143.249\tavg length: 108.960\t avg time: 0.192\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -120.892\tavg length: 112.330\t avg time: 0.214\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -135.174\tavg length: 100.340\t avg time: 0.174\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -115.054\tavg length: 102.490\t avg time: 0.179\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -122.147\tavg length: 108.760\t avg time: 0.191\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -91.869\tavg length: 123.300\t avg time: 0.221\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -74.895\tavg length: 157.430\t avg time: 0.298\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -96.600\tavg length: 213.250\t avg time: 0.459\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -96.706\tavg length: 252.850\t avg time: 0.622\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 6.424 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_1000_0.0018_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -209.083\tavg length: 88.220\t avg time: 0.153\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -216.696\tavg length: 104.790\t avg time: 0.184\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -204.470\tavg length: 109.230\t avg time: 0.207\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -180.460\tavg length: 99.700\t avg time: 0.173\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -147.712\tavg length: 112.430\t avg time: 0.197\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -133.566\tavg length: 112.780\t avg time: 0.197\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -182.734\tavg length: 152.100\t avg time: 0.296\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -139.516\tavg length: 186.910\t avg time: 0.393\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -116.304\tavg length: 170.230\t avg time: 0.329\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -207.135\tavg length: 237.880\t avg time: 0.506\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -159.469\tavg length: 259.670\t avg time: 0.568\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -81.174\tavg length: 195.610\t avg time: 0.400\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -244.833\tavg length: 348.480\t avg time: 0.886\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -46.689\tavg length: 210.420\t avg time: 0.485\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 10.372 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_1000_0.0014_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -285.200\tavg length: 92.990\t avg time: 0.163\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -200.861\tavg length: 88.420\t avg time: 0.154\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -214.895\tavg length: 90.900\t avg time: 0.158\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -146.294\tavg length: 84.980\t avg time: 0.147\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -129.725\tavg length: 84.990\t avg time: 0.147\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -134.869\tavg length: 96.590\t avg time: 0.168\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -139.572\tavg length: 126.840\t avg time: 0.245\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -108.952\tavg length: 118.550\t avg time: 0.214\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -657.682\tavg length: 183.300\t avg time: 0.402\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -825.116\tavg length: 86.730\t avg time: 0.158\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -812.894\tavg length: 83.860\t avg time: 0.152\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -878.497\tavg length: 84.140\t avg time: 0.153\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -875.991\tavg length: 85.730\t avg time: 0.156\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -902.693\tavg length: 88.310\t avg time: 0.160\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.565 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_1000_0.0014_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -300.515\tavg length: 93.210\t avg time: 0.161\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -228.216\tavg length: 78.860\t avg time: 0.133\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -178.027\tavg length: 76.060\t avg time: 0.127\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -164.789\tavg length: 77.450\t avg time: 0.130\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -154.744\tavg length: 82.060\t avg time: 0.138\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -213.640\tavg length: 89.980\t avg time: 0.154\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -180.907\tavg length: 117.160\t avg time: 0.207\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -147.880\tavg length: 133.840\t avg time: 0.243\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -167.189\tavg length: 120.750\t avg time: 0.214\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -109.745\tavg length: 169.330\t avg time: 0.359\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -49.034\tavg length: 206.530\t avg time: 0.444\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -145.337\tavg length: 352.620\t avg time: 1.021\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -188.085\tavg length: 370.970\t avg time: 1.152\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -288.206\tavg length: 408.140\t avg time: 1.319\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 11.349 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_1000_0.0014_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -229.450\tavg length: 89.840\t avg time: 0.154\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -209.195\tavg length: 85.310\t avg time: 0.146\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -175.983\tavg length: 79.530\t avg time: 0.135\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -194.084\tavg length: 84.600\t avg time: 0.143\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -178.579\tavg length: 97.500\t avg time: 0.173\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -306.285\tavg length: 141.960\t avg time: 0.284\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -405.344\tavg length: 142.380\t avg time: 0.276\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -200.370\tavg length: 127.050\t avg time: 0.245\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -164.103\tavg length: 112.160\t avg time: 0.197\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -149.293\tavg length: 116.000\t avg time: 0.205\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -140.686\tavg length: 111.340\t avg time: 0.196\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -111.252\tavg length: 114.130\t avg time: 0.202\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -115.157\tavg length: 125.770\t avg time: 0.225\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -113.822\tavg length: 130.740\t avg time: 0.235\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 5.144 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_1000_0.001_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -242.608\tavg length: 86.490\t avg time: 0.147\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -243.656\tavg length: 74.070\t avg time: 0.124\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -179.176\tavg length: 78.200\t avg time: 0.133\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -170.972\tavg length: 79.330\t avg time: 0.134\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -152.873\tavg length: 77.300\t avg time: 0.131\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -150.957\tavg length: 106.510\t avg time: 0.187\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -250.918\tavg length: 200.440\t avg time: 0.417\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -96.793\tavg length: 347.690\t avg time: 0.885\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -39.294\tavg length: 666.520\t avg time: 2.199\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -34.559\tavg length: 838.670\t avg time: 2.955\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -24.104\tavg length: 742.140\t avg time: 2.534\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: 3.594\tavg length: 694.990\t avg time: 2.373\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -17.304\tavg length: 818.560\t avg time: 2.846\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -6.633\tavg length: 982.980\t avg time: 3.568\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 35.495 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_1000_0.001_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -245.668\tavg length: 85.030\t avg time: 0.148\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -234.224\tavg length: 89.270\t avg time: 0.159\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -189.510\tavg length: 86.110\t avg time: 0.153\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -191.430\tavg length: 103.860\t avg time: 0.196\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -193.431\tavg length: 97.120\t avg time: 0.173\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -241.615\tavg length: 85.290\t avg time: 0.148\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -161.342\tavg length: 82.780\t avg time: 0.144\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -178.450\tavg length: 86.780\t avg time: 0.150\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -165.897\tavg length: 97.480\t avg time: 0.172\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -186.925\tavg length: 165.330\t avg time: 0.329\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -217.944\tavg length: 267.200\t avg time: 0.646\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -138.985\tavg length: 289.020\t avg time: 0.749\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -83.349\tavg length: 280.580\t avg time: 0.712\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -136.614\tavg length: 401.730\t avg time: 1.206\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 11.811 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_1000_0.001_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -242.665\tavg length: 85.460\t avg time: 0.147\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -241.317\tavg length: 84.170\t avg time: 0.145\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -226.866\tavg length: 86.950\t avg time: 0.151\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -199.527\tavg length: 89.160\t avg time: 0.154\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -166.101\tavg length: 87.050\t avg time: 0.151\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -167.077\tavg length: 89.310\t avg time: 0.155\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -172.635\tavg length: 95.530\t avg time: 0.165\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -159.742\tavg length: 94.100\t avg time: 0.162\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -139.111\tavg length: 97.900\t avg time: 0.171\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -168.795\tavg length: 92.530\t avg time: 0.160\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -155.610\tavg length: 101.580\t avg time: 0.177\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -115.748\tavg length: 106.180\t avg time: 0.187\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -135.721\tavg length: 118.840\t avg time: 0.212\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -111.833\tavg length: 129.400\t avg time: 0.234\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.501 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_1000_0.0006_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -214.254\tavg length: 87.090\t avg time: 0.149\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -235.850\tavg length: 116.550\t avg time: 0.219\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -361.056\tavg length: 144.230\t avg time: 0.270\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -247.991\tavg length: 148.820\t avg time: 0.278\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -137.055\tavg length: 128.240\t avg time: 0.245\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -108.200\tavg length: 116.670\t avg time: 0.209\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -99.405\tavg length: 111.010\t avg time: 0.198\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -100.403\tavg length: 122.070\t avg time: 0.220\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -80.442\tavg length: 157.590\t avg time: 0.304\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -154.062\tavg length: 259.450\t avg time: 0.596\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -25.005\tavg length: 237.800\t avg time: 0.512\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -85.470\tavg length: 332.420\t avg time: 0.922\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -28.263\tavg length: 644.980\t avg time: 2.230\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -29.245\tavg length: 796.590\t avg time: 2.990\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 21.235 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_1000_0.0006_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -318.421\tavg length: 88.250\t avg time: 0.154\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -263.823\tavg length: 90.190\t avg time: 0.162\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -243.763\tavg length: 97.360\t avg time: 0.174\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -245.299\tavg length: 110.120\t avg time: 0.198\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -227.531\tavg length: 100.780\t avg time: 0.177\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -205.186\tavg length: 93.470\t avg time: 0.162\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -203.452\tavg length: 97.010\t avg time: 0.168\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -193.100\tavg length: 96.900\t avg time: 0.168\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -220.761\tavg length: 93.920\t avg time: 0.162\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -190.414\tavg length: 100.140\t avg time: 0.173\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -167.258\tavg length: 100.040\t avg time: 0.172\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -160.611\tavg length: 110.820\t avg time: 0.194\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -167.206\tavg length: 120.520\t avg time: 0.214\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -199.488\tavg length: 149.290\t avg time: 0.277\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.711 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_1000_0.0006_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -234.946\tavg length: 81.690\t avg time: 0.139\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -214.706\tavg length: 81.510\t avg time: 0.141\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -193.444\tavg length: 85.860\t avg time: 0.148\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -211.951\tavg length: 94.940\t avg time: 0.174\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -182.553\tavg length: 85.490\t avg time: 0.145\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -187.535\tavg length: 82.990\t avg time: 0.141\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -190.906\tavg length: 89.840\t avg time: 0.153\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -182.646\tavg length: 88.090\t avg time: 0.150\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -168.897\tavg length: 88.750\t avg time: 0.151\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -186.160\tavg length: 93.330\t avg time: 0.160\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -157.168\tavg length: 100.070\t avg time: 0.174\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -195.116\tavg length: 105.870\t avg time: 0.185\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -166.900\tavg length: 104.130\t avg time: 0.183\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -166.814\tavg length: 105.540\t avg time: 0.185\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.039 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_250_0.22_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -483.824\tavg length: 66.550\t avg time: 0.111\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -472.031\tavg length: 63.110\t avg time: 0.105\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -463.719\tavg length: 63.050\t avg time: 0.105\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -455.270\tavg length: 62.010\t avg time: 0.103\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -487.076\tavg length: 62.650\t avg time: 0.104\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -463.398\tavg length: 64.080\t avg time: 0.106\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -471.501\tavg length: 61.270\t avg time: 0.102\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -464.141\tavg length: 62.280\t avg time: 0.103\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -461.632\tavg length: 62.120\t avg time: 0.103\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -462.420\tavg length: 63.380\t avg time: 0.105\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -462.327\tavg length: 63.050\t avg time: 0.105\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -456.127\tavg length: 61.630\t avg time: 0.102\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -480.532\tavg length: 63.810\t avg time: 0.106\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -475.945\tavg length: 64.620\t avg time: 0.107\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.614 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_250_0.22_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -809.814\tavg length: 81.690\t avg time: 0.146\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -841.057\tavg length: 85.640\t avg time: 0.153\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -827.613\tavg length: 84.220\t avg time: 0.151\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -811.149\tavg length: 81.400\t avg time: 0.146\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -836.494\tavg length: 83.970\t avg time: 0.151\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -810.837\tavg length: 84.490\t avg time: 0.152\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -830.500\tavg length: 83.920\t avg time: 0.151\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -843.433\tavg length: 85.360\t avg time: 0.154\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -840.448\tavg length: 84.430\t avg time: 0.152\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -826.540\tavg length: 85.680\t avg time: 0.155\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -842.124\tavg length: 86.050\t avg time: 0.155\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -847.599\tavg length: 88.710\t avg time: 0.161\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -844.666\tavg length: 84.780\t avg time: 0.153\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -828.484\tavg length: 86.080\t avg time: 0.156\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.817 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_250_0.22_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -523.641\tavg length: 61.890\t avg time: 0.102\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -527.368\tavg length: 62.820\t avg time: 0.104\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -524.831\tavg length: 63.640\t avg time: 0.105\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -526.541\tavg length: 62.570\t avg time: 0.103\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -176.585\tavg length: 66.520\t avg time: 0.101\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -156.133\tavg length: 66.490\t avg time: 0.101\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -165.414\tavg length: 66.410\t avg time: 0.101\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -149.533\tavg length: 65.530\t avg time: 0.099\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -166.963\tavg length: 64.270\t avg time: 0.098\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -167.111\tavg length: 63.800\t avg time: 0.097\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -158.168\tavg length: 63.530\t avg time: 0.097\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -160.274\tavg length: 65.350\t avg time: 0.100\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -159.752\tavg length: 65.290\t avg time: 0.099\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -165.531\tavg length: 66.720\t avg time: 0.102\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.514 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_250_0.0018_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -221.501\tavg length: 88.580\t avg time: 0.152\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -186.215\tavg length: 70.770\t avg time: 0.119\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -190.746\tavg length: 69.690\t avg time: 0.117\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -239.780\tavg length: 66.140\t avg time: 0.110\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -401.450\tavg length: 63.840\t avg time: 0.106\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -474.076\tavg length: 63.730\t avg time: 0.106\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -468.570\tavg length: 63.000\t avg time: 0.105\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -476.840\tavg length: 63.980\t avg time: 0.107\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -467.593\tavg length: 63.390\t avg time: 0.106\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -469.841\tavg length: 62.140\t avg time: 0.104\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -469.238\tavg length: 63.140\t avg time: 0.105\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -459.600\tavg length: 62.000\t avg time: 0.103\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -456.350\tavg length: 63.310\t avg time: 0.106\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -479.561\tavg length: 62.740\t avg time: 0.105\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.764 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_250_0.0018_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -269.669\tavg length: 85.280\t avg time: 0.145\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -196.948\tavg length: 85.490\t avg time: 0.145\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -181.534\tavg length: 92.050\t avg time: 0.158\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -198.141\tavg length: 96.690\t avg time: 0.167\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -182.309\tavg length: 89.940\t avg time: 0.154\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -167.122\tavg length: 82.330\t avg time: 0.140\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -167.476\tavg length: 93.230\t avg time: 0.159\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -151.367\tavg length: 94.930\t avg time: 0.163\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -152.373\tavg length: 94.090\t avg time: 0.162\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -147.657\tavg length: 115.760\t avg time: 0.205\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -126.257\tavg length: 112.810\t avg time: 0.197\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -159.502\tavg length: 112.850\t avg time: 0.201\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -235.200\tavg length: 158.270\t avg time: 0.306\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -353.758\tavg length: 198.610\t avg time: 0.476\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 5.425 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_250_0.0018_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -265.692\tavg length: 86.740\t avg time: 0.150\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -231.304\tavg length: 92.160\t avg time: 0.157\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -183.890\tavg length: 96.450\t avg time: 0.164\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -207.452\tavg length: 118.740\t avg time: 0.207\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -196.117\tavg length: 114.770\t avg time: 0.199\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -156.923\tavg length: 95.460\t avg time: 0.164\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -216.492\tavg length: 141.900\t avg time: 0.259\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -195.601\tavg length: 160.480\t avg time: 0.305\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -167.642\tavg length: 160.490\t avg time: 0.316\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -152.345\tavg length: 180.960\t avg time: 0.366\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -92.095\tavg length: 245.230\t avg time: 0.598\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -59.676\tavg length: 306.380\t avg time: 0.800\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -41.781\tavg length: 252.630\t avg time: 0.595\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -13.966\tavg length: 195.150\t avg time: 0.409\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 8.988 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_250_0.0014_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -249.149\tavg length: 82.130\t avg time: 0.143\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -212.567\tavg length: 70.850\t avg time: 0.120\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -198.395\tavg length: 71.140\t avg time: 0.118\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -170.387\tavg length: 71.340\t avg time: 0.120\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -238.998\tavg length: 77.350\t avg time: 0.132\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -166.537\tavg length: 104.650\t avg time: 0.187\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -31.710\tavg length: 260.660\t avg time: 0.681\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -33.496\tavg length: 415.330\t avg time: 1.217\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -21.346\tavg length: 700.470\t avg time: 2.362\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -11.897\tavg length: 426.920\t avg time: 1.230\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: 59.211\tavg length: 405.140\t avg time: 1.083\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: 35.794\tavg length: 466.580\t avg time: 1.329\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: 32.694\tavg length: 474.210\t avg time: 1.401\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 45.912\tavg length: 227.820\t avg time: 0.467\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 19.005 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_250_0.0014_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -272.965\tavg length: 96.570\t avg time: 0.167\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -222.507\tavg length: 88.610\t avg time: 0.151\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -227.591\tavg length: 88.490\t avg time: 0.150\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -176.654\tavg length: 85.210\t avg time: 0.144\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -167.027\tavg length: 104.230\t avg time: 0.181\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -155.208\tavg length: 95.820\t avg time: 0.166\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -154.477\tavg length: 83.620\t avg time: 0.142\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -187.572\tavg length: 78.830\t avg time: 0.141\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -168.115\tavg length: 68.410\t avg time: 0.113\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -150.125\tavg length: 68.010\t avg time: 0.113\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -166.789\tavg length: 68.250\t avg time: 0.113\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -143.007\tavg length: 78.280\t avg time: 0.131\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -137.722\tavg length: 77.870\t avg time: 0.131\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -101.671\tavg length: 120.700\t avg time: 0.228\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.252 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_250_0.0014_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -242.237\tavg length: 82.500\t avg time: 0.140\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -204.790\tavg length: 85.520\t avg time: 0.144\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -201.919\tavg length: 82.480\t avg time: 0.139\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -196.808\tavg length: 81.020\t avg time: 0.135\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -210.793\tavg length: 85.290\t avg time: 0.143\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -201.834\tavg length: 84.580\t avg time: 0.142\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -196.941\tavg length: 87.050\t avg time: 0.147\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -186.456\tavg length: 89.490\t avg time: 0.150\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -178.764\tavg length: 94.720\t avg time: 0.164\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -215.291\tavg length: 94.980\t avg time: 0.156\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -227.470\tavg length: 102.080\t avg time: 0.168\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -199.625\tavg length: 96.090\t avg time: 0.157\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -157.564\tavg length: 89.350\t avg time: 0.146\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -174.268\tavg length: 95.450\t avg time: 0.156\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.722 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_250_0.001_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -249.652\tavg length: 84.160\t avg time: 0.144\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -257.887\tavg length: 87.980\t avg time: 0.152\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -274.167\tavg length: 80.850\t avg time: 0.138\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -153.242\tavg length: 82.930\t avg time: 0.142\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -166.413\tavg length: 86.760\t avg time: 0.150\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -136.478\tavg length: 115.450\t avg time: 0.208\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -164.523\tavg length: 180.340\t avg time: 0.383\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -87.984\tavg length: 141.270\t avg time: 0.261\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -58.111\tavg length: 174.950\t avg time: 0.361\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -5.886\tavg length: 250.070\t avg time: 0.627\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -61.647\tavg length: 438.220\t avg time: 1.302\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -45.895\tavg length: 527.190\t avg time: 1.809\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -31.035\tavg length: 419.900\t avg time: 1.371\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -9.843\tavg length: 545.100\t avg time: 2.027\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 19.434 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_250_0.001_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -233.524\tavg length: 88.350\t avg time: 0.155\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -206.456\tavg length: 86.770\t avg time: 0.150\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -216.243\tavg length: 84.940\t avg time: 0.147\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -235.952\tavg length: 90.180\t avg time: 0.155\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -160.404\tavg length: 97.950\t avg time: 0.188\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -189.966\tavg length: 99.310\t avg time: 0.172\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -208.618\tavg length: 136.710\t avg time: 0.251\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -178.834\tavg length: 177.220\t avg time: 0.369\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -200.614\tavg length: 201.920\t avg time: 0.417\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -430.046\tavg length: 211.580\t avg time: 0.456\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -266.005\tavg length: 200.980\t avg time: 0.415\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -54.960\tavg length: 166.990\t avg time: 0.318\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -65.265\tavg length: 189.570\t avg time: 0.400\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -32.152\tavg length: 267.860\t avg time: 0.680\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 7.768 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_250_0.001_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -218.698\tavg length: 82.490\t avg time: 0.141\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -194.784\tavg length: 81.520\t avg time: 0.139\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -234.271\tavg length: 76.620\t avg time: 0.130\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -231.563\tavg length: 90.040\t avg time: 0.154\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -201.876\tavg length: 81.870\t avg time: 0.139\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -165.385\tavg length: 79.860\t avg time: 0.135\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -176.093\tavg length: 81.240\t avg time: 0.137\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -165.051\tavg length: 77.380\t avg time: 0.131\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -194.285\tavg length: 83.150\t avg time: 0.145\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -157.278\tavg length: 99.010\t avg time: 0.174\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -163.090\tavg length: 100.140\t avg time: 0.173\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -157.852\tavg length: 101.860\t avg time: 0.176\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -160.945\tavg length: 102.960\t avg time: 0.179\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -159.614\tavg length: 110.900\t avg time: 0.194\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.923 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_250_0.0006_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -274.135\tavg length: 80.080\t avg time: 0.137\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -288.385\tavg length: 94.920\t avg time: 0.166\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -207.106\tavg length: 104.980\t avg time: 0.187\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -173.953\tavg length: 98.540\t avg time: 0.173\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -147.208\tavg length: 104.950\t avg time: 0.186\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -167.398\tavg length: 117.530\t avg time: 0.211\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -143.007\tavg length: 122.950\t avg time: 0.223\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -128.531\tavg length: 147.810\t avg time: 0.277\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -92.967\tavg length: 156.630\t avg time: 0.309\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -90.509\tavg length: 186.620\t avg time: 0.407\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -163.898\tavg length: 245.230\t avg time: 0.578\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -273.320\tavg length: 164.290\t avg time: 0.380\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -213.651\tavg length: 281.470\t avg time: 0.766\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -189.567\tavg length: 268.270\t avg time: 0.737\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 8.998 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_250_0.0006_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -247.215\tavg length: 90.830\t avg time: 0.157\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -199.616\tavg length: 98.740\t avg time: 0.187\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -178.471\tavg length: 86.830\t avg time: 0.149\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -166.739\tavg length: 90.980\t avg time: 0.156\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -169.160\tavg length: 95.840\t avg time: 0.166\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -171.752\tavg length: 95.050\t avg time: 0.165\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -148.052\tavg length: 92.900\t avg time: 0.160\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -160.786\tavg length: 89.610\t avg time: 0.154\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -172.970\tavg length: 85.160\t avg time: 0.147\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -150.039\tavg length: 92.010\t avg time: 0.159\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -120.076\tavg length: 109.210\t avg time: 0.201\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -118.406\tavg length: 109.620\t avg time: 0.192\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -113.275\tavg length: 122.280\t avg time: 0.217\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -96.622\tavg length: 119.160\t avg time: 0.226\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.408 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.99_250_0.0006_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -244.198\tavg length: 81.130\t avg time: 0.137\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -200.647\tavg length: 90.390\t avg time: 0.165\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -201.780\tavg length: 86.980\t avg time: 0.147\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -196.736\tavg length: 82.320\t avg time: 0.138\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -217.398\tavg length: 86.110\t avg time: 0.146\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -184.087\tavg length: 90.390\t avg time: 0.155\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -173.940\tavg length: 92.720\t avg time: 0.159\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -187.882\tavg length: 99.690\t avg time: 0.171\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -176.037\tavg length: 100.240\t avg time: 0.171\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -179.922\tavg length: 103.270\t avg time: 0.178\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -165.973\tavg length: 100.670\t avg time: 0.173\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -182.483\tavg length: 106.160\t avg time: 0.184\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -186.530\tavg length: 130.530\t avg time: 0.242\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -168.984\tavg length: 125.800\t avg time: 0.225\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.319 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_1000_0.22_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -405.862\tavg length: 134.740\t avg time: 0.247\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -414.693\tavg length: 138.190\t avg time: 0.256\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -409.827\tavg length: 146.240\t avg time: 0.278\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -403.260\tavg length: 134.010\t avg time: 0.247\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -416.311\tavg length: 140.880\t avg time: 0.270\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -380.953\tavg length: 136.550\t avg time: 0.252\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -417.187\tavg length: 131.670\t avg time: 0.239\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -400.575\tavg length: 134.140\t avg time: 0.243\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -379.658\tavg length: 128.460\t avg time: 0.233\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -390.999\tavg length: 127.670\t avg time: 0.237\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -382.962\tavg length: 128.350\t avg time: 0.245\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -397.801\tavg length: 138.850\t avg time: 0.260\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -393.467\tavg length: 131.740\t avg time: 0.247\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -395.610\tavg length: 126.670\t avg time: 0.233\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 6.300 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_1000_0.22_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -513.012\tavg length: 64.070\t avg time: 0.105\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -520.993\tavg length: 61.070\t avg time: 0.101\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -514.842\tavg length: 63.460\t avg time: 0.104\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -535.708\tavg length: 64.450\t avg time: 0.106\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -538.816\tavg length: 64.290\t avg time: 0.106\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -510.681\tavg length: 63.020\t avg time: 0.104\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -533.808\tavg length: 64.820\t avg time: 0.107\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -524.631\tavg length: 64.120\t avg time: 0.107\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -505.394\tavg length: 61.460\t avg time: 0.103\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -527.342\tavg length: 63.030\t avg time: 0.105\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -500.844\tavg length: 61.620\t avg time: 0.103\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -511.423\tavg length: 64.170\t avg time: 0.107\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -539.897\tavg length: 62.880\t avg time: 0.106\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -517.811\tavg length: 62.860\t avg time: 0.105\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.622 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_1000_0.22_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -931.489\tavg length: 81.050\t avg time: 0.146\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -1040.966\tavg length: 85.730\t avg time: 0.155\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -963.366\tavg length: 78.780\t avg time: 0.142\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -1014.658\tavg length: 84.570\t avg time: 0.152\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -991.884\tavg length: 82.350\t avg time: 0.148\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -945.707\tavg length: 78.700\t avg time: 0.140\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -971.644\tavg length: 79.920\t avg time: 0.143\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -950.046\tavg length: 78.820\t avg time: 0.140\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -974.732\tavg length: 80.690\t avg time: 0.144\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -957.100\tavg length: 79.850\t avg time: 0.143\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -983.243\tavg length: 81.190\t avg time: 0.145\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -970.359\tavg length: 79.810\t avg time: 0.142\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -981.949\tavg length: 80.870\t avg time: 0.144\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -970.348\tavg length: 81.150\t avg time: 0.145\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.623 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_1000_0.0018_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -353.448\tavg length: 92.290\t avg time: 0.161\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -214.863\tavg length: 137.390\t avg time: 0.266\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -177.287\tavg length: 376.320\t avg time: 1.257\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -111.135\tavg length: 696.680\t avg time: 2.847\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -53.758\tavg length: 527.740\t avg time: 2.216\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -31.583\tavg length: 648.240\t avg time: 2.843\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: 1.670\tavg length: 798.190\t avg time: 3.787\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -139.113\tavg length: 388.500\t avg time: 1.682\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -585.275\tavg length: 66.270\t avg time: 0.119\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -637.204\tavg length: 69.280\t avg time: 0.128\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -741.125\tavg length: 75.450\t avg time: 0.139\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -765.033\tavg length: 75.140\t avg time: 0.138\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -736.446\tavg length: 74.500\t avg time: 0.136\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -784.626\tavg length: 78.840\t avg time: 0.144\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 26.673 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_1000_0.0018_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -244.133\tavg length: 88.820\t avg time: 0.153\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -198.158\tavg length: 143.910\t avg time: 0.291\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -190.441\tavg length: 177.980\t avg time: 0.347\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -143.584\tavg length: 165.170\t avg time: 0.333\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -295.442\tavg length: 252.420\t avg time: 0.596\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -53.574\tavg length: 434.830\t avg time: 1.383\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -45.491\tavg length: 536.560\t avg time: 1.788\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -25.452\tavg length: 759.150\t avg time: 2.790\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -49.041\tavg length: 758.620\t avg time: 2.812\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -37.817\tavg length: 868.790\t avg time: 3.254\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: 17.778\tavg length: 805.010\t avg time: 2.746\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: 62.513\tavg length: 863.260\t avg time: 2.775\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: 67.848\tavg length: 923.640\t avg time: 3.050\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 95.438\tavg length: 914.360\t avg time: 2.847\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 46.720 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_1000_0.0018_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -238.377\tavg length: 85.460\t avg time: 0.152\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -227.061\tavg length: 91.140\t avg time: 0.167\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -206.677\tavg length: 100.690\t avg time: 0.184\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -178.803\tavg length: 116.070\t avg time: 0.213\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -139.581\tavg length: 115.080\t avg time: 0.210\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -185.723\tavg length: 152.970\t avg time: 0.285\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -157.753\tavg length: 165.290\t avg time: 0.315\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -107.283\tavg length: 185.910\t avg time: 0.391\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -134.870\tavg length: 266.620\t avg time: 0.675\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -101.874\tavg length: 383.250\t avg time: 1.114\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -141.238\tavg length: 555.820\t avg time: 1.814\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -120.333\tavg length: 758.690\t avg time: 2.746\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -83.734\tavg length: 886.710\t avg time: 3.326\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -73.753\tavg length: 871.010\t avg time: 3.278\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 29.619 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_1000_0.0014_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -234.829\tavg length: 80.150\t avg time: 0.140\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -218.725\tavg length: 86.870\t avg time: 0.156\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -184.336\tavg length: 82.120\t avg time: 0.145\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -229.839\tavg length: 137.920\t avg time: 0.276\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -260.630\tavg length: 462.870\t avg time: 1.398\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -12.959\tavg length: 698.420\t avg time: 2.362\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -8.808\tavg length: 916.250\t avg time: 3.259\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: 48.157\tavg length: 882.560\t avg time: 2.839\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: 103.535\tavg length: 925.320\t avg time: 2.795\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: 87.011\tavg length: 968.200\t avg time: 3.105\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: 98.550\tavg length: 932.490\t avg time: 2.942\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: 95.240\tavg length: 977.440\t avg time: 3.078\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: 118.321\tavg length: 976.230\t avg time: 2.873\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 116.219\tavg length: 971.210\t avg time: 2.968\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 52.171 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_1000_0.0014_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -279.902\tavg length: 91.490\t avg time: 0.160\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -260.955\tavg length: 98.920\t avg time: 0.179\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -189.467\tavg length: 104.750\t avg time: 0.189\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -141.958\tavg length: 124.760\t avg time: 0.231\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -213.558\tavg length: 184.730\t avg time: 0.377\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -171.723\tavg length: 192.880\t avg time: 0.412\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -118.211\tavg length: 157.370\t avg time: 0.297\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -109.892\tavg length: 222.000\t avg time: 0.484\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -174.203\tavg length: 272.400\t avg time: 0.698\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -128.718\tavg length: 486.430\t avg time: 1.472\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -71.937\tavg length: 569.750\t avg time: 1.855\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -118.903\tavg length: 816.010\t avg time: 2.925\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -30.091\tavg length: 891.750\t avg time: 3.305\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -4.741\tavg length: 761.860\t avg time: 2.712\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 29.874 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_1000_0.0014_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -228.837\tavg length: 85.960\t avg time: 0.150\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -196.793\tavg length: 95.790\t avg time: 0.170\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -187.173\tavg length: 104.630\t avg time: 0.187\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -174.719\tavg length: 122.440\t avg time: 0.234\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -120.168\tavg length: 131.120\t avg time: 0.249\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -166.789\tavg length: 206.120\t avg time: 0.418\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -181.310\tavg length: 320.060\t avg time: 0.797\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -123.389\tavg length: 647.870\t avg time: 2.014\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -41.886\tavg length: 741.310\t avg time: 2.596\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -32.406\tavg length: 824.670\t avg time: 2.861\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -8.085\tavg length: 843.250\t avg time: 2.952\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -6.829\tavg length: 716.360\t avg time: 2.420\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: 6.040\tavg length: 848.860\t avg time: 2.910\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 40.296\tavg length: 776.600\t avg time: 2.494\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 38.707 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_1000_0.001_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -212.840\tavg length: 83.160\t avg time: 0.142\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -180.711\tavg length: 79.580\t avg time: 0.134\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -171.730\tavg length: 76.770\t avg time: 0.127\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -174.227\tavg length: 77.360\t avg time: 0.126\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -182.546\tavg length: 80.260\t avg time: 0.131\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -206.797\tavg length: 106.500\t avg time: 0.185\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -221.157\tavg length: 98.490\t avg time: 0.164\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -197.802\tavg length: 98.210\t avg time: 0.163\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -219.462\tavg length: 122.830\t avg time: 0.231\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -230.253\tavg length: 131.800\t avg time: 0.240\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -241.650\tavg length: 114.280\t avg time: 0.195\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -325.060\tavg length: 132.730\t avg time: 0.242\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -184.551\tavg length: 134.340\t avg time: 0.243\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -188.715\tavg length: 180.920\t avg time: 0.410\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 5.153 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_1000_0.001_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -303.693\tavg length: 86.490\t avg time: 0.152\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -232.270\tavg length: 102.410\t avg time: 0.181\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -179.875\tavg length: 126.130\t avg time: 0.233\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -160.567\tavg length: 133.250\t avg time: 0.247\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -124.249\tavg length: 147.600\t avg time: 0.287\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -202.585\tavg length: 228.520\t avg time: 0.548\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -351.553\tavg length: 332.000\t avg time: 0.895\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -317.200\tavg length: 324.970\t avg time: 0.861\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -208.465\tavg length: 236.940\t avg time: 0.573\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -231.790\tavg length: 252.180\t avg time: 0.626\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -61.168\tavg length: 269.500\t avg time: 0.673\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -166.651\tavg length: 450.530\t avg time: 1.429\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -149.134\tavg length: 459.160\t avg time: 1.534\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -37.202\tavg length: 494.530\t avg time: 1.686\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 20.449 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_1000_0.001_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -246.684\tavg length: 91.840\t avg time: 0.161\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -244.670\tavg length: 97.720\t avg time: 0.173\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -225.528\tavg length: 115.510\t avg time: 0.217\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -204.415\tavg length: 106.890\t avg time: 0.187\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -219.854\tavg length: 128.640\t avg time: 0.233\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -195.179\tavg length: 140.170\t avg time: 0.252\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -228.292\tavg length: 132.530\t avg time: 0.239\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -260.362\tavg length: 162.920\t avg time: 0.306\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -180.526\tavg length: 151.160\t avg time: 0.284\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -163.818\tavg length: 161.290\t avg time: 0.299\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -184.109\tavg length: 158.780\t avg time: 0.295\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -222.788\tavg length: 218.730\t avg time: 0.436\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -190.696\tavg length: 291.880\t avg time: 0.717\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -323.235\tavg length: 351.530\t avg time: 0.852\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 9.854 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_1000_0.0006_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -254.252\tavg length: 85.780\t avg time: 0.150\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -247.773\tavg length: 83.190\t avg time: 0.145\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -223.606\tavg length: 101.320\t avg time: 0.179\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -214.187\tavg length: 122.470\t avg time: 0.223\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -207.135\tavg length: 164.210\t avg time: 0.333\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -175.512\tavg length: 196.480\t avg time: 0.406\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -157.463\tavg length: 214.360\t avg time: 0.455\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -113.777\tavg length: 224.690\t avg time: 0.480\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -222.062\tavg length: 381.840\t avg time: 1.022\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -31.700\tavg length: 456.740\t avg time: 1.378\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -17.447\tavg length: 586.290\t avg time: 1.874\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: 10.770\tavg length: 613.850\t avg time: 1.940\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: 30.227\tavg length: 758.630\t avg time: 2.425\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 60.447\tavg length: 733.400\t avg time: 2.212\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 26.147 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_1000_0.0006_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -276.141\tavg length: 84.180\t avg time: 0.147\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -210.811\tavg length: 82.680\t avg time: 0.146\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -211.197\tavg length: 86.600\t avg time: 0.152\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -193.628\tavg length: 89.370\t avg time: 0.156\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -196.327\tavg length: 96.110\t avg time: 0.180\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -173.632\tavg length: 91.250\t avg time: 0.158\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -158.582\tavg length: 91.510\t avg time: 0.159\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -152.529\tavg length: 112.880\t avg time: 0.200\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -157.637\tavg length: 115.370\t avg time: 0.206\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -135.622\tavg length: 141.030\t avg time: 0.272\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -147.794\tavg length: 148.310\t avg time: 0.289\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -167.196\tavg length: 196.980\t avg time: 0.401\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -138.692\tavg length: 189.680\t avg time: 0.373\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -140.728\tavg length: 231.880\t avg time: 0.511\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 6.372 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_1000_0.0006_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -238.169\tavg length: 84.920\t avg time: 0.146\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -217.794\tavg length: 85.150\t avg time: 0.145\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -216.883\tavg length: 95.070\t avg time: 0.174\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -196.349\tavg length: 88.040\t avg time: 0.150\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -188.153\tavg length: 90.220\t avg time: 0.153\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -183.990\tavg length: 91.340\t avg time: 0.156\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -176.979\tavg length: 92.210\t avg time: 0.158\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -173.679\tavg length: 91.870\t avg time: 0.157\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -174.408\tavg length: 94.890\t avg time: 0.164\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -159.264\tavg length: 96.090\t avg time: 0.164\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -150.494\tavg length: 95.320\t avg time: 0.163\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -144.482\tavg length: 100.910\t avg time: 0.174\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -152.783\tavg length: 101.490\t avg time: 0.175\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -144.981\tavg length: 106.750\t avg time: 0.186\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.091 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_250_0.22_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -162.765\tavg length: 65.430\t avg time: 0.100\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -161.331\tavg length: 65.180\t avg time: 0.100\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -163.758\tavg length: 65.520\t avg time: 0.100\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -168.781\tavg length: 66.840\t avg time: 0.102\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -151.496\tavg length: 66.110\t avg time: 0.101\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -156.897\tavg length: 63.680\t avg time: 0.098\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -153.644\tavg length: 64.030\t avg time: 0.098\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -159.893\tavg length: 66.160\t avg time: 0.101\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -158.998\tavg length: 65.740\t avg time: 0.101\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -161.641\tavg length: 65.120\t avg time: 0.100\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -150.124\tavg length: 66.140\t avg time: 0.102\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -167.632\tavg length: 66.380\t avg time: 0.102\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -153.334\tavg length: 63.400\t avg time: 0.097\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -153.850\tavg length: 66.220\t avg time: 0.102\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.514 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_250_0.22_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -501.021\tavg length: 63.850\t avg time: 0.106\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -481.191\tavg length: 63.670\t avg time: 0.105\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -463.449\tavg length: 62.730\t avg time: 0.103\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -479.918\tavg length: 62.950\t avg time: 0.103\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -493.831\tavg length: 65.950\t avg time: 0.109\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -459.114\tavg length: 63.210\t avg time: 0.104\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -466.368\tavg length: 62.950\t avg time: 0.103\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -465.190\tavg length: 62.170\t avg time: 0.103\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -467.296\tavg length: 62.410\t avg time: 0.103\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -467.572\tavg length: 63.650\t avg time: 0.105\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -489.974\tavg length: 65.840\t avg time: 0.108\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -459.801\tavg length: 63.120\t avg time: 0.104\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -489.190\tavg length: 65.080\t avg time: 0.107\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -451.194\tavg length: 62.500\t avg time: 0.103\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.628 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_250_0.22_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -792.673\tavg length: 84.120\t avg time: 0.150\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -859.717\tavg length: 82.970\t avg time: 0.151\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -878.208\tavg length: 83.040\t avg time: 0.150\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -851.928\tavg length: 79.740\t avg time: 0.144\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -858.661\tavg length: 79.630\t avg time: 0.144\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -904.850\tavg length: 84.240\t avg time: 0.152\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -888.900\tavg length: 85.070\t avg time: 0.153\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -901.411\tavg length: 82.570\t avg time: 0.148\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -899.835\tavg length: 84.730\t avg time: 0.152\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -880.851\tavg length: 81.330\t avg time: 0.145\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -873.178\tavg length: 81.620\t avg time: 0.145\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -881.150\tavg length: 81.690\t avg time: 0.146\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -871.291\tavg length: 81.530\t avg time: 0.145\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -926.819\tavg length: 85.270\t avg time: 0.152\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.707 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_250_0.0018_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -258.930\tavg length: 96.460\t avg time: 0.169\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -524.869\tavg length: 128.360\t avg time: 0.250\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -177.161\tavg length: 83.000\t avg time: 0.141\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -226.283\tavg length: 84.330\t avg time: 0.145\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -128.663\tavg length: 117.690\t avg time: 0.210\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -227.462\tavg length: 144.240\t avg time: 0.285\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -398.906\tavg length: 65.200\t avg time: 0.109\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -330.889\tavg length: 65.530\t avg time: 0.109\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -119.814\tavg length: 74.770\t avg time: 0.126\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -84.924\tavg length: 88.940\t avg time: 0.152\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -520.619\tavg length: 157.110\t avg time: 0.303\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -322.884\tavg length: 110.510\t avg time: 0.197\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -179.733\tavg length: 80.110\t avg time: 0.135\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -193.035\tavg length: 76.290\t avg time: 0.128\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.312 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_250_0.0018_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -226.183\tavg length: 87.300\t avg time: 0.148\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -168.283\tavg length: 83.120\t avg time: 0.140\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -156.511\tavg length: 84.640\t avg time: 0.143\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -163.133\tavg length: 85.410\t avg time: 0.145\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -148.524\tavg length: 98.930\t avg time: 0.170\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -209.943\tavg length: 179.200\t avg time: 0.352\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -185.661\tavg length: 143.870\t avg time: 0.295\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -145.708\tavg length: 193.390\t avg time: 0.455\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -63.753\tavg length: 159.900\t avg time: 0.320\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -50.207\tavg length: 200.550\t avg time: 0.451\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -79.318\tavg length: 399.070\t avg time: 1.173\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -689.144\tavg length: 395.360\t avg time: 1.242\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -632.792\tavg length: 160.260\t avg time: 0.335\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -273.505\tavg length: 220.170\t avg time: 0.479\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 10.371 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_250_0.0018_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -219.479\tavg length: 86.460\t avg time: 0.148\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -200.483\tavg length: 93.870\t avg time: 0.162\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -309.131\tavg length: 136.120\t avg time: 0.249\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -640.655\tavg length: 135.730\t avg time: 0.248\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -623.154\tavg length: 95.020\t avg time: 0.168\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -331.393\tavg length: 91.720\t avg time: 0.158\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -161.232\tavg length: 84.570\t avg time: 0.141\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -151.311\tavg length: 97.860\t avg time: 0.161\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -203.047\tavg length: 131.230\t avg time: 0.226\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -208.093\tavg length: 197.560\t avg time: 0.397\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -191.323\tavg length: 266.080\t avg time: 0.616\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -111.531\tavg length: 223.010\t avg time: 0.501\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -72.026\tavg length: 218.970\t avg time: 0.503\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -163.005\tavg length: 332.980\t avg time: 0.826\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 9.944 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_250_0.0014_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -207.895\tavg length: 69.900\t avg time: 0.115\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -200.104\tavg length: 77.940\t avg time: 0.132\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -178.721\tavg length: 67.750\t avg time: 0.111\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -182.571\tavg length: 68.130\t avg time: 0.109\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -165.943\tavg length: 81.980\t avg time: 0.137\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -150.231\tavg length: 119.490\t avg time: 0.213\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -145.687\tavg length: 188.610\t avg time: 0.394\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -86.656\tavg length: 186.910\t avg time: 0.400\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -76.714\tavg length: 804.550\t avg time: 3.017\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -161.183\tavg length: 467.660\t avg time: 1.658\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -468.093\tavg length: 139.670\t avg time: 0.280\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -441.249\tavg length: 96.780\t avg time: 0.180\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -422.739\tavg length: 96.200\t avg time: 0.179\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -394.431\tavg length: 116.210\t avg time: 0.219\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 12.278 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_250_0.0014_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -315.869\tavg length: 83.880\t avg time: 0.143\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -230.698\tavg length: 84.680\t avg time: 0.143\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -213.500\tavg length: 109.620\t avg time: 0.202\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -253.458\tavg length: 140.270\t avg time: 0.279\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -238.384\tavg length: 171.080\t avg time: 0.331\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -416.299\tavg length: 171.270\t avg time: 0.349\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -232.225\tavg length: 176.450\t avg time: 0.389\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -130.803\tavg length: 93.810\t avg time: 0.162\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -138.198\tavg length: 173.560\t avg time: 0.370\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -213.206\tavg length: 385.060\t avg time: 1.068\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -146.567\tavg length: 373.460\t avg time: 1.186\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -109.575\tavg length: 179.850\t avg time: 0.530\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -71.692\tavg length: 97.980\t avg time: 0.175\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -108.976\tavg length: 86.300\t avg time: 0.148\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 9.940 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_250_0.0014_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -231.656\tavg length: 81.050\t avg time: 0.137\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -234.360\tavg length: 83.320\t avg time: 0.141\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -199.622\tavg length: 85.190\t avg time: 0.145\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -198.992\tavg length: 88.720\t avg time: 0.152\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -175.867\tavg length: 111.890\t avg time: 0.222\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -183.459\tavg length: 92.740\t avg time: 0.160\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -167.491\tavg length: 112.730\t avg time: 0.200\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -164.430\tavg length: 125.290\t avg time: 0.227\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -146.976\tavg length: 172.850\t avg time: 0.367\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -114.099\tavg length: 168.870\t avg time: 0.361\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -206.315\tavg length: 199.980\t avg time: 0.423\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -171.835\tavg length: 189.650\t avg time: 0.399\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -324.016\tavg length: 292.730\t avg time: 0.735\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -155.974\tavg length: 245.190\t avg time: 0.611\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 8.223 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_250_0.001_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -245.582\tavg length: 82.570\t avg time: 0.144\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -196.946\tavg length: 100.370\t avg time: 0.178\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -218.222\tavg length: 113.340\t avg time: 0.203\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -204.144\tavg length: 126.440\t avg time: 0.230\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -216.831\tavg length: 145.090\t avg time: 0.292\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -207.276\tavg length: 143.920\t avg time: 0.273\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -133.634\tavg length: 126.400\t avg time: 0.241\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -90.082\tavg length: 140.490\t avg time: 0.257\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -332.622\tavg length: 183.730\t avg time: 0.403\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -438.105\tavg length: 184.550\t avg time: 0.379\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -388.738\tavg length: 142.300\t avg time: 0.270\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -217.803\tavg length: 164.390\t avg time: 0.318\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -262.075\tavg length: 229.510\t avg time: 0.501\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -141.868\tavg length: 341.310\t avg time: 0.915\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 10.825 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_250_0.001_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -230.833\tavg length: 86.110\t avg time: 0.149\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -171.978\tavg length: 90.630\t avg time: 0.157\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -173.044\tavg length: 81.570\t avg time: 0.139\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -187.855\tavg length: 85.770\t avg time: 0.147\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -163.723\tavg length: 95.040\t avg time: 0.165\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -175.654\tavg length: 96.420\t avg time: 0.168\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -162.172\tavg length: 119.070\t avg time: 0.212\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -169.537\tavg length: 149.480\t avg time: 0.281\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -213.380\tavg length: 135.470\t avg time: 0.251\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -66.753\tavg length: 167.710\t avg time: 0.338\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -269.417\tavg length: 335.630\t avg time: 0.860\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -193.539\tavg length: 552.300\t avg time: 1.649\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -85.319\tavg length: 702.780\t avg time: 2.421\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -54.281\tavg length: 449.720\t avg time: 1.448\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 14.576 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_250_0.001_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -280.677\tavg length: 90.230\t avg time: 0.156\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -238.007\tavg length: 86.530\t avg time: 0.148\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -222.468\tavg length: 88.250\t avg time: 0.150\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -214.073\tavg length: 90.950\t avg time: 0.154\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -192.138\tavg length: 96.540\t avg time: 0.163\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -218.577\tavg length: 103.850\t avg time: 0.176\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -229.268\tavg length: 116.250\t avg time: 0.198\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -288.700\tavg length: 157.370\t avg time: 0.316\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -277.104\tavg length: 181.220\t avg time: 0.352\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -178.410\tavg length: 139.260\t avg time: 0.251\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -153.473\tavg length: 168.510\t avg time: 0.326\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -186.437\tavg length: 141.650\t avg time: 0.277\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -107.261\tavg length: 198.510\t avg time: 0.422\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -66.925\tavg length: 154.140\t avg time: 0.289\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 6.157 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_250_0.0006_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -189.607\tavg length: 101.880\t avg time: 0.202\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -239.709\tavg length: 98.680\t avg time: 0.188\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -232.527\tavg length: 122.530\t avg time: 0.244\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -207.648\tavg length: 126.910\t avg time: 0.232\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -315.318\tavg length: 178.580\t avg time: 0.361\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -233.300\tavg length: 232.670\t avg time: 0.533\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -410.153\tavg length: 257.770\t avg time: 0.705\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -292.913\tavg length: 198.120\t avg time: 0.460\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -307.450\tavg length: 333.090\t avg time: 1.108\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -223.384\tavg length: 295.780\t avg time: 1.016\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -247.402\tavg length: 215.580\t avg time: 0.564\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -282.017\tavg length: 207.600\t avg time: 0.544\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -389.291\tavg length: 275.910\t avg time: 0.834\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -279.000\tavg length: 135.740\t avg time: 0.267\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 13.179 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_250_0.0006_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -277.772\tavg length: 80.360\t avg time: 0.137\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -221.912\tavg length: 84.200\t avg time: 0.143\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -208.163\tavg length: 85.420\t avg time: 0.145\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -180.377\tavg length: 96.020\t avg time: 0.171\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -194.527\tavg length: 90.880\t avg time: 0.154\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -203.005\tavg length: 91.810\t avg time: 0.156\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -162.929\tavg length: 93.240\t avg time: 0.160\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -177.183\tavg length: 104.140\t avg time: 0.181\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -151.044\tavg length: 130.780\t avg time: 0.247\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -177.462\tavg length: 141.100\t avg time: 0.273\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -187.483\tavg length: 122.750\t avg time: 0.230\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -119.237\tavg length: 140.160\t avg time: 0.279\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -113.773\tavg length: 157.000\t avg time: 0.307\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -126.115\tavg length: 178.620\t avg time: 0.370\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 5.660 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_8_0.97_250_0.0006_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -261.674\tavg length: 88.700\t avg time: 0.152\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -220.930\tavg length: 86.630\t avg time: 0.148\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -194.548\tavg length: 93.500\t avg time: 0.161\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -201.063\tavg length: 97.990\t avg time: 0.169\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -192.911\tavg length: 96.400\t avg time: 0.165\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -157.410\tavg length: 97.940\t avg time: 0.168\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -173.374\tavg length: 109.870\t avg time: 0.206\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -167.963\tavg length: 101.330\t avg time: 0.176\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -172.885\tavg length: 111.440\t avg time: 0.196\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -171.135\tavg length: 122.800\t avg time: 0.237\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -159.007\tavg length: 123.070\t avg time: 0.221\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -161.150\tavg length: 132.290\t avg time: 0.243\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -162.265\tavg length: 143.410\t avg time: 0.265\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -149.073\tavg length: 145.170\t avg time: 0.271\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 5.062 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_1000_0.22_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -854.759\tavg length: 84.260\t avg time: 0.153\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -807.339\tavg length: 81.140\t avg time: 0.146\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -849.925\tavg length: 84.390\t avg time: 0.152\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -817.819\tavg length: 82.420\t avg time: 0.148\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -789.530\tavg length: 82.590\t avg time: 0.149\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -826.228\tavg length: 79.710\t avg time: 0.144\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -850.992\tavg length: 85.690\t avg time: 0.155\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -783.342\tavg length: 81.590\t avg time: 0.147\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -818.986\tavg length: 81.560\t avg time: 0.147\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -815.628\tavg length: 80.090\t avg time: 0.145\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -850.340\tavg length: 85.430\t avg time: 0.155\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -817.781\tavg length: 80.940\t avg time: 0.147\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -818.671\tavg length: 79.450\t avg time: 0.143\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -854.935\tavg length: 84.240\t avg time: 0.152\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.722 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_1000_0.22_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -527.909\tavg length: 65.120\t avg time: 0.108\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -510.088\tavg length: 63.060\t avg time: 0.105\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -522.641\tavg length: 62.870\t avg time: 0.104\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -530.958\tavg length: 63.270\t avg time: 0.105\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -519.608\tavg length: 62.980\t avg time: 0.105\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -534.284\tavg length: 62.720\t avg time: 0.104\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -529.072\tavg length: 63.270\t avg time: 0.104\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -538.035\tavg length: 63.670\t avg time: 0.105\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -510.801\tavg length: 61.860\t avg time: 0.102\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -547.965\tavg length: 64.570\t avg time: 0.107\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -565.798\tavg length: 66.390\t avg time: 0.110\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -513.996\tavg length: 62.730\t avg time: 0.103\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -526.969\tavg length: 64.160\t avg time: 0.106\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -533.277\tavg length: 62.170\t avg time: 0.103\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.631 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_1000_0.22_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -882.936\tavg length: 80.600\t avg time: 0.145\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -843.817\tavg length: 75.340\t avg time: 0.136\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -842.469\tavg length: 76.210\t avg time: 0.137\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -862.692\tavg length: 77.720\t avg time: 0.140\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -855.649\tavg length: 77.480\t avg time: 0.140\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -880.704\tavg length: 79.740\t avg time: 0.144\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -860.917\tavg length: 78.270\t avg time: 0.141\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -831.171\tavg length: 76.500\t avg time: 0.138\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -846.745\tavg length: 77.550\t avg time: 0.139\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -838.377\tavg length: 76.530\t avg time: 0.138\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -840.421\tavg length: 77.050\t avg time: 0.139\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -861.034\tavg length: 78.000\t avg time: 0.140\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -860.997\tavg length: 78.670\t avg time: 0.142\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -860.650\tavg length: 77.110\t avg time: 0.139\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.493 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_1000_0.0018_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -217.612\tavg length: 82.400\t avg time: 0.141\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -234.117\tavg length: 67.640\t avg time: 0.113\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -400.838\tavg length: 65.030\t avg time: 0.108\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -446.985\tavg length: 63.940\t avg time: 0.106\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -446.695\tavg length: 62.870\t avg time: 0.105\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -447.202\tavg length: 64.360\t avg time: 0.107\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -455.221\tavg length: 65.670\t avg time: 0.109\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -464.099\tavg length: 63.330\t avg time: 0.105\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -460.734\tavg length: 66.110\t avg time: 0.110\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -448.136\tavg length: 64.070\t avg time: 0.107\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -454.268\tavg length: 63.690\t avg time: 0.106\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -454.168\tavg length: 64.090\t avg time: 0.108\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -452.319\tavg length: 63.200\t avg time: 0.106\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -459.255\tavg length: 63.900\t avg time: 0.106\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.744 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_1000_0.0018_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -221.938\tavg length: 85.320\t avg time: 0.146\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -223.211\tavg length: 77.620\t avg time: 0.131\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -196.699\tavg length: 79.540\t avg time: 0.134\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -390.720\tavg length: 66.380\t avg time: 0.111\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -442.106\tavg length: 61.780\t avg time: 0.103\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -452.583\tavg length: 63.330\t avg time: 0.105\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -455.257\tavg length: 64.220\t avg time: 0.107\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -461.956\tavg length: 64.930\t avg time: 0.108\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -460.882\tavg length: 65.590\t avg time: 0.109\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -444.818\tavg length: 64.490\t avg time: 0.108\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -450.365\tavg length: 62.930\t avg time: 0.104\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -430.008\tavg length: 62.930\t avg time: 0.105\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -445.256\tavg length: 63.690\t avg time: 0.106\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -436.823\tavg length: 64.300\t avg time: 0.107\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.821 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_1000_0.0018_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -244.258\tavg length: 97.620\t avg time: 0.182\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -230.605\tavg length: 89.350\t avg time: 0.152\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -198.544\tavg length: 89.050\t avg time: 0.153\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -254.637\tavg length: 97.080\t avg time: 0.169\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -187.385\tavg length: 94.910\t avg time: 0.166\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -147.803\tavg length: 98.280\t avg time: 0.170\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -150.394\tavg length: 94.090\t avg time: 0.163\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -142.291\tavg length: 100.270\t avg time: 0.175\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -134.788\tavg length: 140.250\t avg time: 0.295\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -173.865\tavg length: 180.120\t avg time: 0.354\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -99.269\tavg length: 180.590\t avg time: 0.377\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -65.850\tavg length: 201.350\t avg time: 0.424\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -98.888\tavg length: 253.340\t avg time: 0.566\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -122.303\tavg length: 348.540\t avg time: 0.897\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 8.613 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_1000_0.0014_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -227.185\tavg length: 93.580\t avg time: 0.185\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -247.062\tavg length: 84.160\t avg time: 0.150\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -207.079\tavg length: 93.160\t avg time: 0.165\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -186.309\tavg length: 109.230\t avg time: 0.215\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -150.074\tavg length: 107.940\t avg time: 0.193\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -89.017\tavg length: 148.010\t avg time: 0.275\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -74.020\tavg length: 370.180\t avg time: 1.055\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -81.693\tavg length: 492.800\t avg time: 1.527\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -90.856\tavg length: 789.330\t avg time: 2.731\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -44.800\tavg length: 896.840\t avg time: 3.321\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -14.718\tavg length: 882.660\t avg time: 3.357\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -4.909\tavg length: 861.950\t avg time: 3.308\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -11.049\tavg length: 848.150\t avg time: 3.219\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -11.957\tavg length: 921.080\t avg time: 3.501\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 44.979 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_1000_0.0014_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -388.305\tavg length: 99.920\t avg time: 0.180\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -283.113\tavg length: 87.400\t avg time: 0.160\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -200.510\tavg length: 81.460\t avg time: 0.147\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -191.571\tavg length: 92.010\t avg time: 0.167\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -187.989\tavg length: 99.970\t avg time: 0.181\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -278.689\tavg length: 134.830\t avg time: 0.256\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -274.224\tavg length: 182.080\t avg time: 0.368\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -134.978\tavg length: 165.840\t avg time: 0.322\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -131.559\tavg length: 199.770\t avg time: 0.458\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -62.638\tavg length: 235.500\t avg time: 0.574\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -98.057\tavg length: 332.720\t avg time: 0.956\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -69.357\tavg length: 480.220\t avg time: 1.604\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -43.763\tavg length: 686.940\t avg time: 2.711\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -35.482\tavg length: 855.590\t avg time: 3.568\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 25.493 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_1000_0.0014_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -221.729\tavg length: 90.780\t avg time: 0.159\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -210.749\tavg length: 105.100\t avg time: 0.206\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -201.540\tavg length: 106.630\t avg time: 0.191\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -180.644\tavg length: 101.750\t avg time: 0.181\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -187.811\tavg length: 107.950\t avg time: 0.189\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -173.629\tavg length: 114.590\t avg time: 0.204\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -145.647\tavg length: 124.760\t avg time: 0.226\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -123.913\tavg length: 160.050\t avg time: 0.305\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -114.074\tavg length: 154.010\t avg time: 0.293\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -90.560\tavg length: 232.080\t avg time: 0.545\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -112.703\tavg length: 267.160\t avg time: 0.647\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -109.150\tavg length: 256.980\t avg time: 0.612\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -42.040\tavg length: 316.340\t avg time: 0.869\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -46.777\tavg length: 404.030\t avg time: 1.210\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 13.641 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_1000_0.001_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -263.322\tavg length: 95.580\t avg time: 0.172\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -182.025\tavg length: 85.460\t avg time: 0.152\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -228.856\tavg length: 144.230\t avg time: 0.270\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -159.112\tavg length: 332.150\t avg time: 0.817\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -64.542\tavg length: 631.840\t avg time: 2.021\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -31.569\tavg length: 709.370\t avg time: 2.309\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -20.001\tavg length: 891.770\t avg time: 3.115\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -24.636\tavg length: 935.940\t avg time: 3.275\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -9.265\tavg length: 956.750\t avg time: 3.392\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: 5.225\tavg length: 935.660\t avg time: 3.301\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -2.034\tavg length: 911.860\t avg time: 3.204\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -2.010\tavg length: 983.120\t avg time: 3.446\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: 21.620\tavg length: 931.920\t avg time: 3.222\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 61.914\tavg length: 947.760\t avg time: 2.909\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 56.831 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_1000_0.001_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -294.181\tavg length: 87.090\t avg time: 0.153\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -270.133\tavg length: 87.530\t avg time: 0.154\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -215.011\tavg length: 98.960\t avg time: 0.192\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -220.043\tavg length: 87.140\t avg time: 0.154\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -216.412\tavg length: 90.870\t avg time: 0.161\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -216.692\tavg length: 98.550\t avg time: 0.172\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -232.701\tavg length: 110.790\t avg time: 0.193\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -245.105\tavg length: 108.020\t avg time: 0.188\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -172.624\tavg length: 103.130\t avg time: 0.179\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -176.763\tavg length: 96.500\t avg time: 0.167\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -189.165\tavg length: 128.750\t avg time: 0.234\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -167.205\tavg length: 135.330\t avg time: 0.245\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -140.927\tavg length: 135.440\t avg time: 0.244\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -134.218\tavg length: 145.300\t avg time: 0.268\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.888 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_1000_0.001_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -232.043\tavg length: 86.870\t avg time: 0.150\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -228.986\tavg length: 86.480\t avg time: 0.149\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -179.420\tavg length: 82.930\t avg time: 0.142\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -179.109\tavg length: 82.870\t avg time: 0.142\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -192.404\tavg length: 83.630\t avg time: 0.143\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -176.005\tavg length: 83.640\t avg time: 0.145\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -174.422\tavg length: 90.560\t avg time: 0.158\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -171.665\tavg length: 96.520\t avg time: 0.170\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -160.863\tavg length: 104.930\t avg time: 0.204\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -163.752\tavg length: 101.760\t avg time: 0.179\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -158.232\tavg length: 109.140\t avg time: 0.194\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -136.669\tavg length: 107.810\t avg time: 0.191\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -114.599\tavg length: 125.060\t avg time: 0.225\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -135.285\tavg length: 120.020\t avg time: 0.215\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.494 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_1000_0.0006_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -247.039\tavg length: 87.940\t avg time: 0.152\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -200.476\tavg length: 93.450\t avg time: 0.163\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -190.150\tavg length: 90.470\t avg time: 0.159\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -156.366\tavg length: 90.090\t avg time: 0.157\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -126.120\tavg length: 97.520\t avg time: 0.172\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -143.591\tavg length: 105.290\t avg time: 0.200\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -131.832\tavg length: 100.270\t avg time: 0.177\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -118.997\tavg length: 150.400\t avg time: 0.283\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -101.717\tavg length: 161.880\t avg time: 0.308\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -85.687\tavg length: 201.490\t avg time: 0.459\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -45.467\tavg length: 220.010\t avg time: 0.532\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -37.564\tavg length: 217.810\t avg time: 0.521\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -54.255\tavg length: 597.470\t avg time: 2.029\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 14.593 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_1000_0.0006_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -303.340\tavg length: 87.610\t avg time: 0.155\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -235.820\tavg length: 97.840\t avg time: 0.175\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -237.276\tavg length: 100.720\t avg time: 0.180\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -215.605\tavg length: 97.270\t avg time: 0.172\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -180.456\tavg length: 98.290\t avg time: 0.175\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -195.414\tavg length: 110.980\t avg time: 0.200\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -185.236\tavg length: 113.000\t avg time: 0.203\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -172.057\tavg length: 115.230\t avg time: 0.210\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -163.165\tavg length: 141.970\t avg time: 0.264\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -142.533\tavg length: 140.540\t avg time: 0.262\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -142.669\tavg length: 145.630\t avg time: 0.277\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -136.345\tavg length: 174.600\t avg time: 0.338\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -201.255\tavg length: 203.580\t avg time: 0.416\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -186.699\tavg length: 219.080\t avg time: 0.487\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 6.706 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_1000_0.0006_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -229.748\tavg length: 89.430\t avg time: 0.156\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -218.320\tavg length: 90.160\t avg time: 0.157\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -223.010\tavg length: 88.020\t avg time: 0.153\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -235.378\tavg length: 94.820\t avg time: 0.166\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -224.061\tavg length: 107.010\t avg time: 0.209\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -199.959\tavg length: 93.730\t avg time: 0.163\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -192.635\tavg length: 86.690\t avg time: 0.150\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -226.268\tavg length: 94.420\t avg time: 0.164\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -199.269\tavg length: 99.230\t avg time: 0.192\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -191.594\tavg length: 92.600\t avg time: 0.160\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -190.723\tavg length: 88.680\t avg time: 0.153\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -180.041\tavg length: 93.990\t avg time: 0.164\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -179.007\tavg length: 92.690\t avg time: 0.161\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -177.612\tavg length: 98.840\t avg time: 0.172\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.151 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_250_0.22_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -879.492\tavg length: 74.170\t avg time: 0.134\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -888.295\tavg length: 75.150\t avg time: 0.136\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -916.380\tavg length: 79.450\t avg time: 0.144\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -857.750\tavg length: 73.600\t avg time: 0.134\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -906.531\tavg length: 78.760\t avg time: 0.144\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -919.071\tavg length: 77.400\t avg time: 0.141\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -922.876\tavg length: 77.010\t avg time: 0.140\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -905.325\tavg length: 77.940\t avg time: 0.141\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -886.475\tavg length: 76.540\t avg time: 0.139\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -892.003\tavg length: 77.650\t avg time: 0.141\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -905.486\tavg length: 78.350\t avg time: 0.142\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -924.080\tavg length: 77.580\t avg time: 0.141\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -863.598\tavg length: 74.080\t avg time: 0.134\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -928.217\tavg length: 79.920\t avg time: 0.145\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.499 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_250_0.22_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -785.233\tavg length: 70.950\t avg time: 0.127\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -813.726\tavg length: 71.300\t avg time: 0.128\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -874.635\tavg length: 76.100\t avg time: 0.137\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -834.257\tavg length: 74.610\t avg time: 0.134\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -821.259\tavg length: 72.510\t avg time: 0.130\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -816.957\tavg length: 75.110\t avg time: 0.134\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -831.121\tavg length: 73.800\t avg time: 0.132\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -852.202\tavg length: 75.600\t avg time: 0.136\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -820.019\tavg length: 73.300\t avg time: 0.131\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -884.374\tavg length: 77.540\t avg time: 0.140\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -875.354\tavg length: 79.550\t avg time: 0.144\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -830.791\tavg length: 74.820\t avg time: 0.135\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -833.399\tavg length: 72.660\t avg time: 0.131\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -823.729\tavg length: 73.180\t avg time: 0.132\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.341 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_250_0.22_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -756.572\tavg length: 80.240\t avg time: 0.144\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -807.025\tavg length: 84.570\t avg time: 0.153\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -802.830\tavg length: 82.310\t avg time: 0.149\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -799.580\tavg length: 85.150\t avg time: 0.154\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -767.245\tavg length: 78.780\t avg time: 0.142\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -807.325\tavg length: 82.950\t avg time: 0.149\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -766.232\tavg length: 78.470\t avg time: 0.141\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -782.432\tavg length: 80.450\t avg time: 0.145\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -769.295\tavg length: 81.630\t avg time: 0.147\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -764.971\tavg length: 79.830\t avg time: 0.143\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -786.283\tavg length: 79.540\t avg time: 0.142\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -766.051\tavg length: 79.590\t avg time: 0.142\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -782.311\tavg length: 79.820\t avg time: 0.143\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -758.035\tavg length: 79.270\t avg time: 0.141\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.633 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_250_0.0018_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -238.480\tavg length: 99.820\t avg time: 0.175\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -307.378\tavg length: 98.260\t avg time: 0.174\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -443.064\tavg length: 110.410\t avg time: 0.204\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -171.337\tavg length: 147.690\t avg time: 0.294\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -191.763\tavg length: 154.900\t avg time: 0.314\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -184.610\tavg length: 157.760\t avg time: 0.329\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -151.337\tavg length: 172.110\t avg time: 0.433\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -155.015\tavg length: 156.600\t avg time: 0.334\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -148.841\tavg length: 115.250\t avg time: 0.213\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -210.307\tavg length: 97.850\t avg time: 0.180\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -202.410\tavg length: 149.020\t avg time: 0.325\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -160.461\tavg length: 162.940\t avg time: 0.354\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -154.848\tavg length: 146.860\t avg time: 0.292\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -132.288\tavg length: 155.400\t avg time: 0.318\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 7.083 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_250_0.0018_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -276.025\tavg length: 90.850\t avg time: 0.160\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -252.514\tavg length: 95.560\t avg time: 0.168\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -210.696\tavg length: 85.670\t avg time: 0.148\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -178.819\tavg length: 96.350\t avg time: 0.169\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -164.077\tavg length: 90.970\t avg time: 0.158\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -170.344\tavg length: 100.230\t avg time: 0.175\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -171.367\tavg length: 105.660\t avg time: 0.186\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -249.423\tavg length: 119.920\t avg time: 0.218\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -153.820\tavg length: 144.600\t avg time: 0.267\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -113.071\tavg length: 154.320\t avg time: 0.314\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -209.333\tavg length: 168.130\t avg time: 0.344\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -232.005\tavg length: 159.950\t avg time: 0.326\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -122.097\tavg length: 179.980\t avg time: 0.371\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -74.106\tavg length: 198.920\t avg time: 0.419\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 8.102 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_250_0.0018_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -264.217\tavg length: 94.400\t avg time: 0.167\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -209.350\tavg length: 95.920\t avg time: 0.168\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -194.765\tavg length: 90.210\t avg time: 0.155\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -196.686\tavg length: 96.450\t avg time: 0.167\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -126.264\tavg length: 98.500\t avg time: 0.171\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -145.541\tavg length: 99.750\t avg time: 0.172\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -148.823\tavg length: 121.200\t avg time: 0.214\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -110.588\tavg length: 147.670\t avg time: 0.271\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -110.898\tavg length: 170.310\t avg time: 0.346\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -113.193\tavg length: 212.960\t avg time: 0.504\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -148.261\tavg length: 347.440\t avg time: 1.026\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -52.091\tavg length: 317.470\t avg time: 0.891\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -119.623\tavg length: 246.080\t avg time: 0.588\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -642.039\tavg length: 174.360\t avg time: 0.407\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 8.983 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_250_0.0014_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -287.235\tavg length: 98.390\t avg time: 0.171\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -191.855\tavg length: 93.200\t avg time: 0.162\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -189.615\tavg length: 94.950\t avg time: 0.166\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -218.972\tavg length: 132.920\t avg time: 0.260\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -114.128\tavg length: 162.090\t avg time: 0.389\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -189.989\tavg length: 146.690\t avg time: 0.307\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -146.887\tavg length: 160.870\t avg time: 0.370\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -23.102\tavg length: 183.150\t avg time: 0.402\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -41.736\tavg length: 197.810\t avg time: 0.441\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: 0.196\tavg length: 191.010\t avg time: 0.420\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -8.848\tavg length: 221.970\t avg time: 0.541\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -2.494\tavg length: 282.090\t avg time: 0.870\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -368.886\tavg length: 262.020\t avg time: 0.880\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -764.646\tavg length: 72.060\t avg time: 0.132\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 9.430 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_250_0.0014_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -228.800\tavg length: 86.500\t avg time: 0.147\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -210.882\tavg length: 92.590\t avg time: 0.159\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -187.952\tavg length: 95.950\t avg time: 0.166\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -279.621\tavg length: 138.640\t avg time: 0.261\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -188.023\tavg length: 89.610\t avg time: 0.156\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -275.420\tavg length: 185.320\t avg time: 0.444\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -163.898\tavg length: 173.700\t avg time: 0.364\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -236.416\tavg length: 234.700\t avg time: 0.579\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -355.339\tavg length: 263.030\t avg time: 0.722\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -165.069\tavg length: 190.260\t avg time: 0.404\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -105.719\tavg length: 198.930\t avg time: 0.424\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -86.247\tavg length: 265.450\t avg time: 0.646\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -109.375\tavg length: 349.780\t avg time: 1.140\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -34.950\tavg length: 262.530\t avg time: 0.682\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 11.313 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_250_0.0014_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -268.075\tavg length: 95.190\t avg time: 0.167\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -232.074\tavg length: 89.150\t avg time: 0.156\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -231.387\tavg length: 85.190\t avg time: 0.147\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -197.872\tavg length: 86.530\t avg time: 0.149\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -175.638\tavg length: 85.680\t avg time: 0.147\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -168.357\tavg length: 85.310\t avg time: 0.146\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -171.862\tavg length: 87.930\t avg time: 0.150\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -162.568\tavg length: 84.270\t avg time: 0.145\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -164.822\tavg length: 88.950\t avg time: 0.153\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -173.552\tavg length: 97.510\t avg time: 0.170\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -155.840\tavg length: 105.250\t avg time: 0.183\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -143.316\tavg length: 101.430\t avg time: 0.177\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -138.311\tavg length: 102.650\t avg time: 0.180\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -130.906\tavg length: 113.100\t avg time: 0.200\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.130 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_250_0.001_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -345.700\tavg length: 94.420\t avg time: 0.165\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -492.118\tavg length: 121.450\t avg time: 0.223\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -460.717\tavg length: 143.440\t avg time: 0.273\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -146.348\tavg length: 129.520\t avg time: 0.251\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -227.361\tavg length: 159.560\t avg time: 0.314\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -310.936\tavg length: 220.280\t avg time: 0.532\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -121.700\tavg length: 295.370\t avg time: 0.849\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -31.649\tavg length: 260.120\t avg time: 0.703\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -40.322\tavg length: 276.530\t avg time: 0.730\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -15.881\tavg length: 249.880\t avg time: 0.686\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -29.567\tavg length: 466.140\t avg time: 1.657\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -57.330\tavg length: 706.520\t avg time: 2.883\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -10.683\tavg length: 775.180\t avg time: 3.399\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -13.392\tavg length: 918.160\t avg time: 4.180\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 33.550 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_250_0.001_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -260.303\tavg length: 96.240\t avg time: 0.171\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -231.678\tavg length: 107.620\t avg time: 0.202\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -278.159\tavg length: 109.960\t avg time: 0.197\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -296.762\tavg length: 108.480\t avg time: 0.191\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -206.074\tavg length: 110.440\t avg time: 0.193\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -217.461\tavg length: 127.290\t avg time: 0.245\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -191.523\tavg length: 135.390\t avg time: 0.258\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -199.295\tavg length: 121.320\t avg time: 0.216\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -272.907\tavg length: 181.090\t avg time: 0.374\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -161.421\tavg length: 214.560\t avg time: 0.468\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -94.958\tavg length: 227.050\t avg time: 0.550\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -271.269\tavg length: 236.330\t avg time: 0.538\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -332.875\tavg length: 262.800\t avg time: 0.622\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -129.545\tavg length: 497.950\t avg time: 1.637\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 13.705 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_250_0.001_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -296.849\tavg length: 91.700\t avg time: 0.160\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -234.062\tavg length: 99.320\t avg time: 0.173\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -225.088\tavg length: 91.420\t avg time: 0.158\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -233.140\tavg length: 91.670\t avg time: 0.157\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -203.794\tavg length: 102.030\t avg time: 0.189\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -190.276\tavg length: 94.550\t avg time: 0.162\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -183.497\tavg length: 93.830\t avg time: 0.160\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -156.811\tavg length: 92.340\t avg time: 0.158\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -177.851\tavg length: 88.000\t avg time: 0.151\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -158.785\tavg length: 97.840\t avg time: 0.169\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -167.703\tavg length: 99.230\t avg time: 0.171\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -153.528\tavg length: 92.910\t avg time: 0.160\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -152.510\tavg length: 102.560\t avg time: 0.183\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -146.716\tavg length: 98.830\t avg time: 0.174\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.185 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_250_0.0006_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -286.548\tavg length: 84.540\t avg time: 0.147\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -232.722\tavg length: 80.650\t avg time: 0.141\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -221.596\tavg length: 81.890\t avg time: 0.141\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -226.798\tavg length: 89.870\t avg time: 0.157\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -199.505\tavg length: 89.890\t avg time: 0.156\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -174.941\tavg length: 87.740\t avg time: 0.152\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -158.765\tavg length: 99.880\t avg time: 0.174\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -153.809\tavg length: 111.700\t avg time: 0.198\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -134.008\tavg length: 150.600\t avg time: 0.294\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -126.868\tavg length: 152.200\t avg time: 0.305\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -146.905\tavg length: 225.810\t avg time: 0.502\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -77.218\tavg length: 194.820\t avg time: 0.408\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -116.714\tavg length: 316.260\t avg time: 0.891\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -134.990\tavg length: 317.200\t avg time: 0.911\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 9.589 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_250_0.0006_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -324.071\tavg length: 92.500\t avg time: 0.163\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -244.629\tavg length: 98.380\t avg time: 0.173\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -245.272\tavg length: 111.590\t avg time: 0.199\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -214.158\tavg length: 108.240\t avg time: 0.192\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -184.739\tavg length: 112.200\t avg time: 0.199\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -204.505\tavg length: 118.030\t avg time: 0.213\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -188.942\tavg length: 121.470\t avg time: 0.221\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -278.108\tavg length: 153.230\t avg time: 0.304\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -241.988\tavg length: 158.390\t avg time: 0.321\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -373.251\tavg length: 195.080\t avg time: 0.399\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -226.190\tavg length: 183.620\t avg time: 0.362\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -141.342\tavg length: 172.010\t avg time: 0.344\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -204.485\tavg length: 198.680\t avg time: 0.396\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -143.210\tavg length: 168.550\t avg time: 0.322\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 6.798 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.99_250_0.0006_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -285.296\tavg length: 92.040\t avg time: 0.160\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -256.598\tavg length: 102.310\t avg time: 0.178\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -277.345\tavg length: 91.610\t avg time: 0.158\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -247.922\tavg length: 94.830\t avg time: 0.164\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -219.825\tavg length: 105.630\t avg time: 0.198\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -205.722\tavg length: 91.120\t avg time: 0.158\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -216.633\tavg length: 103.030\t avg time: 0.180\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -212.615\tavg length: 95.240\t avg time: 0.166\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -195.900\tavg length: 109.460\t avg time: 0.193\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -188.802\tavg length: 108.250\t avg time: 0.208\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -191.172\tavg length: 108.140\t avg time: 0.194\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -192.549\tavg length: 107.180\t avg time: 0.189\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -165.766\tavg length: 106.640\t avg time: 0.188\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -172.320\tavg length: 110.440\t avg time: 0.195\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.589 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_1000_0.22_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -474.043\tavg length: 63.680\t avg time: 0.106\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -485.666\tavg length: 65.590\t avg time: 0.109\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -492.309\tavg length: 64.180\t avg time: 0.107\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -511.971\tavg length: 64.570\t avg time: 0.108\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -477.378\tavg length: 62.270\t avg time: 0.104\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -477.279\tavg length: 62.450\t avg time: 0.104\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -498.648\tavg length: 63.500\t avg time: 0.106\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -471.760\tavg length: 63.270\t avg time: 0.105\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -488.418\tavg length: 61.850\t avg time: 0.103\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -479.772\tavg length: 61.120\t avg time: 0.102\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -499.738\tavg length: 64.910\t avg time: 0.109\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -486.881\tavg length: 63.400\t avg time: 0.105\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -478.404\tavg length: 62.560\t avg time: 0.104\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -486.863\tavg length: 64.040\t avg time: 0.106\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.643 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_1000_0.22_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -479.953\tavg length: 64.120\t avg time: 0.106\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -277.788\tavg length: 85.730\t avg time: 0.145\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -511.850\tavg length: 109.600\t avg time: 0.201\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -631.311\tavg length: 87.110\t avg time: 0.158\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -750.588\tavg length: 77.080\t avg time: 0.139\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -781.459\tavg length: 76.950\t avg time: 0.138\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -576.383\tavg length: 102.610\t avg time: 0.188\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -402.855\tavg length: 96.840\t avg time: 0.177\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -686.620\tavg length: 141.990\t avg time: 0.292\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -567.370\tavg length: 168.550\t avg time: 0.327\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -587.688\tavg length: 171.400\t avg time: 0.334\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -527.846\tavg length: 153.590\t avg time: 0.297\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -887.187\tavg length: 79.560\t avg time: 0.145\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -964.277\tavg length: 84.740\t avg time: 0.153\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.924 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_1000_0.22_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -734.830\tavg length: 131.870\t avg time: 0.311\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -772.054\tavg length: 84.980\t avg time: 0.154\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -769.453\tavg length: 84.470\t avg time: 0.153\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -765.815\tavg length: 84.500\t avg time: 0.153\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -773.592\tavg length: 83.420\t avg time: 0.150\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -785.131\tavg length: 83.400\t avg time: 0.150\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -787.337\tavg length: 86.030\t avg time: 0.155\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -777.778\tavg length: 85.080\t avg time: 0.153\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -736.188\tavg length: 81.190\t avg time: 0.146\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -741.041\tavg length: 81.560\t avg time: 0.147\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -752.341\tavg length: 82.240\t avg time: 0.148\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -758.126\tavg length: 84.420\t avg time: 0.152\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -739.911\tavg length: 81.740\t avg time: 0.147\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -735.722\tavg length: 82.040\t avg time: 0.148\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.030 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_1000_0.0018_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -259.512\tavg length: 93.460\t avg time: 0.163\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -246.978\tavg length: 92.020\t avg time: 0.161\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -196.393\tavg length: 123.610\t avg time: 0.229\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -194.547\tavg length: 260.640\t avg time: 0.688\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -207.859\tavg length: 351.590\t avg time: 1.134\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -155.664\tavg length: 577.330\t avg time: 2.249\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -122.125\tavg length: 720.150\t avg time: 2.801\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -80.880\tavg length: 935.720\t avg time: 3.688\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -6.016\tavg length: 944.030\t avg time: 3.904\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: 11.183\tavg length: 862.140\t avg time: 3.264\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -20.267\tavg length: 694.780\t avg time: 2.566\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: 39.680\tavg length: 640.080\t avg time: 2.018\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: 39.158\tavg length: 841.450\t avg time: 3.084\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 73.216\tavg length: 880.640\t avg time: 3.276\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 53.394 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_1000_0.0018_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -295.211\tavg length: 98.750\t avg time: 0.179\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -615.953\tavg length: 80.800\t avg time: 0.150\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -615.201\tavg length: 96.480\t avg time: 0.179\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -257.363\tavg length: 113.310\t avg time: 0.221\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -96.137\tavg length: 132.920\t avg time: 0.264\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -49.808\tavg length: 270.500\t avg time: 0.737\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -83.695\tavg length: 496.190\t avg time: 1.669\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -65.985\tavg length: 618.280\t avg time: 2.333\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -52.048\tavg length: 699.560\t avg time: 2.642\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -33.950\tavg length: 742.870\t avg time: 2.911\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -75.832\tavg length: 698.870\t avg time: 2.618\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -86.613\tavg length: 865.210\t avg time: 3.482\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -18.782\tavg length: 931.710\t avg time: 3.761\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -13.541\tavg length: 970.250\t avg time: 4.001\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 47.289 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_1000_0.0018_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -333.990\tavg length: 95.990\t avg time: 0.172\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -343.660\tavg length: 105.770\t avg time: 0.196\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -278.147\tavg length: 154.400\t avg time: 0.313\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -223.265\tavg length: 213.270\t avg time: 0.478\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -218.077\tavg length: 194.350\t avg time: 0.402\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -138.790\tavg length: 225.720\t avg time: 0.511\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -169.517\tavg length: 404.400\t avg time: 1.162\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -77.477\tavg length: 543.850\t avg time: 1.911\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -53.466\tavg length: 659.560\t avg time: 2.496\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -47.014\tavg length: 728.920\t avg time: 3.008\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -47.264\tavg length: 830.590\t avg time: 3.518\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -43.198\tavg length: 827.860\t avg time: 3.434\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -54.574\tavg length: 895.690\t avg time: 3.728\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -41.240\tavg length: 766.560\t avg time: 3.224\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 47.300 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_1000_0.0014_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -290.146\tavg length: 115.550\t avg time: 0.233\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -204.314\tavg length: 96.250\t avg time: 0.175\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -160.178\tavg length: 112.040\t avg time: 0.227\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -206.934\tavg length: 118.740\t avg time: 0.218\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -164.863\tavg length: 135.270\t avg time: 0.270\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -131.928\tavg length: 191.710\t avg time: 0.409\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -267.091\tavg length: 350.770\t avg time: 1.009\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -339.408\tavg length: 384.420\t avg time: 1.342\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -575.975\tavg length: 552.240\t avg time: 2.047\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -401.075\tavg length: 537.540\t avg time: 2.042\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -376.530\tavg length: 476.920\t avg time: 1.644\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -254.990\tavg length: 406.430\t avg time: 1.257\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -62.147\tavg length: 402.730\t avg time: 1.294\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 6.150\tavg length: 459.510\t avg time: 1.707\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 25.026 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_1000_0.0014_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -246.601\tavg length: 101.680\t avg time: 0.200\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -213.704\tavg length: 98.820\t avg time: 0.175\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -207.586\tavg length: 102.600\t avg time: 0.183\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -207.035\tavg length: 124.980\t avg time: 0.231\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -173.846\tavg length: 182.090\t avg time: 0.409\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -165.225\tavg length: 196.540\t avg time: 0.414\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -212.840\tavg length: 226.240\t avg time: 0.519\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -290.103\tavg length: 389.150\t avg time: 1.223\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -221.108\tavg length: 413.130\t avg time: 1.320\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -131.478\tavg length: 398.490\t avg time: 1.323\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -51.851\tavg length: 452.950\t avg time: 1.583\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -17.199\tavg length: 471.340\t avg time: 1.528\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -10.118\tavg length: 644.810\t avg time: 2.411\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -23.074\tavg length: 756.000\t avg time: 2.812\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 29.560 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_1000_0.0014_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -267.299\tavg length: 102.220\t avg time: 0.183\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -248.848\tavg length: 101.150\t avg time: 0.184\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -255.289\tavg length: 109.080\t avg time: 0.198\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -204.005\tavg length: 125.760\t avg time: 0.230\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -210.579\tavg length: 141.730\t avg time: 0.265\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -215.373\tavg length: 193.400\t avg time: 0.414\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -162.875\tavg length: 169.510\t avg time: 0.336\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -175.814\tavg length: 241.800\t avg time: 0.545\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -143.743\tavg length: 268.490\t avg time: 0.641\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -209.491\tavg length: 264.630\t avg time: 0.623\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -186.710\tavg length: 305.240\t avg time: 0.774\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -82.215\tavg length: 345.860\t avg time: 0.923\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -69.753\tavg length: 395.750\t avg time: 1.146\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -101.914\tavg length: 560.220\t avg time: 1.865\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 18.471 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_1000_0.001_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -266.000\tavg length: 88.290\t avg time: 0.157\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -161.581\tavg length: 103.370\t avg time: 0.188\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -256.161\tavg length: 114.230\t avg time: 0.208\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -182.018\tavg length: 113.630\t avg time: 0.204\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -154.134\tavg length: 101.680\t avg time: 0.180\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -162.788\tavg length: 143.660\t avg time: 0.289\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -123.532\tavg length: 164.410\t avg time: 0.341\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -190.934\tavg length: 302.150\t avg time: 0.781\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -203.809\tavg length: 354.310\t avg time: 0.958\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -151.555\tavg length: 435.160\t avg time: 1.338\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -31.631\tavg length: 400.860\t avg time: 1.190\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -33.313\tavg length: 792.280\t avg time: 3.006\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -24.224\tavg length: 888.250\t avg time: 3.514\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -14.658\tavg length: 799.660\t avg time: 3.140\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 31.163 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_1000_0.001_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -229.813\tavg length: 90.420\t avg time: 0.160\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -190.152\tavg length: 84.880\t avg time: 0.153\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -161.113\tavg length: 91.640\t avg time: 0.180\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -165.299\tavg length: 81.570\t avg time: 0.144\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -177.334\tavg length: 131.660\t avg time: 0.246\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -138.926\tavg length: 175.740\t avg time: 0.349\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -395.359\tavg length: 275.070\t avg time: 0.658\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -114.272\tavg length: 147.620\t avg time: 0.275\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -97.927\tavg length: 228.410\t avg time: 0.523\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -103.857\tavg length: 331.420\t avg time: 0.837\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -69.792\tavg length: 395.750\t avg time: 1.180\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -3.203\tavg length: 301.690\t avg time: 0.819\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -6.288\tavg length: 507.540\t avg time: 1.708\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -22.087\tavg length: 678.700\t avg time: 2.462\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 20.366 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_1000_0.001_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -291.303\tavg length: 89.070\t avg time: 0.158\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -266.977\tavg length: 91.030\t avg time: 0.161\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -248.810\tavg length: 99.970\t avg time: 0.178\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -224.488\tavg length: 114.930\t avg time: 0.228\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -238.320\tavg length: 114.840\t avg time: 0.208\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -251.117\tavg length: 123.910\t avg time: 0.224\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -286.637\tavg length: 151.400\t avg time: 0.287\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -269.772\tavg length: 141.910\t avg time: 0.263\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -166.402\tavg length: 133.990\t avg time: 0.244\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -212.473\tavg length: 162.570\t avg time: 0.307\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -189.361\tavg length: 168.220\t avg time: 0.324\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -197.428\tavg length: 255.280\t avg time: 0.581\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -151.292\tavg length: 296.980\t avg time: 0.753\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -110.005\tavg length: 300.270\t avg time: 0.757\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 9.613 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_1000_0.0006_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -228.099\tavg length: 92.820\t avg time: 0.175\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -186.653\tavg length: 93.310\t avg time: 0.165\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -169.107\tavg length: 89.740\t avg time: 0.158\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -174.851\tavg length: 100.350\t avg time: 0.179\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -189.272\tavg length: 109.870\t avg time: 0.199\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -175.694\tavg length: 145.450\t avg time: 0.276\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -193.266\tavg length: 176.870\t avg time: 0.377\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -308.731\tavg length: 193.920\t avg time: 0.405\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -256.969\tavg length: 253.960\t avg time: 0.614\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -403.203\tavg length: 307.160\t avg time: 0.772\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -235.581\tavg length: 276.130\t avg time: 0.680\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -274.559\tavg length: 289.560\t avg time: 0.729\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -365.197\tavg length: 371.810\t avg time: 1.010\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -141.177\tavg length: 406.850\t avg time: 1.236\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 14.805 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_1000_0.0006_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -282.903\tavg length: 89.230\t avg time: 0.158\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -221.550\tavg length: 87.290\t avg time: 0.154\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -215.047\tavg length: 89.860\t avg time: 0.158\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -190.048\tavg length: 99.610\t avg time: 0.176\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -192.798\tavg length: 99.330\t avg time: 0.175\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -166.072\tavg length: 100.830\t avg time: 0.178\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -166.979\tavg length: 118.240\t avg time: 0.223\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -166.121\tavg length: 119.190\t avg time: 0.215\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -199.746\tavg length: 125.010\t avg time: 0.227\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -169.843\tavg length: 144.850\t avg time: 0.273\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -249.749\tavg length: 182.230\t avg time: 0.361\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -307.697\tavg length: 234.010\t avg time: 0.509\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -386.607\tavg length: 283.410\t avg time: 0.686\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -232.108\tavg length: 243.380\t avg time: 0.565\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 7.933 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_1000_0.0006_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -251.960\tavg length: 91.900\t avg time: 0.161\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -256.695\tavg length: 91.960\t avg time: 0.161\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -237.547\tavg length: 111.610\t avg time: 0.214\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -238.159\tavg length: 107.240\t avg time: 0.190\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -220.685\tavg length: 104.330\t avg time: 0.185\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -216.150\tavg length: 108.070\t avg time: 0.193\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -214.937\tavg length: 111.290\t avg time: 0.197\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -229.518\tavg length: 121.480\t avg time: 0.217\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -215.812\tavg length: 120.500\t avg time: 0.228\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -192.245\tavg length: 116.430\t avg time: 0.208\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -181.887\tavg length: 121.840\t avg time: 0.236\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -157.188\tavg length: 125.770\t avg time: 0.245\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -150.713\tavg length: 118.030\t avg time: 0.212\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -140.680\tavg length: 122.560\t avg time: 0.221\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 5.220 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_250_0.22_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -757.917\tavg length: 76.420\t avg time: 0.138\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -797.494\tavg length: 79.770\t avg time: 0.144\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -779.711\tavg length: 80.680\t avg time: 0.146\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -808.580\tavg length: 83.190\t avg time: 0.151\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -785.302\tavg length: 81.520\t avg time: 0.147\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -783.543\tavg length: 80.190\t avg time: 0.145\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -793.878\tavg length: 79.570\t avg time: 0.144\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -798.757\tavg length: 81.930\t avg time: 0.149\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -749.090\tavg length: 80.160\t avg time: 0.146\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -795.962\tavg length: 81.470\t avg time: 0.148\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -784.366\tavg length: 82.130\t avg time: 0.149\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -766.017\tavg length: 79.740\t avg time: 0.144\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -787.545\tavg length: 81.720\t avg time: 0.147\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -765.843\tavg length: 79.790\t avg time: 0.144\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.647 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_250_0.22_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -709.954\tavg length: 77.870\t avg time: 0.140\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -702.956\tavg length: 78.070\t avg time: 0.141\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -706.358\tavg length: 79.350\t avg time: 0.144\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -742.514\tavg length: 82.000\t avg time: 0.148\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -710.123\tavg length: 77.450\t avg time: 0.139\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -683.888\tavg length: 76.000\t avg time: 0.136\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -709.601\tavg length: 76.480\t avg time: 0.137\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -765.946\tavg length: 80.760\t avg time: 0.145\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -730.830\tavg length: 79.710\t avg time: 0.143\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -707.147\tavg length: 80.240\t avg time: 0.144\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -722.185\tavg length: 80.170\t avg time: 0.144\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -736.798\tavg length: 80.320\t avg time: 0.144\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -720.407\tavg length: 79.020\t avg time: 0.142\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -705.398\tavg length: 76.600\t avg time: 0.137\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.533 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_250_0.22_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -496.537\tavg length: 67.790\t avg time: 0.112\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -484.438\tavg length: 62.560\t avg time: 0.103\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -478.707\tavg length: 63.020\t avg time: 0.104\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -489.971\tavg length: 64.110\t avg time: 0.106\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -490.211\tavg length: 63.500\t avg time: 0.105\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -506.432\tavg length: 65.240\t avg time: 0.108\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -486.018\tavg length: 62.900\t avg time: 0.104\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -489.188\tavg length: 62.620\t avg time: 0.104\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -480.449\tavg length: 62.520\t avg time: 0.104\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -471.393\tavg length: 63.980\t avg time: 0.106\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -485.407\tavg length: 61.760\t avg time: 0.102\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -498.130\tavg length: 64.480\t avg time: 0.107\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -479.524\tavg length: 62.680\t avg time: 0.103\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -499.491\tavg length: 64.720\t avg time: 0.106\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.627 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_250_0.0018_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -517.839\tavg length: 100.490\t avg time: 0.180\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -396.315\tavg length: 133.850\t avg time: 0.251\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -427.967\tavg length: 171.010\t avg time: 0.368\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -246.773\tavg length: 211.320\t avg time: 0.477\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -95.575\tavg length: 409.300\t avg time: 1.254\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -63.295\tavg length: 650.800\t avg time: 2.428\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -40.038\tavg length: 631.680\t avg time: 2.332\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: 9.067\tavg length: 746.940\t avg time: 2.641\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: 65.162\tavg length: 655.560\t avg time: 2.011\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: 81.047\tavg length: 571.800\t avg time: 1.712\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: 101.133\tavg length: 757.640\t avg time: 2.401\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -13.402\tavg length: 889.640\t avg time: 3.297\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: 134.214\tavg length: 641.890\t avg time: 1.858\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 131.336\tavg length: 741.960\t avg time: 2.156\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 43.314 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_250_0.0018_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -243.935\tavg length: 102.990\t avg time: 0.195\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -265.959\tavg length: 120.040\t avg time: 0.216\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -269.642\tavg length: 114.810\t avg time: 0.194\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -224.662\tavg length: 97.580\t avg time: 0.160\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -236.181\tavg length: 121.000\t avg time: 0.234\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -269.696\tavg length: 118.780\t avg time: 0.199\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -223.195\tavg length: 101.760\t avg time: 0.165\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -227.358\tavg length: 85.170\t avg time: 0.136\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -431.759\tavg length: 135.010\t avg time: 0.244\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -303.405\tavg length: 121.340\t avg time: 0.209\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -340.497\tavg length: 126.170\t avg time: 0.217\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -233.355\tavg length: 103.240\t avg time: 0.169\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -267.443\tavg length: 128.240\t avg time: 0.252\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -278.947\tavg length: 122.780\t avg time: 0.221\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 5.051 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_250_0.0018_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -274.761\tavg length: 93.230\t avg time: 0.161\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -235.907\tavg length: 101.670\t avg time: 0.178\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -261.779\tavg length: 121.780\t avg time: 0.222\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -222.546\tavg length: 116.340\t avg time: 0.209\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -244.891\tavg length: 115.130\t avg time: 0.206\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -167.214\tavg length: 131.070\t avg time: 0.239\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -147.436\tavg length: 160.140\t avg time: 0.324\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -234.702\tavg length: 133.240\t avg time: 0.252\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -96.819\tavg length: 131.990\t avg time: 0.240\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -104.768\tavg length: 165.420\t avg time: 0.343\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -139.237\tavg length: 226.660\t avg time: 0.562\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -197.418\tavg length: 251.340\t avg time: 0.692\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -213.471\tavg length: 152.260\t avg time: 0.317\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -111.069\tavg length: 167.330\t avg time: 0.341\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 7.618 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_250_0.0014_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -230.855\tavg length: 93.400\t avg time: 0.180\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -182.395\tavg length: 98.520\t avg time: 0.170\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -618.756\tavg length: 170.190\t avg time: 0.372\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -865.624\tavg length: 79.310\t avg time: 0.144\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -865.925\tavg length: 81.540\t avg time: 0.148\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -894.576\tavg length: 81.660\t avg time: 0.148\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -825.818\tavg length: 76.610\t avg time: 0.138\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -899.413\tavg length: 81.740\t avg time: 0.148\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -865.927\tavg length: 79.730\t avg time: 0.144\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -828.119\tavg length: 76.960\t avg time: 0.140\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -763.594\tavg length: 84.300\t avg time: 0.154\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -727.271\tavg length: 79.910\t avg time: 0.147\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -765.423\tavg length: 77.570\t avg time: 0.140\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -372.973\tavg length: 112.990\t avg time: 0.214\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.572 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_250_0.0014_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -332.751\tavg length: 96.540\t avg time: 0.170\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -378.126\tavg length: 110.730\t avg time: 0.196\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -263.882\tavg length: 131.160\t avg time: 0.277\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -193.732\tavg length: 99.700\t avg time: 0.170\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -188.113\tavg length: 100.720\t avg time: 0.173\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -234.909\tavg length: 142.000\t avg time: 0.269\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -194.869\tavg length: 129.320\t avg time: 0.231\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -163.043\tavg length: 145.000\t avg time: 0.265\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -119.934\tavg length: 147.480\t avg time: 0.269\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -192.900\tavg length: 185.380\t avg time: 0.381\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -136.973\tavg length: 228.770\t avg time: 0.526\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -27.192\tavg length: 141.840\t avg time: 0.280\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -20.677\tavg length: 211.890\t avg time: 0.457\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -270.569\tavg length: 240.900\t avg time: 0.517\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 7.443 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_250_0.0014_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -222.470\tavg length: 89.840\t avg time: 0.156\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -198.883\tavg length: 98.350\t avg time: 0.171\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -206.820\tavg length: 98.940\t avg time: 0.172\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -164.653\tavg length: 108.960\t avg time: 0.190\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -188.155\tavg length: 115.480\t avg time: 0.204\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -204.841\tavg length: 163.630\t avg time: 0.306\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -207.303\tavg length: 188.300\t avg time: 0.358\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -236.746\tavg length: 173.720\t avg time: 0.331\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -280.442\tavg length: 166.950\t avg time: 0.325\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -172.941\tavg length: 181.910\t avg time: 0.334\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -295.944\tavg length: 277.280\t avg time: 0.671\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -217.696\tavg length: 282.130\t avg time: 0.696\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -150.313\tavg length: 192.230\t avg time: 0.393\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -116.835\tavg length: 218.090\t avg time: 0.482\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 8.816 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_250_0.001_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -255.320\tavg length: 97.630\t avg time: 0.171\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -473.008\tavg length: 107.620\t avg time: 0.190\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -240.340\tavg length: 97.310\t avg time: 0.169\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -222.303\tavg length: 130.400\t avg time: 0.234\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -143.437\tavg length: 126.920\t avg time: 0.228\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -157.424\tavg length: 142.750\t avg time: 0.262\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -149.011\tavg length: 137.380\t avg time: 0.260\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -109.660\tavg length: 135.870\t avg time: 0.246\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -135.896\tavg length: 154.430\t avg time: 0.292\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -122.615\tavg length: 152.860\t avg time: 0.287\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -113.544\tavg length: 261.100\t avg time: 0.646\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -117.813\tavg length: 245.730\t avg time: 0.583\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -97.877\tavg length: 237.910\t avg time: 0.602\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -13.662\tavg length: 449.030\t avg time: 1.575\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 12.293 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_250_0.001_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -243.362\tavg length: 90.390\t avg time: 0.158\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -213.685\tavg length: 99.880\t avg time: 0.174\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -188.233\tavg length: 103.850\t avg time: 0.182\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -183.201\tavg length: 122.900\t avg time: 0.236\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -150.786\tavg length: 119.360\t avg time: 0.211\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -164.439\tavg length: 130.400\t avg time: 0.256\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -119.846\tavg length: 147.480\t avg time: 0.289\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -144.909\tavg length: 159.400\t avg time: 0.307\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -100.586\tavg length: 167.420\t avg time: 0.359\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -113.425\tavg length: 180.930\t avg time: 0.410\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -118.816\tavg length: 178.740\t avg time: 0.352\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -60.970\tavg length: 218.070\t avg time: 0.511\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -33.427\tavg length: 191.550\t avg time: 0.431\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -125.244\tavg length: 198.740\t avg time: 0.469\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 11.133 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_250_0.001_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -277.259\tavg length: 92.070\t avg time: 0.162\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -275.559\tavg length: 110.300\t avg time: 0.197\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -215.554\tavg length: 116.420\t avg time: 0.210\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -189.433\tavg length: 129.600\t avg time: 0.236\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -189.205\tavg length: 145.550\t avg time: 0.277\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -194.420\tavg length: 167.980\t avg time: 0.343\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -187.165\tavg length: 136.810\t avg time: 0.250\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -158.498\tavg length: 138.740\t avg time: 0.255\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -172.685\tavg length: 176.020\t avg time: 0.344\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -160.166\tavg length: 166.150\t avg time: 0.324\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -118.061\tavg length: 180.360\t avg time: 0.363\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -131.708\tavg length: 188.660\t avg time: 0.394\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -117.095\tavg length: 201.020\t avg time: 0.426\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -130.128\tavg length: 175.370\t avg time: 0.354\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 7.620 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_250_0.0006_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -242.673\tavg length: 90.590\t avg time: 0.160\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -253.722\tavg length: 108.980\t avg time: 0.197\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -255.239\tavg length: 113.480\t avg time: 0.206\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -244.852\tavg length: 135.010\t avg time: 0.250\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -201.737\tavg length: 137.000\t avg time: 0.255\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -165.817\tavg length: 124.940\t avg time: 0.229\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -164.867\tavg length: 165.100\t avg time: 0.350\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -179.163\tavg length: 158.600\t avg time: 0.316\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -175.954\tavg length: 125.320\t avg time: 0.230\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -124.199\tavg length: 157.410\t avg time: 0.320\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -124.601\tavg length: 167.770\t avg time: 0.333\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -122.633\tavg length: 178.320\t avg time: 0.372\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -179.721\tavg length: 222.240\t avg time: 0.494\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -239.510\tavg length: 282.140\t avg time: 0.705\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 8.600 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_250_0.0006_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -330.718\tavg length: 91.730\t avg time: 0.162\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -274.139\tavg length: 97.000\t avg time: 0.171\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -238.801\tavg length: 108.370\t avg time: 0.192\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -189.808\tavg length: 119.320\t avg time: 0.235\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -196.313\tavg length: 105.530\t avg time: 0.186\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -162.960\tavg length: 116.050\t avg time: 0.206\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -184.422\tavg length: 118.740\t avg time: 0.211\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -169.000\tavg length: 121.480\t avg time: 0.217\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -159.356\tavg length: 124.130\t avg time: 0.223\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -173.511\tavg length: 163.550\t avg time: 0.318\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -187.254\tavg length: 235.880\t avg time: 0.548\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -181.771\tavg length: 246.550\t avg time: 0.557\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -153.222\tavg length: 197.010\t avg time: 0.418\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -98.603\tavg length: 196.500\t avg time: 0.448\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 7.504 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_10_0.97_250_0.0006_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -321.551\tavg length: 91.580\t avg time: 0.161\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -240.321\tavg length: 98.940\t avg time: 0.174\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -243.944\tavg length: 90.080\t avg time: 0.157\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -211.847\tavg length: 92.500\t avg time: 0.161\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -234.646\tavg length: 99.760\t avg time: 0.174\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -218.751\tavg length: 96.040\t avg time: 0.167\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -193.448\tavg length: 103.860\t avg time: 0.182\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -170.433\tavg length: 98.380\t avg time: 0.171\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -178.769\tavg length: 109.170\t avg time: 0.192\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -174.713\tavg length: 107.080\t avg time: 0.189\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -214.774\tavg length: 110.210\t avg time: 0.196\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -204.109\tavg length: 113.910\t avg time: 0.203\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -168.292\tavg length: 116.200\t avg time: 0.206\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -173.972\tavg length: 118.610\t avg time: 0.213\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.621 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_1000_0.22_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -535.687\tavg length: 63.710\t avg time: 0.106\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -533.701\tavg length: 63.470\t avg time: 0.106\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -511.964\tavg length: 63.050\t avg time: 0.105\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -529.482\tavg length: 63.220\t avg time: 0.105\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -571.086\tavg length: 65.910\t avg time: 0.110\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -541.827\tavg length: 63.740\t avg time: 0.106\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -533.928\tavg length: 63.420\t avg time: 0.106\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -545.545\tavg length: 64.340\t avg time: 0.107\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -533.101\tavg length: 63.370\t avg time: 0.106\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -551.946\tavg length: 63.270\t avg time: 0.106\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -524.554\tavg length: 63.820\t avg time: 0.107\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -553.682\tavg length: 64.580\t avg time: 0.108\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -529.619\tavg length: 65.130\t avg time: 0.109\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -537.854\tavg length: 63.870\t avg time: 0.107\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.668 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_1000_0.22_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -458.835\tavg length: 65.970\t avg time: 0.109\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -432.010\tavg length: 64.530\t avg time: 0.107\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -447.441\tavg length: 63.370\t avg time: 0.106\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -442.959\tavg length: 64.570\t avg time: 0.107\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -419.494\tavg length: 62.630\t avg time: 0.104\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -440.514\tavg length: 63.500\t avg time: 0.105\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -431.605\tavg length: 63.980\t avg time: 0.106\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -414.091\tavg length: 62.500\t avg time: 0.103\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -433.163\tavg length: 63.950\t avg time: 0.106\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -425.092\tavg length: 62.530\t avg time: 0.104\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -446.738\tavg length: 62.810\t avg time: 0.105\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -449.380\tavg length: 63.290\t avg time: 0.105\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -448.508\tavg length: 66.020\t avg time: 0.109\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -436.467\tavg length: 64.460\t avg time: 0.107\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.653 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_1000_0.22_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -871.569\tavg length: 79.930\t avg time: 0.144\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -925.873\tavg length: 80.950\t avg time: 0.146\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -919.994\tavg length: 79.100\t avg time: 0.142\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -876.844\tavg length: 75.100\t avg time: 0.134\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -893.885\tavg length: 81.120\t avg time: 0.146\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -884.433\tavg length: 77.010\t avg time: 0.138\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -914.884\tavg length: 78.110\t avg time: 0.140\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -891.513\tavg length: 79.380\t avg time: 0.143\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -862.150\tavg length: 75.870\t avg time: 0.136\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -898.877\tavg length: 79.310\t avg time: 0.143\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -855.350\tavg length: 75.170\t avg time: 0.135\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -922.903\tavg length: 77.880\t avg time: 0.140\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -917.231\tavg length: 79.410\t avg time: 0.143\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -865.954\tavg length: 76.460\t avg time: 0.137\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.508 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_1000_0.0018_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -246.133\tavg length: 82.420\t avg time: 0.141\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -383.222\tavg length: 64.410\t avg time: 0.107\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -399.608\tavg length: 64.640\t avg time: 0.107\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -384.495\tavg length: 60.060\t avg time: 0.100\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -412.471\tavg length: 63.620\t avg time: 0.106\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -413.663\tavg length: 63.590\t avg time: 0.106\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -411.499\tavg length: 63.980\t avg time: 0.106\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -344.409\tavg length: 65.570\t avg time: 0.109\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -403.840\tavg length: 65.020\t avg time: 0.108\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -424.215\tavg length: 64.490\t avg time: 0.108\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -424.553\tavg length: 64.970\t avg time: 0.109\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -416.235\tavg length: 64.140\t avg time: 0.107\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -411.894\tavg length: 64.480\t avg time: 0.108\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -427.693\tavg length: 66.100\t avg time: 0.110\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.732 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_1000_0.0018_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -247.205\tavg length: 88.560\t avg time: 0.151\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -183.700\tavg length: 95.900\t avg time: 0.165\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -166.198\tavg length: 115.610\t avg time: 0.205\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -151.905\tavg length: 169.810\t avg time: 0.325\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -259.434\tavg length: 181.100\t avg time: 0.371\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -124.367\tavg length: 212.530\t avg time: 0.436\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -17.128\tavg length: 210.350\t avg time: 0.436\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -15.253\tavg length: 299.610\t avg time: 0.785\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -24.448\tavg length: 684.530\t avg time: 2.314\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -16.371\tavg length: 580.690\t avg time: 1.912\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -21.087\tavg length: 859.380\t avg time: 3.167\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: 0.990\tavg length: 535.000\t avg time: 1.829\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -1.358\tavg length: 741.260\t avg time: 2.778\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -16.471\tavg length: 905.710\t avg time: 3.360\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 34.841 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_1000_0.0018_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -242.591\tavg length: 83.330\t avg time: 0.150\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -247.165\tavg length: 81.090\t avg time: 0.146\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -240.777\tavg length: 82.860\t avg time: 0.150\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -194.068\tavg length: 88.980\t avg time: 0.160\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -207.140\tavg length: 93.470\t avg time: 0.168\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -163.844\tavg length: 90.720\t avg time: 0.162\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -125.062\tavg length: 118.030\t avg time: 0.213\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -136.034\tavg length: 177.930\t avg time: 0.355\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -199.742\tavg length: 170.180\t avg time: 0.352\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -305.532\tavg length: 266.350\t avg time: 0.620\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -145.656\tavg length: 256.840\t avg time: 0.610\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -61.705\tavg length: 231.050\t avg time: 0.521\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -34.479\tavg length: 310.870\t avg time: 0.826\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -193.567\tavg length: 387.440\t avg time: 1.221\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 11.556 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_1000_0.0014_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -434.480\tavg length: 90.220\t avg time: 0.165\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -850.270\tavg length: 85.110\t avg time: 0.158\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -811.226\tavg length: 82.250\t avg time: 0.150\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -761.847\tavg length: 78.120\t avg time: 0.142\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -820.241\tavg length: 82.820\t avg time: 0.150\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -843.043\tavg length: 84.490\t avg time: 0.153\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -697.095\tavg length: 89.680\t avg time: 0.163\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -341.711\tavg length: 95.750\t avg time: 0.171\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -288.539\tavg length: 79.050\t avg time: 0.135\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -224.474\tavg length: 67.310\t avg time: 0.112\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -155.011\tavg length: 66.240\t avg time: 0.110\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -175.234\tavg length: 67.210\t avg time: 0.112\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -158.691\tavg length: 66.090\t avg time: 0.110\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -150.747\tavg length: 67.700\t avg time: 0.113\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.439 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_1000_0.0014_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -224.827\tavg length: 85.810\t avg time: 0.147\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -224.179\tavg length: 90.120\t avg time: 0.156\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -201.178\tavg length: 83.620\t avg time: 0.143\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -209.706\tavg length: 73.450\t avg time: 0.124\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -219.747\tavg length: 68.360\t avg time: 0.113\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -465.601\tavg length: 62.600\t avg time: 0.104\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -454.459\tavg length: 62.370\t avg time: 0.104\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -308.094\tavg length: 87.620\t avg time: 0.151\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -180.150\tavg length: 81.700\t avg time: 0.138\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -210.872\tavg length: 69.550\t avg time: 0.116\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -155.875\tavg length: 77.500\t avg time: 0.130\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -245.690\tavg length: 77.330\t avg time: 0.130\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -226.290\tavg length: 91.500\t avg time: 0.155\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -201.982\tavg length: 90.010\t avg time: 0.153\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.428 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_1000_0.0014_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -260.526\tavg length: 83.180\t avg time: 0.143\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -226.499\tavg length: 103.140\t avg time: 0.199\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -206.470\tavg length: 90.360\t avg time: 0.156\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -216.232\tavg length: 100.400\t avg time: 0.175\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -209.499\tavg length: 107.800\t avg time: 0.197\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -168.452\tavg length: 95.960\t avg time: 0.167\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -161.536\tavg length: 108.170\t avg time: 0.189\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -130.011\tavg length: 106.330\t avg time: 0.185\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -149.042\tavg length: 122.670\t avg time: 0.217\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -139.387\tavg length: 132.840\t avg time: 0.240\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -163.999\tavg length: 165.510\t avg time: 0.328\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -121.194\tavg length: 181.900\t avg time: 0.366\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -127.597\tavg length: 196.690\t avg time: 0.394\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -143.788\tavg length: 189.770\t avg time: 0.377\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 6.437 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_1000_0.001_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -239.772\tavg length: 83.770\t avg time: 0.146\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -193.540\tavg length: 85.360\t avg time: 0.147\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -181.397\tavg length: 90.750\t avg time: 0.158\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -203.803\tavg length: 103.190\t avg time: 0.189\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -307.282\tavg length: 179.460\t avg time: 0.374\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -242.246\tavg length: 401.240\t avg time: 1.218\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -194.516\tavg length: 375.770\t avg time: 1.112\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -126.598\tavg length: 336.260\t avg time: 0.990\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -165.158\tavg length: 351.760\t avg time: 1.037\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -148.083\tavg length: 360.140\t avg time: 1.133\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -7.868\tavg length: 261.390\t avg time: 0.733\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -51.395\tavg length: 387.670\t avg time: 1.331\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: 10.508\tavg length: 224.610\t avg time: 0.517\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 0.357\tavg length: 209.320\t avg time: 0.483\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 16.588 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_1000_0.001_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -269.001\tavg length: 83.720\t avg time: 0.147\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -207.265\tavg length: 87.300\t avg time: 0.152\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -191.738\tavg length: 83.080\t avg time: 0.144\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -172.507\tavg length: 78.380\t avg time: 0.135\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -186.516\tavg length: 93.180\t avg time: 0.162\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -171.450\tavg length: 90.470\t avg time: 0.156\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -182.273\tavg length: 86.160\t avg time: 0.148\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -368.548\tavg length: 181.250\t avg time: 0.369\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -504.463\tavg length: 216.390\t avg time: 0.445\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -86.797\tavg length: 139.720\t avg time: 0.266\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -146.183\tavg length: 184.490\t avg time: 0.384\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -40.440\tavg length: 219.900\t avg time: 0.500\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -41.656\tavg length: 287.880\t avg time: 0.749\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -35.108\tavg length: 237.120\t avg time: 0.578\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 9.940 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_1000_0.001_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -237.465\tavg length: 84.890\t avg time: 0.150\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -234.403\tavg length: 89.900\t avg time: 0.159\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -219.830\tavg length: 86.120\t avg time: 0.151\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -212.909\tavg length: 91.600\t avg time: 0.161\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -196.104\tavg length: 89.720\t avg time: 0.157\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -215.161\tavg length: 86.470\t avg time: 0.150\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -188.545\tavg length: 92.390\t avg time: 0.161\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -224.762\tavg length: 90.020\t avg time: 0.158\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -218.171\tavg length: 99.530\t avg time: 0.177\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -208.599\tavg length: 109.770\t avg time: 0.207\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -201.827\tavg length: 128.100\t avg time: 0.245\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -173.275\tavg length: 111.970\t avg time: 0.199\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -169.118\tavg length: 102.000\t avg time: 0.179\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -161.966\tavg length: 106.800\t avg time: 0.188\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.398 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_1000_0.0006_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -243.247\tavg length: 95.500\t avg time: 0.167\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -263.894\tavg length: 90.020\t avg time: 0.157\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -246.026\tavg length: 92.780\t avg time: 0.162\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -189.414\tavg length: 97.440\t avg time: 0.171\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -167.860\tavg length: 89.210\t avg time: 0.155\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -159.803\tavg length: 102.140\t avg time: 0.180\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -124.788\tavg length: 137.370\t avg time: 0.255\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -141.766\tavg length: 198.570\t avg time: 0.453\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -177.110\tavg length: 259.750\t avg time: 0.607\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -62.701\tavg length: 189.350\t avg time: 0.375\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -199.065\tavg length: 276.280\t avg time: 0.658\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -83.667\tavg length: 221.480\t avg time: 0.487\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -24.498\tavg length: 382.700\t avg time: 1.048\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -9.525\tavg length: 445.410\t avg time: 1.319\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 13.582 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_1000_0.0006_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -263.825\tavg length: 86.720\t avg time: 0.154\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -226.416\tavg length: 92.390\t avg time: 0.180\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -210.437\tavg length: 83.050\t avg time: 0.147\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -216.430\tavg length: 83.520\t avg time: 0.145\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -193.806\tavg length: 83.670\t avg time: 0.145\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -182.652\tavg length: 84.820\t avg time: 0.146\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -162.353\tavg length: 81.660\t avg time: 0.140\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -191.764\tavg length: 82.220\t avg time: 0.140\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -199.652\tavg length: 82.480\t avg time: 0.139\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -160.509\tavg length: 80.310\t avg time: 0.136\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -179.903\tavg length: 84.120\t avg time: 0.143\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -156.749\tavg length: 86.000\t avg time: 0.146\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -141.750\tavg length: 82.080\t avg time: 0.140\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -168.672\tavg length: 83.210\t avg time: 0.143\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.661 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_1000_0.0006_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -254.236\tavg length: 94.130\t avg time: 0.164\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -229.713\tavg length: 90.610\t avg time: 0.157\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -211.979\tavg length: 92.040\t avg time: 0.160\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -196.509\tavg length: 88.660\t avg time: 0.153\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -202.606\tavg length: 90.280\t avg time: 0.156\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -204.324\tavg length: 99.290\t avg time: 0.189\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -196.096\tavg length: 91.600\t avg time: 0.158\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -190.191\tavg length: 86.860\t avg time: 0.149\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -187.852\tavg length: 99.760\t avg time: 0.188\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -181.187\tavg length: 91.080\t avg time: 0.160\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -170.246\tavg length: 88.340\t avg time: 0.153\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -173.071\tavg length: 90.730\t avg time: 0.156\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -168.128\tavg length: 90.310\t avg time: 0.155\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -157.612\tavg length: 96.920\t avg time: 0.167\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.052 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_250_0.22_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -795.713\tavg length: 84.290\t avg time: 0.154\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -828.073\tavg length: 87.190\t avg time: 0.159\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -815.873\tavg length: 87.330\t avg time: 0.158\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -788.012\tavg length: 81.590\t avg time: 0.147\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -799.395\tavg length: 86.680\t avg time: 0.156\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -814.532\tavg length: 85.350\t avg time: 0.154\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -812.895\tavg length: 85.740\t avg time: 0.155\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -796.566\tavg length: 83.370\t avg time: 0.151\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -816.174\tavg length: 84.560\t avg time: 0.153\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -787.654\tavg length: 82.020\t avg time: 0.148\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -812.185\tavg length: 84.410\t avg time: 0.153\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -794.064\tavg length: 82.260\t avg time: 0.148\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -810.741\tavg length: 83.650\t avg time: 0.151\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -791.926\tavg length: 86.390\t avg time: 0.156\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.832 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_250_0.22_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -545.800\tavg length: 61.990\t avg time: 0.102\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -588.394\tavg length: 63.350\t avg time: 0.105\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -550.750\tavg length: 62.040\t avg time: 0.103\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -565.027\tavg length: 63.040\t avg time: 0.105\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -563.435\tavg length: 62.920\t avg time: 0.104\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -585.758\tavg length: 63.210\t avg time: 0.105\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -565.259\tavg length: 63.270\t avg time: 0.105\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -564.079\tavg length: 63.760\t avg time: 0.106\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -571.436\tavg length: 63.100\t avg time: 0.105\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -568.107\tavg length: 63.770\t avg time: 0.106\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -582.263\tavg length: 63.790\t avg time: 0.106\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -564.200\tavg length: 62.890\t avg time: 0.105\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -546.947\tavg length: 61.950\t avg time: 0.103\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -559.761\tavg length: 62.040\t avg time: 0.103\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.615 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_250_0.22_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -944.409\tavg length: 79.270\t avg time: 0.142\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -931.085\tavg length: 77.550\t avg time: 0.139\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -906.991\tavg length: 74.210\t avg time: 0.133\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -921.050\tavg length: 77.560\t avg time: 0.139\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -924.701\tavg length: 77.000\t avg time: 0.138\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -933.887\tavg length: 77.010\t avg time: 0.138\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -915.521\tavg length: 78.020\t avg time: 0.140\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -948.878\tavg length: 77.900\t avg time: 0.140\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -910.583\tavg length: 75.650\t avg time: 0.136\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -948.406\tavg length: 77.630\t avg time: 0.139\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -942.570\tavg length: 77.060\t avg time: 0.138\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -899.682\tavg length: 75.960\t avg time: 0.137\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -926.901\tavg length: 77.110\t avg time: 0.139\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -905.874\tavg length: 75.440\t avg time: 0.135\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.447 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_250_0.0018_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -544.315\tavg length: 90.660\t avg time: 0.160\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -702.256\tavg length: 130.940\t avg time: 0.233\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -553.351\tavg length: 132.710\t avg time: 0.239\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -524.802\tavg length: 134.820\t avg time: 0.257\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -486.005\tavg length: 150.670\t avg time: 0.296\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -614.646\tavg length: 196.930\t avg time: 0.423\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -371.486\tavg length: 147.690\t avg time: 0.295\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -464.112\tavg length: 166.040\t avg time: 0.383\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -591.009\tavg length: 81.110\t avg time: 0.149\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -502.815\tavg length: 84.670\t avg time: 0.155\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -443.021\tavg length: 85.780\t avg time: 0.158\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -407.142\tavg length: 91.290\t avg time: 0.169\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -395.626\tavg length: 170.090\t avg time: 0.390\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -513.803\tavg length: 164.810\t avg time: 0.478\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 6.925 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_250_0.0018_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -222.011\tavg length: 79.500\t avg time: 0.167\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -218.501\tavg length: 76.350\t avg time: 0.128\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -436.473\tavg length: 67.230\t avg time: 0.111\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -534.783\tavg length: 60.760\t avg time: 0.100\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -572.590\tavg length: 62.930\t avg time: 0.104\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -578.451\tavg length: 64.040\t avg time: 0.105\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -579.429\tavg length: 64.090\t avg time: 0.106\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -581.084\tavg length: 64.280\t avg time: 0.106\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -589.041\tavg length: 65.150\t avg time: 0.107\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -573.544\tavg length: 63.890\t avg time: 0.105\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -577.630\tavg length: 63.050\t avg time: 0.105\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -567.018\tavg length: 62.630\t avg time: 0.104\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -558.512\tavg length: 63.170\t avg time: 0.105\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -555.711\tavg length: 62.280\t avg time: 0.104\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.771 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_250_0.0018_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -366.891\tavg length: 86.690\t avg time: 0.151\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -256.405\tavg length: 93.870\t avg time: 0.165\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -210.314\tavg length: 89.240\t avg time: 0.155\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -173.504\tavg length: 109.590\t avg time: 0.198\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -181.438\tavg length: 127.660\t avg time: 0.242\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -147.881\tavg length: 137.850\t avg time: 0.258\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -138.192\tavg length: 147.160\t avg time: 0.286\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -113.314\tavg length: 206.130\t avg time: 0.491\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -85.432\tavg length: 229.240\t avg time: 0.552\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -158.848\tavg length: 180.760\t avg time: 0.397\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -212.654\tavg length: 259.510\t avg time: 0.676\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -235.458\tavg length: 153.280\t avg time: 0.338\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -87.542\tavg length: 185.510\t avg time: 0.410\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -70.681\tavg length: 172.770\t avg time: 0.354\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 8.766 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_250_0.0014_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -249.934\tavg length: 91.680\t avg time: 0.161\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -239.907\tavg length: 84.700\t avg time: 0.148\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -180.270\tavg length: 95.790\t avg time: 0.167\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -162.765\tavg length: 99.510\t avg time: 0.175\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -154.845\tavg length: 105.180\t avg time: 0.185\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -138.832\tavg length: 106.510\t avg time: 0.189\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -143.706\tavg length: 124.300\t avg time: 0.226\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -181.114\tavg length: 106.880\t avg time: 0.189\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -204.301\tavg length: 122.260\t avg time: 0.230\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -105.473\tavg length: 120.890\t avg time: 0.217\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -72.732\tavg length: 183.320\t avg time: 0.403\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: 1.894\tavg length: 489.520\t avg time: 1.622\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -11.567\tavg length: 753.690\t avg time: 2.704\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 18.706\tavg length: 796.190\t avg time: 2.756\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 17.546 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_250_0.0014_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -287.321\tavg length: 88.030\t avg time: 0.153\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -266.712\tavg length: 106.440\t avg time: 0.187\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -201.278\tavg length: 109.620\t avg time: 0.194\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -166.300\tavg length: 109.930\t avg time: 0.193\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -160.516\tavg length: 113.240\t avg time: 0.199\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -139.179\tavg length: 129.760\t avg time: 0.232\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -137.946\tavg length: 145.670\t avg time: 0.284\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -116.827\tavg length: 160.920\t avg time: 0.305\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -161.963\tavg length: 239.160\t avg time: 0.562\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -209.530\tavg length: 229.450\t avg time: 0.498\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -131.673\tavg length: 188.760\t avg time: 0.397\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -186.262\tavg length: 176.310\t avg time: 0.387\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -42.243\tavg length: 439.320\t avg time: 1.359\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -26.580\tavg length: 508.290\t avg time: 1.652\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 16.031 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_250_0.0014_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -275.699\tavg length: 88.440\t avg time: 0.156\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -236.318\tavg length: 88.340\t avg time: 0.154\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -220.763\tavg length: 85.000\t avg time: 0.147\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -202.090\tavg length: 88.700\t avg time: 0.153\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -200.990\tavg length: 95.980\t avg time: 0.165\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -185.498\tavg length: 96.300\t avg time: 0.166\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -152.173\tavg length: 103.780\t avg time: 0.181\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -159.090\tavg length: 128.490\t avg time: 0.233\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -127.594\tavg length: 138.800\t avg time: 0.253\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -111.438\tavg length: 150.910\t avg time: 0.280\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -73.019\tavg length: 149.290\t avg time: 0.274\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -66.289\tavg length: 169.010\t avg time: 0.353\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -45.626\tavg length: 223.970\t avg time: 0.489\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -35.767\tavg length: 196.640\t avg time: 0.384\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 6.294 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_250_0.001_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -271.763\tavg length: 106.010\t avg time: 0.201\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -205.817\tavg length: 90.890\t avg time: 0.159\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -191.684\tavg length: 82.760\t avg time: 0.142\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -176.181\tavg length: 86.590\t avg time: 0.150\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -144.906\tavg length: 84.000\t avg time: 0.145\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -160.828\tavg length: 104.730\t avg time: 0.186\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -116.362\tavg length: 196.130\t avg time: 0.444\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -157.238\tavg length: 616.500\t avg time: 1.963\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -50.355\tavg length: 761.900\t avg time: 2.625\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: 5.585\tavg length: 794.330\t avg time: 2.859\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: 39.691\tavg length: 589.420\t avg time: 1.874\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: 66.555\tavg length: 304.290\t avg time: 0.705\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: 117.107\tavg length: 355.420\t avg time: 0.815\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 85.280\tavg length: 425.960\t avg time: 1.057\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 23.521 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_250_0.001_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -228.072\tavg length: 86.080\t avg time: 0.151\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -213.513\tavg length: 90.010\t avg time: 0.157\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -212.775\tavg length: 91.510\t avg time: 0.160\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -223.638\tavg length: 106.470\t avg time: 0.187\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -186.490\tavg length: 124.050\t avg time: 0.222\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -167.346\tavg length: 122.730\t avg time: 0.220\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -173.338\tavg length: 187.400\t avg time: 0.410\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -221.704\tavg length: 224.570\t avg time: 0.494\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -164.380\tavg length: 279.910\t avg time: 0.689\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -86.943\tavg length: 295.250\t avg time: 0.725\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -159.036\tavg length: 190.770\t avg time: 0.379\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -95.227\tavg length: 213.980\t avg time: 0.438\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -206.504\tavg length: 211.170\t avg time: 0.459\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -306.563\tavg length: 220.330\t avg time: 0.469\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 9.508 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_250_0.001_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -256.986\tavg length: 90.140\t avg time: 0.157\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -227.666\tavg length: 90.330\t avg time: 0.157\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -227.348\tavg length: 91.940\t avg time: 0.159\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -202.326\tavg length: 90.780\t avg time: 0.156\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -191.982\tavg length: 90.240\t avg time: 0.155\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -191.354\tavg length: 87.370\t avg time: 0.149\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -196.425\tavg length: 88.610\t avg time: 0.152\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -189.249\tavg length: 94.960\t avg time: 0.163\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -169.248\tavg length: 94.690\t avg time: 0.163\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -189.671\tavg length: 99.040\t avg time: 0.172\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -173.419\tavg length: 93.760\t avg time: 0.162\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -189.978\tavg length: 99.400\t avg time: 0.173\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -185.994\tavg length: 106.930\t avg time: 0.188\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -194.020\tavg length: 116.730\t avg time: 0.207\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.203 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_250_0.0006_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -246.770\tavg length: 91.350\t avg time: 0.160\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -224.413\tavg length: 89.010\t avg time: 0.155\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -204.734\tavg length: 85.960\t avg time: 0.149\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -196.196\tavg length: 91.810\t avg time: 0.160\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -180.229\tavg length: 90.770\t avg time: 0.159\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -154.077\tavg length: 106.350\t avg time: 0.189\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -139.541\tavg length: 102.660\t avg time: 0.183\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -116.079\tavg length: 136.070\t avg time: 0.280\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -118.556\tavg length: 135.920\t avg time: 0.248\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -101.901\tavg length: 158.290\t avg time: 0.306\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -53.074\tavg length: 199.990\t avg time: 0.435\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -24.615\tavg length: 228.750\t avg time: 0.518\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -31.993\tavg length: 167.090\t avg time: 0.344\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -12.405\tavg length: 201.520\t avg time: 0.410\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 6.821 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_250_0.0006_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -274.111\tavg length: 85.350\t avg time: 0.147\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -225.570\tavg length: 91.270\t avg time: 0.157\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -201.082\tavg length: 92.900\t avg time: 0.162\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -222.775\tavg length: 92.250\t avg time: 0.160\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -231.736\tavg length: 101.370\t avg time: 0.178\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -217.108\tavg length: 97.780\t avg time: 0.171\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -202.627\tavg length: 98.770\t avg time: 0.173\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -186.031\tavg length: 100.410\t avg time: 0.178\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -165.433\tavg length: 110.680\t avg time: 0.196\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -168.073\tavg length: 120.130\t avg time: 0.215\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -148.846\tavg length: 109.200\t avg time: 0.192\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -161.395\tavg length: 132.220\t avg time: 0.237\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -181.110\tavg length: 129.610\t avg time: 0.232\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -177.197\tavg length: 127.890\t avg time: 0.230\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.857 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.99_250_0.0006_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -259.495\tavg length: 87.540\t avg time: 0.150\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -263.871\tavg length: 91.830\t avg time: 0.158\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -209.897\tavg length: 87.120\t avg time: 0.150\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -210.503\tavg length: 89.030\t avg time: 0.154\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -230.440\tavg length: 90.360\t avg time: 0.156\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -219.843\tavg length: 91.520\t avg time: 0.158\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -202.219\tavg length: 95.460\t avg time: 0.164\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -190.620\tavg length: 91.780\t avg time: 0.158\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -193.759\tavg length: 100.840\t avg time: 0.175\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -202.782\tavg length: 98.100\t avg time: 0.170\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -179.516\tavg length: 98.340\t avg time: 0.170\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -165.732\tavg length: 98.130\t avg time: 0.170\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -187.410\tavg length: 104.840\t avg time: 0.183\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -173.016\tavg length: 105.020\t avg time: 0.184\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.138 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_1000_0.22_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -919.036\tavg length: 76.450\t avg time: 0.138\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -931.886\tavg length: 76.890\t avg time: 0.139\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -922.233\tavg length: 77.210\t avg time: 0.139\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -934.664\tavg length: 76.520\t avg time: 0.138\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -966.289\tavg length: 79.010\t avg time: 0.143\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -956.547\tavg length: 80.030\t avg time: 0.145\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -970.965\tavg length: 80.540\t avg time: 0.146\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -938.113\tavg length: 78.380\t avg time: 0.142\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -963.990\tavg length: 80.620\t avg time: 0.146\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -1004.937\tavg length: 81.280\t avg time: 0.147\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -925.216\tavg length: 74.850\t avg time: 0.136\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -956.715\tavg length: 81.390\t avg time: 0.148\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -922.671\tavg length: 78.310\t avg time: 0.143\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -923.280\tavg length: 78.390\t avg time: 0.142\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.561 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_1000_0.22_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -916.632\tavg length: 84.320\t avg time: 0.152\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -811.545\tavg length: 77.050\t avg time: 0.139\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -888.365\tavg length: 80.070\t avg time: 0.144\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -860.265\tavg length: 78.960\t avg time: 0.142\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -938.784\tavg length: 84.780\t avg time: 0.153\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -825.483\tavg length: 77.570\t avg time: 0.140\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -829.919\tavg length: 77.660\t avg time: 0.140\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -874.612\tavg length: 79.120\t avg time: 0.142\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -875.029\tavg length: 79.040\t avg time: 0.142\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -874.867\tavg length: 78.700\t avg time: 0.141\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -860.602\tavg length: 77.800\t avg time: 0.140\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -886.094\tavg length: 79.330\t avg time: 0.143\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -885.430\tavg length: 79.630\t avg time: 0.143\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -853.875\tavg length: 78.390\t avg time: 0.141\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.574 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_1000_0.22_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -717.470\tavg length: 82.240\t avg time: 0.148\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -719.349\tavg length: 80.300\t avg time: 0.145\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -739.015\tavg length: 81.940\t avg time: 0.147\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -723.940\tavg length: 82.010\t avg time: 0.147\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -747.596\tavg length: 83.840\t avg time: 0.150\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -739.261\tavg length: 81.900\t avg time: 0.147\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -736.191\tavg length: 83.590\t avg time: 0.150\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -716.843\tavg length: 80.140\t avg time: 0.143\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -749.007\tavg length: 85.200\t avg time: 0.153\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -712.874\tavg length: 83.430\t avg time: 0.150\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -717.203\tavg length: 85.260\t avg time: 0.153\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -711.863\tavg length: 79.430\t avg time: 0.142\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -729.843\tavg length: 84.120\t avg time: 0.150\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -727.915\tavg length: 80.340\t avg time: 0.143\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.695 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_1000_0.0018_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -390.212\tavg length: 85.870\t avg time: 0.148\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -148.577\tavg length: 310.000\t avg time: 0.739\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -115.625\tavg length: 707.420\t avg time: 2.216\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -63.720\tavg length: 841.400\t avg time: 2.645\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: 45.510\tavg length: 942.260\t avg time: 2.919\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: 99.992\tavg length: 965.050\t avg time: 2.886\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: 101.190\tavg length: 944.790\t avg time: 2.715\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: 108.104\tavg length: 950.090\t avg time: 2.766\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: 116.342\tavg length: 986.270\t avg time: 2.870\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: 122.109\tavg length: 827.210\t avg time: 2.223\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -129.366\tavg length: 713.920\t avg time: 2.094\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: 17.222\tavg length: 971.310\t avg time: 3.268\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: 117.732\tavg length: 843.790\t avg time: 2.444\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 138.859\tavg length: 793.350\t avg time: 2.238\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 58.309 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_1000_0.0018_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -261.577\tavg length: 89.150\t avg time: 0.158\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -215.013\tavg length: 83.530\t avg time: 0.153\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -178.025\tavg length: 93.830\t avg time: 0.171\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -144.728\tavg length: 129.900\t avg time: 0.241\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -314.410\tavg length: 173.500\t avg time: 0.346\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -142.836\tavg length: 122.760\t avg time: 0.224\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -102.527\tavg length: 260.110\t avg time: 0.583\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -133.855\tavg length: 387.360\t avg time: 1.049\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -107.876\tavg length: 719.810\t avg time: 2.435\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -35.327\tavg length: 812.460\t avg time: 2.933\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -19.147\tavg length: 808.310\t avg time: 2.920\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: 16.778\tavg length: 843.130\t avg time: 2.976\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: 40.054\tavg length: 750.500\t avg time: 2.361\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 80.452\tavg length: 884.240\t avg time: 2.850\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 37.498 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_1000_0.0018_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -270.556\tavg length: 89.380\t avg time: 0.157\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -197.575\tavg length: 86.650\t avg time: 0.156\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -244.434\tavg length: 91.790\t avg time: 0.165\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -193.675\tavg length: 115.440\t avg time: 0.227\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -206.318\tavg length: 137.650\t avg time: 0.255\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -241.156\tavg length: 222.030\t avg time: 0.481\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -164.382\tavg length: 403.330\t avg time: 1.104\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -98.833\tavg length: 730.390\t avg time: 2.407\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -30.603\tavg length: 826.100\t avg time: 2.814\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -61.018\tavg length: 829.060\t avg time: 2.856\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -13.832\tavg length: 771.500\t avg time: 2.723\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: 1.828\tavg length: 848.320\t avg time: 2.924\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: 45.922\tavg length: 808.680\t avg time: 2.575\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 79.378\tavg length: 777.460\t avg time: 2.287\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 39.208 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_1000_0.0014_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -213.271\tavg length: 84.490\t avg time: 0.147\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -172.493\tavg length: 86.610\t avg time: 0.154\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -186.001\tavg length: 103.190\t avg time: 0.187\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -142.976\tavg length: 125.960\t avg time: 0.235\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -241.115\tavg length: 187.400\t avg time: 0.373\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -108.662\tavg length: 209.660\t avg time: 0.442\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -77.467\tavg length: 307.340\t avg time: 0.771\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -31.710\tavg length: 590.500\t avg time: 1.962\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -35.920\tavg length: 728.010\t avg time: 2.522\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -98.521\tavg length: 806.500\t avg time: 2.845\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -54.341\tavg length: 370.490\t avg time: 1.243\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -125.155\tavg length: 835.730\t avg time: 2.682\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: 73.980\tavg length: 939.040\t avg time: 2.998\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 83.297\tavg length: 993.090\t avg time: 3.426\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 38.210 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_1000_0.0014_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -250.542\tavg length: 90.450\t avg time: 0.163\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -210.611\tavg length: 103.560\t avg time: 0.189\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -185.557\tavg length: 165.760\t avg time: 0.335\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -149.102\tavg length: 265.750\t avg time: 0.622\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -158.760\tavg length: 329.680\t avg time: 0.907\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -200.608\tavg length: 571.220\t avg time: 1.908\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -31.512\tavg length: 839.880\t avg time: 3.160\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -26.939\tavg length: 803.820\t avg time: 2.912\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: 18.548\tavg length: 885.840\t avg time: 3.144\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: 70.725\tavg length: 866.570\t avg time: 2.846\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: 83.964\tavg length: 872.160\t avg time: 2.783\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: 64.371\tavg length: 959.300\t avg time: 3.399\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: 95.206\tavg length: 951.050\t avg time: 3.055\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 84.495\tavg length: 985.320\t avg time: 3.322\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 53.150 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_1000_0.0014_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -262.875\tavg length: 94.770\t avg time: 0.177\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -227.554\tavg length: 87.660\t avg time: 0.157\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -195.076\tavg length: 94.400\t avg time: 0.169\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -217.285\tavg length: 123.490\t avg time: 0.229\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -229.784\tavg length: 122.430\t avg time: 0.223\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -202.969\tavg length: 197.850\t avg time: 0.406\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -399.572\tavg length: 204.490\t avg time: 0.430\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -270.280\tavg length: 273.120\t avg time: 0.615\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -167.799\tavg length: 316.980\t avg time: 0.792\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -82.070\tavg length: 401.280\t avg time: 1.172\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -98.393\tavg length: 638.820\t avg time: 2.196\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -42.396\tavg length: 732.050\t avg time: 2.595\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -41.869\tavg length: 849.390\t avg time: 3.286\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -3.518\tavg length: 823.260\t avg time: 3.076\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 30.923 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_1000_0.001_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -230.571\tavg length: 85.360\t avg time: 0.149\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -223.940\tavg length: 91.920\t avg time: 0.165\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -276.207\tavg length: 92.330\t avg time: 0.165\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -248.811\tavg length: 90.260\t avg time: 0.160\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -231.761\tavg length: 97.010\t avg time: 0.171\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -220.159\tavg length: 90.400\t avg time: 0.158\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -166.019\tavg length: 90.340\t avg time: 0.157\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -190.191\tavg length: 98.130\t avg time: 0.171\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -179.851\tavg length: 107.500\t avg time: 0.192\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -160.729\tavg length: 144.000\t avg time: 0.287\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -126.724\tavg length: 177.730\t avg time: 0.354\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -153.793\tavg length: 312.850\t avg time: 0.801\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -143.839\tavg length: 407.050\t avg time: 1.184\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -92.118\tavg length: 518.540\t avg time: 1.700\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 14.062 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_1000_0.001_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -241.951\tavg length: 89.180\t avg time: 0.157\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -210.597\tavg length: 86.610\t avg time: 0.154\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -197.019\tavg length: 88.550\t avg time: 0.156\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -156.188\tavg length: 89.080\t avg time: 0.155\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -152.397\tavg length: 124.340\t avg time: 0.235\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -124.718\tavg length: 119.410\t avg time: 0.215\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -125.922\tavg length: 145.410\t avg time: 0.273\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -108.525\tavg length: 251.940\t avg time: 0.559\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -93.312\tavg length: 339.880\t avg time: 0.914\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -80.172\tavg length: 449.840\t avg time: 1.407\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -93.391\tavg length: 548.210\t avg time: 1.714\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -66.130\tavg length: 682.800\t avg time: 2.292\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -20.391\tavg length: 708.730\t avg time: 2.507\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -57.377\tavg length: 589.220\t avg time: 2.020\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 26.191 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_1000_0.001_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -274.650\tavg length: 86.450\t avg time: 0.151\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -254.304\tavg length: 84.510\t avg time: 0.150\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -207.689\tavg length: 86.940\t avg time: 0.157\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -189.394\tavg length: 85.810\t avg time: 0.150\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -182.401\tavg length: 92.960\t avg time: 0.162\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -167.978\tavg length: 93.800\t avg time: 0.163\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -153.280\tavg length: 106.110\t avg time: 0.197\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -148.446\tavg length: 107.050\t avg time: 0.188\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -144.534\tavg length: 111.830\t avg time: 0.198\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -133.403\tavg length: 134.840\t avg time: 0.246\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -173.061\tavg length: 150.990\t avg time: 0.284\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -149.254\tavg length: 148.800\t avg time: 0.277\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -138.692\tavg length: 176.350\t avg time: 0.344\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -220.059\tavg length: 247.890\t avg time: 0.551\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 6.250 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_1000_0.0006_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -245.410\tavg length: 88.810\t avg time: 0.155\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -221.403\tavg length: 95.670\t avg time: 0.168\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -233.515\tavg length: 113.150\t avg time: 0.205\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -272.712\tavg length: 148.300\t avg time: 0.286\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -378.221\tavg length: 225.190\t avg time: 0.491\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -228.640\tavg length: 198.450\t avg time: 0.440\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -147.259\tavg length: 313.330\t avg time: 0.750\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -177.109\tavg length: 459.720\t avg time: 1.329\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -71.035\tavg length: 622.830\t avg time: 2.044\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -43.688\tavg length: 724.780\t avg time: 2.592\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -36.395\tavg length: 808.010\t avg time: 3.042\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -99.455\tavg length: 675.780\t avg time: 2.309\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -24.833\tavg length: 864.850\t avg time: 3.245\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 8.027\tavg length: 801.010\t avg time: 2.732\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 38.147 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_1000_0.0006_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -259.309\tavg length: 98.540\t avg time: 0.189\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -219.156\tavg length: 97.350\t avg time: 0.174\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -209.487\tavg length: 97.860\t avg time: 0.175\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -193.684\tavg length: 111.890\t avg time: 0.200\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -207.843\tavg length: 122.170\t avg time: 0.227\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -194.490\tavg length: 136.030\t avg time: 0.260\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -224.310\tavg length: 172.590\t avg time: 0.343\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -193.265\tavg length: 198.040\t avg time: 0.420\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -240.056\tavg length: 209.520\t avg time: 0.450\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -184.476\tavg length: 212.990\t avg time: 0.448\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -185.461\tavg length: 252.190\t avg time: 0.589\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -264.518\tavg length: 260.880\t avg time: 0.604\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -293.646\tavg length: 259.090\t avg time: 0.591\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -476.513\tavg length: 290.730\t avg time: 0.706\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 9.895 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_1000_0.0006_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -268.339\tavg length: 89.960\t avg time: 0.156\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -282.076\tavg length: 94.800\t avg time: 0.164\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -257.645\tavg length: 88.960\t avg time: 0.153\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -254.513\tavg length: 88.190\t avg time: 0.152\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -229.377\tavg length: 92.090\t avg time: 0.158\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -231.953\tavg length: 95.170\t avg time: 0.163\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -241.909\tavg length: 91.010\t avg time: 0.156\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -204.247\tavg length: 93.380\t avg time: 0.161\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -190.591\tavg length: 100.530\t avg time: 0.184\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -190.483\tavg length: 96.500\t avg time: 0.169\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -205.535\tavg length: 99.230\t avg time: 0.172\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -197.436\tavg length: 106.530\t avg time: 0.186\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -179.461\tavg length: 116.000\t avg time: 0.223\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -174.357\tavg length: 111.030\t avg time: 0.197\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.361 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_250_0.22_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -553.790\tavg length: 64.470\t avg time: 0.107\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -554.977\tavg length: 64.400\t avg time: 0.107\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -528.023\tavg length: 62.030\t avg time: 0.103\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -532.726\tavg length: 63.030\t avg time: 0.105\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -548.020\tavg length: 64.210\t avg time: 0.107\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -548.424\tavg length: 63.060\t avg time: 0.105\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -513.975\tavg length: 62.360\t avg time: 0.104\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -537.938\tavg length: 63.470\t avg time: 0.106\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -535.293\tavg length: 63.460\t avg time: 0.106\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -508.924\tavg length: 61.780\t avg time: 0.103\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -504.897\tavg length: 60.650\t avg time: 0.101\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -532.718\tavg length: 61.910\t avg time: 0.103\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -549.127\tavg length: 64.680\t avg time: 0.108\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -517.424\tavg length: 60.910\t avg time: 0.102\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 2.622 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_250_0.22_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -776.372\tavg length: 76.600\t avg time: 0.137\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -766.812\tavg length: 78.900\t avg time: 0.142\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -803.700\tavg length: 78.430\t avg time: 0.142\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -797.835\tavg length: 80.420\t avg time: 0.146\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -755.614\tavg length: 77.520\t avg time: 0.140\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -806.252\tavg length: 80.340\t avg time: 0.145\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -810.796\tavg length: 78.160\t avg time: 0.141\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -779.402\tavg length: 80.270\t avg time: 0.145\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -792.938\tavg length: 79.900\t avg time: 0.144\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -756.829\tavg length: 75.780\t avg time: 0.136\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -797.625\tavg length: 80.900\t avg time: 0.146\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -766.817\tavg length: 77.590\t avg time: 0.140\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -789.707\tavg length: 79.690\t avg time: 0.143\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -793.894\tavg length: 81.230\t avg time: 0.146\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.554 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_250_0.22_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -946.290\tavg length: 81.140\t avg time: 0.145\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -996.982\tavg length: 83.300\t avg time: 0.149\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -952.320\tavg length: 80.450\t avg time: 0.144\n",
      "stepped scheduler, new lr: 0.19800\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -917.175\tavg length: 79.580\t avg time: 0.142\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -994.316\tavg length: 83.830\t avg time: 0.150\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -958.369\tavg length: 81.750\t avg time: 0.147\n",
      "stepped scheduler, new lr: 0.17820\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -933.539\tavg length: 79.410\t avg time: 0.142\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -942.915\tavg length: 79.160\t avg time: 0.141\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -914.988\tavg length: 78.260\t avg time: 0.140\n",
      "stepped scheduler, new lr: 0.16038\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -940.239\tavg length: 78.970\t avg time: 0.142\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -914.366\tavg length: 76.800\t avg time: 0.138\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -943.738\tavg length: 80.490\t avg time: 0.145\n",
      "stepped scheduler, new lr: 0.14434\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -959.967\tavg length: 79.890\t avg time: 0.144\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -922.271\tavg length: 77.980\t avg time: 0.140\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 3.595 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_250_0.0018_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -220.341\tavg length: 96.460\t avg time: 0.180\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -179.215\tavg length: 80.450\t avg time: 0.138\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -184.414\tavg length: 87.780\t avg time: 0.152\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -266.608\tavg length: 134.020\t avg time: 0.254\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -99.231\tavg length: 141.450\t avg time: 0.269\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -107.591\tavg length: 336.100\t avg time: 0.980\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: 1.739\tavg length: 494.800\t avg time: 1.674\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -13.380\tavg length: 937.440\t avg time: 3.352\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -18.539\tavg length: 987.030\t avg time: 3.379\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -56.211\tavg length: 966.470\t avg time: 3.242\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -0.296\tavg length: 830.470\t avg time: 2.820\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -25.698\tavg length: 752.100\t avg time: 2.377\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -30.901\tavg length: 124.930\t avg time: 0.232\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 0.480\tavg length: 500.250\t avg time: 2.071\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 42.047 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_250_0.0018_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -285.162\tavg length: 94.400\t avg time: 0.165\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -169.715\tavg length: 84.230\t avg time: 0.145\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -144.753\tavg length: 94.050\t avg time: 0.163\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -156.733\tavg length: 175.800\t avg time: 0.370\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -270.395\tavg length: 298.230\t avg time: 0.774\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -278.719\tavg length: 213.400\t avg time: 0.508\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -343.334\tavg length: 246.200\t avg time: 0.618\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -308.009\tavg length: 134.790\t avg time: 0.254\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -244.578\tavg length: 152.040\t avg time: 0.288\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -221.619\tavg length: 183.910\t avg time: 0.390\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -51.646\tavg length: 547.590\t avg time: 1.996\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -13.354\tavg length: 615.120\t avg time: 2.320\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -293.711\tavg length: 332.220\t avg time: 1.047\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -204.694\tavg length: 89.210\t avg time: 0.153\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 15.624 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_250_0.0018_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -275.762\tavg length: 85.640\t avg time: 0.147\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -315.748\tavg length: 84.470\t avg time: 0.146\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -265.795\tavg length: 89.480\t avg time: 0.157\n",
      "stepped scheduler, new lr: 0.00162\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -259.913\tavg length: 111.480\t avg time: 0.202\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -209.772\tavg length: 126.780\t avg time: 0.234\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -176.772\tavg length: 114.790\t avg time: 0.205\n",
      "stepped scheduler, new lr: 0.00146\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -172.565\tavg length: 134.540\t avg time: 0.263\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -185.264\tavg length: 146.030\t avg time: 0.270\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -245.891\tavg length: 150.630\t avg time: 0.278\n",
      "stepped scheduler, new lr: 0.00131\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -194.758\tavg length: 174.920\t avg time: 0.343\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -126.162\tavg length: 148.540\t avg time: 0.273\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -181.735\tavg length: 154.610\t avg time: 0.290\n",
      "stepped scheduler, new lr: 0.00118\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -178.682\tavg length: 135.030\t avg time: 0.243\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -99.332\tavg length: 201.050\t avg time: 0.400\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 6.553 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_250_0.0014_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -313.742\tavg length: 99.640\t avg time: 0.187\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -164.329\tavg length: 76.290\t avg time: 0.129\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -159.198\tavg length: 91.930\t avg time: 0.158\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -132.329\tavg length: 103.890\t avg time: 0.185\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -128.595\tavg length: 109.120\t avg time: 0.193\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -103.602\tavg length: 136.320\t avg time: 0.249\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -176.148\tavg length: 150.420\t avg time: 0.306\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -97.808\tavg length: 146.650\t avg time: 0.271\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -155.926\tavg length: 131.340\t avg time: 0.258\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -132.842\tavg length: 190.220\t avg time: 0.436\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -288.205\tavg length: 167.510\t avg time: 0.355\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -393.514\tavg length: 139.410\t avg time: 0.275\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -108.155\tavg length: 278.490\t avg time: 0.750\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -214.216\tavg length: 198.730\t avg time: 0.507\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 8.163 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_250_0.0014_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -206.909\tavg length: 92.360\t avg time: 0.160\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -243.767\tavg length: 108.570\t avg time: 0.209\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -215.507\tavg length: 124.540\t avg time: 0.239\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -153.317\tavg length: 144.010\t avg time: 0.263\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -195.346\tavg length: 178.260\t avg time: 0.351\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -175.529\tavg length: 206.870\t avg time: 0.440\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -116.669\tavg length: 270.610\t avg time: 0.705\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -197.489\tavg length: 214.460\t avg time: 0.538\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -175.848\tavg length: 308.110\t avg time: 0.861\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -98.208\tavg length: 403.510\t avg time: 1.273\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -87.796\tavg length: 710.760\t avg time: 2.428\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -23.762\tavg length: 676.950\t avg time: 2.456\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: 63.472\tavg length: 648.880\t avg time: 1.852\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: 43.111\tavg length: 647.680\t avg time: 1.981\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 26.458 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_250_0.0014_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -258.245\tavg length: 92.970\t avg time: 0.161\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -234.788\tavg length: 98.300\t avg time: 0.171\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -211.201\tavg length: 98.640\t avg time: 0.170\n",
      "stepped scheduler, new lr: 0.00126\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -243.402\tavg length: 132.750\t avg time: 0.238\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -187.623\tavg length: 112.920\t avg time: 0.197\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -164.364\tavg length: 111.420\t avg time: 0.197\n",
      "stepped scheduler, new lr: 0.00113\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -196.814\tavg length: 131.230\t avg time: 0.239\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -133.103\tavg length: 143.500\t avg time: 0.266\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -197.346\tavg length: 148.940\t avg time: 0.282\n",
      "stepped scheduler, new lr: 0.00102\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -126.214\tavg length: 134.930\t avg time: 0.245\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -89.502\tavg length: 173.250\t avg time: 0.353\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -80.036\tavg length: 159.540\t avg time: 0.300\n",
      "stepped scheduler, new lr: 0.00092\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -86.703\tavg length: 193.750\t avg time: 0.417\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -173.271\tavg length: 218.460\t avg time: 0.481\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 7.097 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_250_0.001_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -219.890\tavg length: 85.870\t avg time: 0.148\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -186.841\tavg length: 93.170\t avg time: 0.161\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -225.379\tavg length: 107.660\t avg time: 0.188\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -157.300\tavg length: 121.110\t avg time: 0.215\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -263.132\tavg length: 175.230\t avg time: 0.364\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -203.070\tavg length: 107.470\t avg time: 0.194\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -165.672\tavg length: 175.750\t avg time: 0.396\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -207.396\tavg length: 299.390\t avg time: 0.889\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -61.687\tavg length: 223.840\t avg time: 0.559\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -29.299\tavg length: 151.340\t avg time: 0.294\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -4.187\tavg length: 219.800\t avg time: 0.522\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -95.472\tavg length: 413.240\t avg time: 1.467\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -89.459\tavg length: 590.150\t avg time: 2.427\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -70.594\tavg length: 637.900\t avg time: 2.714\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -226.197\tavg length: 84.550\t avg time: 0.144\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -214.638\tavg length: 86.960\t avg time: 0.148\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -198.431\tavg length: 87.310\t avg time: 0.148\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -186.326\tavg length: 86.500\t avg time: 0.145\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -166.417\tavg length: 88.870\t avg time: 0.150\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -164.793\tavg length: 90.660\t avg time: 0.153\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -169.293\tavg length: 107.940\t avg time: 0.186\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -132.343\tavg length: 139.760\t avg time: 0.253\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -149.573\tavg length: 166.600\t avg time: 0.333\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -133.382\tavg length: 181.590\t avg time: 0.361\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -145.555\tavg length: 190.350\t avg time: 0.381\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -109.076\tavg length: 194.320\t avg time: 0.424\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -272.141\tavg length: 137.390\t avg time: 0.255\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -97.589\tavg length: 221.020\t avg time: 0.475\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 6.900 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_250_0.001_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -255.930\tavg length: 87.910\t avg time: 0.151\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -259.829\tavg length: 91.760\t avg time: 0.157\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -254.672\tavg length: 90.240\t avg time: 0.155\n",
      "stepped scheduler, new lr: 0.00090\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -240.380\tavg length: 99.490\t avg time: 0.171\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -210.599\tavg length: 100.920\t avg time: 0.174\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -208.823\tavg length: 111.610\t avg time: 0.225\n",
      "stepped scheduler, new lr: 0.00081\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -194.335\tavg length: 97.500\t avg time: 0.168\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -181.028\tavg length: 97.590\t avg time: 0.169\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -187.273\tavg length: 122.510\t avg time: 0.232\n",
      "stepped scheduler, new lr: 0.00073\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -147.435\tavg length: 127.190\t avg time: 0.243\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -157.518\tavg length: 146.430\t avg time: 0.277\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -264.405\tavg length: 169.720\t avg time: 0.323\n",
      "stepped scheduler, new lr: 0.00066\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -236.463\tavg length: 157.810\t avg time: 0.312\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -156.654\tavg length: 172.190\t avg time: 0.326\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 6.026 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_250_0.0006_512 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -294.792\tavg length: 98.350\t avg time: 0.172\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -248.859\tavg length: 105.850\t avg time: 0.187\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -245.998\tavg length: 131.280\t avg time: 0.239\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -200.349\tavg length: 130.840\t avg time: 0.237\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -297.348\tavg length: 179.680\t avg time: 0.348\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -82.449\tavg length: 135.760\t avg time: 0.246\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -113.236\tavg length: 199.550\t avg time: 0.406\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -413.825\tavg length: 218.250\t avg time: 0.446\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -242.035\tavg length: 179.940\t avg time: 0.345\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -161.392\tavg length: 223.840\t avg time: 0.475\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -85.049\tavg length: 179.590\t avg time: 0.356\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -114.623\tavg length: 178.620\t avg time: 0.335\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -96.476\tavg length: 191.070\t avg time: 0.392\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -94.661\tavg length: 335.150\t avg time: 0.842\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 10.272 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_250_0.0006_256 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -259.331\tavg length: 91.240\t avg time: 0.158\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -226.915\tavg length: 87.210\t avg time: 0.151\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -186.375\tavg length: 84.710\t avg time: 0.146\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -213.118\tavg length: 94.990\t avg time: 0.164\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -208.354\tavg length: 88.990\t avg time: 0.152\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -174.720\tavg length: 97.770\t avg time: 0.168\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -178.827\tavg length: 100.680\t avg time: 0.174\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -186.880\tavg length: 101.650\t avg time: 0.176\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -179.497\tavg length: 111.750\t avg time: 0.195\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -194.147\tavg length: 124.010\t avg time: 0.220\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -206.600\tavg length: 148.300\t avg time: 0.273\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -373.070\tavg length: 172.460\t avg time: 0.323\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -650.856\tavg length: 182.180\t avg time: 0.334\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -272.545\tavg length: 144.920\t avg time: 0.258\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 5.234 minutes -----\n",
      "\n",
      "#################### SplitDiscreteActorCritic_12_0.97_250_0.0006_128 ####################\n",
      "Using CUDA\n",
      "episode: 100, stats for last 100 episodes:\tavg reward: -276.731\tavg length: 85.860\t avg time: 0.148\n",
      "episode: 200, stats for last 100 episodes:\tavg reward: -257.139\tavg length: 87.890\t avg time: 0.152\n",
      "episode: 300, stats for last 100 episodes:\tavg reward: -275.616\tavg length: 95.400\t avg time: 0.166\n",
      "stepped scheduler, new lr: 0.00054\n",
      "episode: 400, stats for last 100 episodes:\tavg reward: -215.360\tavg length: 92.610\t avg time: 0.161\n",
      "episode: 500, stats for last 100 episodes:\tavg reward: -226.532\tavg length: 95.320\t avg time: 0.166\n",
      "episode: 600, stats for last 100 episodes:\tavg reward: -220.253\tavg length: 94.080\t avg time: 0.163\n",
      "stepped scheduler, new lr: 0.00049\n",
      "episode: 700, stats for last 100 episodes:\tavg reward: -229.609\tavg length: 102.790\t avg time: 0.180\n",
      "episode: 800, stats for last 100 episodes:\tavg reward: -215.598\tavg length: 107.240\t avg time: 0.191\n",
      "episode: 900, stats for last 100 episodes:\tavg reward: -224.344\tavg length: 114.910\t avg time: 0.204\n",
      "stepped scheduler, new lr: 0.00044\n",
      "episode: 1000, stats for last 100 episodes:\tavg reward: -235.924\tavg length: 105.180\t avg time: 0.186\n",
      "episode: 1100, stats for last 100 episodes:\tavg reward: -213.081\tavg length: 127.570\t avg time: 0.228\n",
      "episode: 1200, stats for last 100 episodes:\tavg reward: -282.574\tavg length: 148.060\t avg time: 0.271\n",
      "stepped scheduler, new lr: 0.00039\n",
      "episode: 1300, stats for last 100 episodes:\tavg reward: -193.160\tavg length: 123.400\t avg time: 0.221\n",
      "episode: 1400, stats for last 100 episodes:\tavg reward: -210.170\tavg length: 145.550\t avg time: 0.269\n",
      "---------- Finished training ----------\n",
      "Killed env gen process\n",
      "Saved agent to ./model.pkl\n",
      "----- training took 4.965 minutes -----\n",
      "\n",
      "CPU times: user 2d 1h 58min 40s, sys: 53.1 s, total: 2d 1h 59min 33s\n",
      "Wall time: 2d 1h 58min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "SAVE_PATH = '/tf/kfir/nir/sokoban_rl/hyperparam_tunings.pkl'\n",
    "EPOCHS = 1500\n",
    "TRAJ_LEN = 1000\n",
    "BETA = 1e-3\n",
    "PRINT_INTERVAL = 100\n",
    "LOG_INTERVAL = 0\n",
    "SCHED_GAMMA = 0.9\n",
    "SCHED_INTERVAL = 300\n",
    "CONE_TRICK = True\n",
    "\n",
    "discrete_models = ['SplitDiscreteActorCritic']\n",
    "num_discretes = [6, 8, 10, 12]\n",
    "discounts = [0.99, 0.97]\n",
    "traj_lengths = [1000, 250]\n",
    "lrs = [0.22, 0.0018, 0.0014, 0.001, 0.0006]\n",
    "hidden_sizes = [512, 256, 128]\n",
    "discrete_agents = {}\n",
    "\n",
    "for model_name in discrete_models:\n",
    "    for num_discrete in num_discretes:\n",
    "        for discount in discounts:\n",
    "            for traj in traj_lengths:\n",
    "                for lr in lrs:\n",
    "                    for hidden_size in hidden_sizes:\n",
    "                        agent_name = '{}_{}_{}_{}_{}_{}'.format(model_name, num_discrete, discount, traj, lr, hidden_size)\n",
    "                        print('#'*20, agent_name, '#'*20)    \n",
    "                        start = time.time()\n",
    "                        envs = [utils.EnvWrapper('LunarLanderContinuous-v2', utils.ObsType['BOX2D'], \n",
    "                                                utils.ActionType['DISCRETIZIED'], \n",
    "                                                5000, num_discrete=num_discrete,\n",
    "                                                cone_trick=CONE_TRICK) for _ in range(2)]\n",
    "                        env_gen = utils.AsyncEnvGen(envs, 1)\n",
    "                        model = getattr(models, model_name)(envs[0].obs_size, envs[0].num_actions, \n",
    "                                                            hidden_size=hidden_size, num_discrete=num_discrete)\n",
    "                        save_path = './model.pkl'\n",
    "                        log_path = './log.log'\n",
    "                        agent = A2CAgent(model, save_path, log_path)\n",
    "\n",
    "                        try:\n",
    "                            agent.train(epochs=EPOCHS,\n",
    "                                        trajectory_len=traj,\n",
    "                                        env_gen=env_gen,\n",
    "                                        lr=lr,\n",
    "                                        discount_gamma=discount,\n",
    "                                        scheduler_gamma=SCHED_GAMMA,\n",
    "                                        beta=BETA,\n",
    "                                        print_interval=PRINT_INTERVAL,\n",
    "                                        log_interval=LOG_INTERVAL,\n",
    "                                        scheduler_interval=SCHED_INTERVAL)\n",
    "                        except Exception as e:\n",
    "                            raise e\n",
    "                        finally:\n",
    "                            end = time.time()\n",
    "                            utils.kill_process(env_gen)\n",
    "                            discrete_agents[agent_name] = agent\n",
    "                        print('----- training took {:.3f} minutes -----\\n'.format((end - start)/60))\n",
    "\n",
    "with open(SAVE_PATH, 'wb') as f:\n",
    "    pickle.dump(discrete_agents, f)                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(discrete_agents)\n",
    "a = discrete_agents['SplitDiscreteActorCritic_6_0.99_1000_0.0018_256']\n",
    "means = []\n",
    "for agent_name in discrete_agents:\n",
    "    agent = discrete_agents[agent_name]\n",
    "    means.append((agent_name, np.mean(agent.all_rewards[500:1100])))\n",
    "sorted_means = sorted(means, key=lambda tup: tup[-1])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SplitDiscreteActorCritic_12_0.97_1000_0.0018_512', 69.92015922673431),\n",
       " ('SplitDiscreteActorCritic_8_0.97_1000_0.0014_512', 52.23887745118383),\n",
       " ('SplitDiscreteActorCritic_10_0.97_250_0.0018_512', 25.346733632064186),\n",
       " ('SplitDiscreteActorCritic_8_0.99_1000_0.0018_512', 12.310947706782414),\n",
       " ('SplitDiscreteActorCritic_6_0.97_250_0.001_512', 11.743650461927519),\n",
       " ('SplitDiscreteActorCritic_10_0.99_1000_0.001_512', -13.666838961136273),\n",
       " ('SplitDiscreteActorCritic_12_0.97_1000_0.0014_256', -14.407398399187253),\n",
       " ('SplitDiscreteActorCritic_6_0.97_1000_0.001_512', -25.95494292303923),\n",
       " ('SplitDiscreteActorCritic_12_0.97_250_0.0018_512', -32.201290126811216),\n",
       " ('SplitDiscreteActorCritic_8_0.97_1000_0.0018_256', -32.36395076335634),\n",
       " ('SplitDiscreteActorCritic_6_0.99_250_0.0018_512', -32.71136579355764),\n",
       " ('SplitDiscreteActorCritic_8_0.99_250_0.0014_512', -34.1795561887192),\n",
       " ('SplitDiscreteActorCritic_12_0.99_1000_0.0018_256', -36.983812352881735),\n",
       " ('SplitDiscreteActorCritic_10_0.97_1000_0.0018_256', -60.02310980415333),\n",
       " ('SplitDiscreteActorCritic_10_0.97_1000_0.0018_512', -62.31342548293095),\n",
       " ('SplitDiscreteActorCritic_10_0.99_1000_0.0014_512', -65.92429263493638),\n",
       " ('SplitDiscreteActorCritic_12_0.97_1000_0.0014_512', -67.50206866107192),\n",
       " ('SplitDiscreteActorCritic_10_0.99_250_0.0014_512', -68.74451537414402),\n",
       " ('SplitDiscreteActorCritic_6_0.97_1000_0.0018_128', -73.43375411635297),\n",
       " ('SplitDiscreteActorCritic_12_0.99_250_0.001_512', -73.45971155756851),\n",
       " ('SplitDiscreteActorCritic_6_0.97_250_0.0018_256', -78.72132423183126),\n",
       " ('SplitDiscreteActorCritic_6_0.99_1000_0.0014_256', -81.23879120122515),\n",
       " ('SplitDiscreteActorCritic_6_0.97_1000_0.0014_256', -82.65682678248926),\n",
       " ('SplitDiscreteActorCritic_8_0.99_250_0.001_512', -85.60188096604519),\n",
       " ('SplitDiscreteActorCritic_10_0.97_1000_0.0018_128', -89.41477758786795),\n",
       " ('SplitDiscreteActorCritic_12_0.97_1000_0.0018_256', -90.7170900515071),\n",
       " ('SplitDiscreteActorCritic_10_0.99_250_0.001_512', -92.2493333906839),\n",
       " ('SplitDiscreteActorCritic_8_0.97_1000_0.0014_128', -92.52879027501086),\n",
       " ('SplitDiscreteActorCritic_6_0.99_1000_0.0006_512', -93.77165314218718),\n",
       " ('SplitDiscreteActorCritic_8_0.99_1000_0.0006_512', -94.6850899556774),\n",
       " ('SplitDiscreteActorCritic_6_0.99_1000_0.0014_128', -95.62622324625542),\n",
       " ('SplitDiscreteActorCritic_6_0.97_250_0.0014_512', -96.42748953007072),\n",
       " ('SplitDiscreteActorCritic_8_0.99_1000_0.001_512', -99.90271053014007),\n",
       " ('SplitDiscreteActorCritic_12_0.97_1000_0.0018_128', -101.93716965281416),\n",
       " ('SplitDiscreteActorCritic_12_0.97_1000_0.001_256', -104.43593158872481),\n",
       " ('SplitDiscreteActorCritic_10_0.99_1000_0.0006_512', -104.52333153480555),\n",
       " ('SplitDiscreteActorCritic_12_0.97_250_0.001_512', -112.38374894558997),\n",
       " ('SplitDiscreteActorCritic_12_0.99_250_0.0006_512', -114.16609607334142),\n",
       " ('SplitDiscreteActorCritic_6_0.97_250_0.0014_128', -114.89874992880955),\n",
       " ('SplitDiscreteActorCritic_6_0.97_250_0.0018_128', -116.89020073081277),\n",
       " ('SplitDiscreteActorCritic_12_0.97_1000_0.0006_512', -118.0195262410449),\n",
       " ('SplitDiscreteActorCritic_8_0.97_1000_0.0006_512', -120.03085413022532),\n",
       " ('SplitDiscreteActorCritic_8_0.99_1000_0.0018_256', -121.50220950060059),\n",
       " ('SplitDiscreteActorCritic_8_0.97_250_0.0018_256', -122.50074251072319),\n",
       " ('SplitDiscreteActorCritic_10_0.99_1000_0.0014_128', -127.20463852088913),\n",
       " ('SplitDiscreteActorCritic_10_0.97_250_0.001_256', -127.22750151849885),\n",
       " ('SplitDiscreteActorCritic_8_0.97_1000_0.0014_256', -129.08446185666762),\n",
       " ('SplitDiscreteActorCritic_10_0.99_250_0.0018_128', -129.42136885095096),\n",
       " ('SplitDiscreteActorCritic_8_0.99_250_0.0006_512', -130.7601300381125),\n",
       " ('SplitDiscreteActorCritic_10_0.97_250_0.001_512', -131.26479527034255),\n",
       " ('SplitDiscreteActorCritic_6_0.99_250_0.0014_128', -131.42768169944776),\n",
       " ('SplitDiscreteActorCritic_12_0.99_250_0.0014_128', -135.11770993711158),\n",
       " ('SplitDiscreteActorCritic_8_0.97_1000_0.0018_128', -138.22434171992683),\n",
       " ('SplitDiscreteActorCritic_12_0.99_250_0.0014_512', -141.16847925463662),\n",
       " ('SplitDiscreteActorCritic_10_0.99_1000_0.0018_128', -141.57128797610542),\n",
       " ('SplitDiscreteActorCritic_12_0.97_250_0.0014_256', -142.16610604440876),\n",
       " ('SplitDiscreteActorCritic_12_0.99_250_0.0018_128', -143.0804739490116),\n",
       " ('SplitDiscreteActorCritic_12_0.99_1000_0.0006_512', -144.1958238065612),\n",
       " ('SplitDiscreteActorCritic_10_0.97_1000_0.001_512', -144.39425256651842),\n",
       " ('SplitDiscreteActorCritic_8_0.99_1000_0.0014_256', -145.05268772763355),\n",
       " ('SplitDiscreteActorCritic_6_0.99_1000_0.001_512', -146.97950424264042),\n",
       " ('SplitDiscreteActorCritic_6_0.97_250_0.001_128', -147.21224256375908),\n",
       " ('SplitDiscreteActorCritic_10_0.99_250_0.0006_512', -148.183260540459),\n",
       " ('SplitDiscreteActorCritic_12_0.99_1000_0.001_512', -148.19753954960296),\n",
       " ('SplitDiscreteActorCritic_10_0.97_250_0.0018_128', -148.79736325059812),\n",
       " ('SplitDiscreteActorCritic_12_0.97_250_0.001_256', -149.31352215973777),\n",
       " ('SplitDiscreteActorCritic_12_0.99_250_0.0014_256', -149.845134259127),\n",
       " ('SplitDiscreteActorCritic_6_0.99_1000_0.0018_256', -151.1906612919306),\n",
       " ('SplitDiscreteActorCritic_12_0.97_250_0.0014_128', -151.8190373552576),\n",
       " ('SplitDiscreteActorCritic_12_0.99_1000_0.0014_128', -151.8610772001815),\n",
       " ('SplitDiscreteActorCritic_8_0.99_250_0.0018_256', -152.14217706617845),\n",
       " ('SplitDiscreteActorCritic_10_0.97_1000_0.001_256', -153.37270060942126),\n",
       " ('SplitDiscreteActorCritic_12_0.97_1000_0.001_128', -153.4468150415911),\n",
       " ('SplitDiscreteActorCritic_6_0.99_250_0.0018_256', -153.6225740374679),\n",
       " ('SplitDiscreteActorCritic_8_0.99_250_0.0006_256', -154.05784176394513),\n",
       " ('SplitDiscreteActorCritic_6_0.99_1000_0.0006_256', -154.08338356392423),\n",
       " ('SplitDiscreteActorCritic_8_0.97_1000_0.0006_256', -154.28713944148933),\n",
       " ('SplitDiscreteActorCritic_8_0.99_1000_0.0018_128', -156.22178405186122),\n",
       " ('SplitDiscreteActorCritic_10_0.97_250_0.0006_512', -156.24740902069115),\n",
       " ('SplitDiscreteActorCritic_8_0.97_250_0.22_512', -156.93925953806993),\n",
       " ('SplitDiscreteActorCritic_6_0.99_250_0.0014_256', -157.22919354836264),\n",
       " ('SplitDiscreteActorCritic_6_0.97_250_0.001_256', -157.43680957182326),\n",
       " ('SplitDiscreteActorCritic_12_0.97_250_0.0014_512', -158.6766035287882),\n",
       " ('SplitDiscreteActorCritic_6_0.97_1000_0.22_512', -159.22520379663095),\n",
       " ('SplitDiscreteActorCritic_8_0.99_250_0.22_128', -160.2410193471189),\n",
       " ('SplitDiscreteActorCritic_6_0.97_250_0.0006_128', -160.25917813991305),\n",
       " ('SplitDiscreteActorCritic_8_0.99_1000_0.001_128', -160.57506389710264),\n",
       " ('SplitDiscreteActorCritic_12_0.99_250_0.001_256', -162.20492477549365),\n",
       " ('SplitDiscreteActorCritic_10_0.99_1000_0.0014_256', -163.64228126665822),\n",
       " ('SplitDiscreteActorCritic_8_0.99_250_0.0014_256', -163.73664462547285),\n",
       " ('SplitDiscreteActorCritic_8_0.99_250_0.0018_128', -163.78797954364026),\n",
       " ('SplitDiscreteActorCritic_6_0.97_1000_0.0006_128', -164.1372375222154),\n",
       " ('SplitDiscreteActorCritic_8_0.97_250_0.0014_128', -164.3683499241339),\n",
       " ('SplitDiscreteActorCritic_6_0.97_1000_0.001_128', -164.51155586647337),\n",
       " ('SplitDiscreteActorCritic_6_0.99_1000_0.0018_128', -165.37513903280842),\n",
       " ('SplitDiscreteActorCritic_10_0.97_250_0.001_128', -165.48521442804844),\n",
       " ('SplitDiscreteActorCritic_10_0.99_250_0.0014_128', -166.17334077722134),\n",
       " ('SplitDiscreteActorCritic_6_0.99_1000_0.0006_128', -166.538921423861),\n",
       " ('SplitDiscreteActorCritic_10_0.99_1000_0.0006_256', -166.8250426857763),\n",
       " ('SplitDiscreteActorCritic_8_0.97_250_0.0006_128', -167.4619865083859),\n",
       " ('SplitDiscreteActorCritic_10_0.99_1000_0.001_128', -167.69810017146818),\n",
       " ('SplitDiscreteActorCritic_8_0.97_1000_0.0006_128', -169.85497725624612),\n",
       " ('SplitDiscreteActorCritic_8_0.99_250_0.001_128', -170.50636518652226),\n",
       " ('SplitDiscreteActorCritic_6_0.99_250_0.0006_256', -171.92088850244397),\n",
       " ('SplitDiscreteActorCritic_6_0.99_250_0.0018_128', -172.34510909139868),\n",
       " ('SplitDiscreteActorCritic_10_0.97_250_0.0006_256', -172.49874096827028),\n",
       " ('SplitDiscreteActorCritic_10_0.99_250_0.001_128', -172.75306285558938),\n",
       " ('SplitDiscreteActorCritic_6_0.97_1000_0.0014_128', -172.92716893224903),\n",
       " ('SplitDiscreteActorCritic_6_0.99_1000_0.0018_512', -172.99700781923366),\n",
       " ('SplitDiscreteActorCritic_6_0.99_1000_0.001_128', -173.31991707852566),\n",
       " ('SplitDiscreteActorCritic_12_0.97_1000_0.001_512', -174.17792208552706),\n",
       " ('SplitDiscreteActorCritic_10_0.97_250_0.0014_256', -174.22005954433348),\n",
       " ('SplitDiscreteActorCritic_10_0.99_250_0.0018_512', -175.34423341403584),\n",
       " ('SplitDiscreteActorCritic_8_0.97_250_0.001_256', -176.22414844166076),\n",
       " ('SplitDiscreteActorCritic_8_0.97_250_0.0006_256', -176.3656113838615),\n",
       " ('SplitDiscreteActorCritic_6_0.99_250_0.0006_128', -177.17947941497857),\n",
       " ('SplitDiscreteActorCritic_10_0.99_250_0.0018_256', -177.70393160228238),\n",
       " ('SplitDiscreteActorCritic_6_0.99_250_0.001_128', -177.99790031012643),\n",
       " ('SplitDiscreteActorCritic_8_0.99_250_0.0006_128', -178.04189220224777),\n",
       " ('SplitDiscreteActorCritic_8_0.99_1000_0.0006_128', -178.69085175903257),\n",
       " ('SplitDiscreteActorCritic_10_0.97_1000_0.0014_256', -178.8156339779062),\n",
       " ('SplitDiscreteActorCritic_12_0.97_250_0.001_128', -179.12476356549274),\n",
       " ('SplitDiscreteActorCritic_12_0.99_1000_0.0006_256', -179.50803612760222),\n",
       " ('SplitDiscreteActorCritic_12_0.99_1000_0.0018_128', -179.73274322150866),\n",
       " ('SplitDiscreteActorCritic_8_0.97_250_0.0014_512', -180.69911509198354),\n",
       " ('SplitDiscreteActorCritic_12_0.99_250_0.0006_256', -181.10850453264928),\n",
       " ('SplitDiscreteActorCritic_6_0.99_250_0.001_512', -181.81863299847623),\n",
       " ('SplitDiscreteActorCritic_12_0.97_250_0.0006_512', -182.74153249137555),\n",
       " ('SplitDiscreteActorCritic_10_0.97_1000_0.0014_128', -182.85678217869858),\n",
       " ('SplitDiscreteActorCritic_12_0.97_250_0.0018_128', -183.76682521339777),\n",
       " ('SplitDiscreteActorCritic_12_0.99_250_0.001_128', -184.55315574674034),\n",
       " ('SplitDiscreteActorCritic_10_0.97_1000_0.0006_256', -186.20274652507072),\n",
       " ('SplitDiscreteActorCritic_12_0.97_250_0.0006_256', -187.11311713365822),\n",
       " ('SplitDiscreteActorCritic_12_0.99_1000_0.0006_128', -188.7134827255568),\n",
       " ('SplitDiscreteActorCritic_10_0.99_250_0.001_256', -190.01405429392358),\n",
       " ('SplitDiscreteActorCritic_10_0.97_250_0.0006_128', -191.6110893773327),\n",
       " ('SplitDiscreteActorCritic_8_0.99_1000_0.001_256', -192.1407219186885),\n",
       " ('SplitDiscreteActorCritic_8_0.99_1000_0.0006_256', -197.01584337063977),\n",
       " ('SplitDiscreteActorCritic_12_0.99_250_0.0006_128', -198.22683374449127),\n",
       " ('SplitDiscreteActorCritic_10_0.99_1000_0.0006_128', -200.71093461586483),\n",
       " ('SplitDiscreteActorCritic_8_0.99_250_0.0014_128', -200.93686173425448),\n",
       " ('SplitDiscreteActorCritic_10_0.99_250_0.0006_128', -202.13796711138195),\n",
       " ('SplitDiscreteActorCritic_8_0.97_1000_0.001_128', -202.6225776387039),\n",
       " ('SplitDiscreteActorCritic_12_0.97_1000_0.0014_128', -203.90542762900233),\n",
       " ('SplitDiscreteActorCritic_12_0.97_1000_0.0006_256', -204.0470892895496),\n",
       " ('SplitDiscreteActorCritic_10_0.99_1000_0.001_256', -205.90506849257724),\n",
       " ('SplitDiscreteActorCritic_6_0.97_1000_0.001_256', -207.90515510924223),\n",
       " ('SplitDiscreteActorCritic_10_0.97_1000_0.0006_128', -208.79833976829445),\n",
       " ('SplitDiscreteActorCritic_8_0.97_250_0.0018_128', -208.9253230391687),\n",
       " ('SplitDiscreteActorCritic_12_0.99_1000_0.001_128', -209.53544174523705),\n",
       " ('SplitDiscreteActorCritic_12_0.97_1000_0.0006_128', -211.16740723898602),\n",
       " ('SplitDiscreteActorCritic_8_0.97_250_0.0014_256', -212.77295326745528),\n",
       " ('SplitDiscreteActorCritic_6_0.99_1000_0.001_256', -215.05656015625507),\n",
       " ('SplitDiscreteActorCritic_10_0.99_250_0.0014_256', -217.30662451573068),\n",
       " ('SplitDiscreteActorCritic_6_0.97_1000_0.0006_256', -217.72609434894835),\n",
       " ('SplitDiscreteActorCritic_8_0.97_1000_0.001_512', -219.4708862236045),\n",
       " ('SplitDiscreteActorCritic_12_0.97_250_0.0006_128', -223.00812968023737),\n",
       " ('SplitDiscreteActorCritic_6_0.97_250_0.0014_256', -223.52765774032207),\n",
       " ('SplitDiscreteActorCritic_8_0.97_250_0.001_128', -224.61753843498852),\n",
       " ('SplitDiscreteActorCritic_8_0.99_1000_0.0014_128', -227.6578130086863),\n",
       " ('SplitDiscreteActorCritic_8_0.97_1000_0.001_256', -228.52324050985024),\n",
       " ('SplitDiscreteActorCritic_10_0.97_1000_0.001_128', -229.15663877434892),\n",
       " ('SplitDiscreteActorCritic_10_0.97_250_0.0014_128', -232.99324240850373),\n",
       " ('SplitDiscreteActorCritic_12_0.97_250_0.0018_256', -242.06422711953132),\n",
       " ('SplitDiscreteActorCritic_12_0.99_1000_0.001_256', -243.6129602934665),\n",
       " ('SplitDiscreteActorCritic_8_0.99_250_0.001_256', -245.74771421645022),\n",
       " ('SplitDiscreteActorCritic_10_0.99_250_0.0006_256', -252.57783036701818),\n",
       " ('SplitDiscreteActorCritic_10_0.97_1000_0.0006_512', -262.42402420555874),\n",
       " ('SplitDiscreteActorCritic_6_0.97_250_0.0006_512', -262.8119590568304),\n",
       " ('SplitDiscreteActorCritic_6_0.99_250_0.0006_512', -263.56243961004003),\n",
       " ('SplitDiscreteActorCritic_8_0.97_250_0.001_512', -264.16307802232865),\n",
       " ('SplitDiscreteActorCritic_8_0.97_250_0.0018_512', -279.15998519263934),\n",
       " ('SplitDiscreteActorCritic_8_0.97_250_0.0006_512', -285.8910286987996),\n",
       " ('SplitDiscreteActorCritic_6_0.99_250_0.001_256', -286.9622857352553),\n",
       " ('SplitDiscreteActorCritic_12_0.99_1000_0.0014_256', -296.58156362870204),\n",
       " ('SplitDiscreteActorCritic_10_0.97_250_0.0018_256', -299.27814355546417),\n",
       " ('SplitDiscreteActorCritic_6_0.99_1000_0.0014_512', -302.0614017568513),\n",
       " ('SplitDiscreteActorCritic_10_0.97_1000_0.0014_512', -348.2790591052664),\n",
       " ('SplitDiscreteActorCritic_6_0.97_250_0.0006_256', -353.15528830885177),\n",
       " ('SplitDiscreteActorCritic_8_0.97_1000_0.0018_512', -353.82495366755154),\n",
       " ('SplitDiscreteActorCritic_6_0.97_1000_0.0006_512', -380.5251394382521),\n",
       " ('SplitDiscreteActorCritic_8_0.97_1000_0.22_512', -392.09335407576776),\n",
       " ('SplitDiscreteActorCritic_12_0.99_1000_0.0018_512', -403.71710616434035),\n",
       " ('SplitDiscreteActorCritic_6_0.97_250_0.0018_512', -406.50440457902124),\n",
       " ('SplitDiscreteActorCritic_6_0.97_1000_0.0014_512', -416.21152998371934),\n",
       " ('SplitDiscreteActorCritic_12_0.99_1000_0.0014_512', -425.8283894843221),\n",
       " ('SplitDiscreteActorCritic_12_0.99_1000_0.22_256', -431.93153741898885),\n",
       " ('SplitDiscreteActorCritic_6_0.99_1000_0.22_512', -433.9054695421774),\n",
       " ('SplitDiscreteActorCritic_6_0.99_1000_0.22_256', -435.60915799329996),\n",
       " ('SplitDiscreteActorCritic_6_0.99_250_0.0014_512', -437.5862692981266),\n",
       " ('SplitDiscreteActorCritic_8_0.99_1000_0.0014_512', -444.94998389453315),\n",
       " ('SplitDiscreteActorCritic_10_0.99_1000_0.0018_256', -454.0501409389463),\n",
       " ('SplitDiscreteActorCritic_10_0.99_1000_0.0018_512', -454.9223911634529),\n",
       " ('SplitDiscreteActorCritic_8_0.99_250_0.22_512', -463.9553591907092),\n",
       " ('SplitDiscreteActorCritic_8_0.97_250_0.22_256', -468.87184219051),\n",
       " ('SplitDiscreteActorCritic_8_0.99_250_0.0018_512', -471.24666065129327),\n",
       " ('SplitDiscreteActorCritic_10_0.97_1000_0.22_512', -486.37618551507813),\n",
       " ('SplitDiscreteActorCritic_10_0.97_250_0.22_128', -486.49456093792145),\n",
       " ('SplitDiscreteActorCritic_6_0.99_1000_0.22_128', -494.2581363026557),\n",
       " ('SplitDiscreteActorCritic_12_0.99_250_0.0018_512', -497.5012327954268),\n",
       " ('SplitDiscreteActorCritic_6_0.99_250_0.22_128', -501.03054133941185),\n",
       " ('SplitDiscreteActorCritic_6_0.99_250_0.22_512', -507.00285558620595),\n",
       " ('SplitDiscreteActorCritic_6_0.97_1000_0.0018_256', -507.2634908870978),\n",
       " ('SplitDiscreteActorCritic_8_0.97_1000_0.22_256', -517.0235055773763),\n",
       " ('SplitDiscreteActorCritic_12_0.97_250_0.22_512', -524.6792466435327),\n",
       " ('SplitDiscreteActorCritic_8_0.99_1000_0.22_128', -524.8644316839759),\n",
       " ('SplitDiscreteActorCritic_10_0.99_1000_0.22_256', -537.4870805431224),\n",
       " ('SplitDiscreteActorCritic_12_0.99_1000_0.22_512', -538.2042983136924),\n",
       " ('SplitDiscreteActorCritic_6_0.97_1000_0.22_256', -564.4703090935055),\n",
       " ('SplitDiscreteActorCritic_6_0.99_250_0.22_256', -564.6616198516762),\n",
       " ('SplitDiscreteActorCritic_8_0.99_1000_0.22_512', -571.7278960178263),\n",
       " ('SplitDiscreteActorCritic_12_0.99_250_0.22_256', -572.3772928971022),\n",
       " ('SplitDiscreteActorCritic_12_0.99_250_0.0018_256', -580.3031667305279),\n",
       " ('SplitDiscreteActorCritic_10_0.97_1000_0.22_256', -600.9019806838144),\n",
       " ('SplitDiscreteActorCritic_10_0.97_250_0.22_256', -719.8473737662111),\n",
       " ('SplitDiscreteActorCritic_12_0.97_1000_0.22_128', -727.7890500538557),\n",
       " ('SplitDiscreteActorCritic_10_0.97_1000_0.22_128', -763.9997031888645),\n",
       " ('SplitDiscreteActorCritic_10_0.99_250_0.22_128', -779.1453973472729),\n",
       " ('SplitDiscreteActorCritic_10_0.97_250_0.22_512', -784.4208298941121),\n",
       " ('SplitDiscreteActorCritic_12_0.97_250_0.22_256', -790.7158170114784),\n",
       " ('SplitDiscreteActorCritic_12_0.99_250_0.22_512', -806.5502045831315),\n",
       " ('SplitDiscreteActorCritic_10_0.99_1000_0.22_512', -823.8445835829942),\n",
       " ('SplitDiscreteActorCritic_8_0.99_250_0.22_256', -831.741389906129),\n",
       " ('SplitDiscreteActorCritic_10_0.97_250_0.0014_512', -846.5113036403825),\n",
       " ('SplitDiscreteActorCritic_10_0.99_250_0.22_256', -847.3175549176285),\n",
       " ('SplitDiscreteActorCritic_10_0.99_1000_0.22_128', -850.1299987914149),\n",
       " ('SplitDiscreteActorCritic_12_0.97_1000_0.22_256', -856.9603780291569),\n",
       " ('SplitDiscreteActorCritic_6_0.97_250_0.22_256', -865.4781309761922),\n",
       " ('SplitDiscreteActorCritic_6_0.97_250_0.22_512', -869.0927482268936),\n",
       " ('SplitDiscreteActorCritic_6_0.97_1000_0.0018_512', -879.0465137335924),\n",
       " ('SplitDiscreteActorCritic_8_0.99_1000_0.22_256', -883.9118929509939),\n",
       " ('SplitDiscreteActorCritic_12_0.99_1000_0.22_128', -884.29381846024),\n",
       " ('SplitDiscreteActorCritic_6_0.97_250_0.22_128', -884.9900246681896),\n",
       " ('SplitDiscreteActorCritic_8_0.97_250_0.22_128', -891.4182844034963),\n",
       " ('SplitDiscreteActorCritic_10_0.99_250_0.22_512', -906.1430733492667),\n",
       " ('SplitDiscreteActorCritic_12_0.97_250_0.22_128', -933.5635440883271),\n",
       " ('SplitDiscreteActorCritic_12_0.99_250_0.22_128', -933.5868326946352),\n",
       " ('SplitDiscreteActorCritic_12_0.97_1000_0.22_512', -959.0594660481001),\n",
       " ('SplitDiscreteActorCritic_8_0.97_1000_0.22_128', -962.5080456003213),\n",
       " ('SplitDiscreteActorCritic_6_0.97_1000_0.22_128', -996.3754239276707)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# TURN OFF CONE-TRICK AT EPOCH 700\n",
    "# try higher penalty in cone trick (-400)\n",
    "# test gaussians and convs\n",
    "# use clip_grad for gaussians!\n",
    "# Finally train the Sokoban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7731,
     "status": "ok",
     "timestamp": 1628250349998,
     "user": {
      "displayName": "Nir Weingarten",
      "photoUrl": "",
      "userId": "08827275300476553234"
     },
     "user_tz": -180
    },
    "id": "cc1nYlV5R_qL"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/RL_project/saved_agents/test_agent.pkl', 'rb') as f:\n",
    "    discrete_agents = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 412,
     "status": "ok",
     "timestamp": 1628250711222,
     "user": {
      "displayName": "Nir Weingarten",
      "photoUrl": "",
      "userId": "08827275300476553234"
     },
     "user_tz": -180
    },
    "id": "hJZRDyFuSFqK"
   },
   "outputs": [],
   "source": [
    "def moving_average(numbers, window_size):\n",
    "    i = 0\n",
    "    moving_averages = []\n",
    "    while i < len(numbers) - window_size + 1:\n",
    "        this_window = numbers[i : i + window_size]\n",
    "        window_average = sum(this_window) / window_size\n",
    "        moving_averages.append(window_average)\n",
    "        i += 1\n",
    "    return moving_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1628251701286,
     "user": {
      "displayName": "Nir Weingarten",
      "photoUrl": "",
      "userId": "08827275300476553234"
     },
     "user_tz": -180
    },
    "id": "5J2jrRwTUrRF"
   },
   "outputs": [],
   "source": [
    "keys = [key for key in discrete_agents.keys()]\n",
    "COLORS = ['b', 'g', 'c', 'm', 'y', 'k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3331,
     "status": "ok",
     "timestamp": 1628251742892,
     "user": {
      "displayName": "Nir Weingarten",
      "photoUrl": "",
      "userId": "08827275300476553234"
     },
     "user_tz": -180
    },
    "id": "W3CAvVI_SdwY",
    "outputId": "39a8f826-328a-47b0-81b4-49abf7af81ca"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1,  figsize=(10,25))\n",
    "for num, key in enumerate(discrete_agents):\n",
    "    axes[num//4].plot(moving_average(discrete_agents[key].all_rewards, 100), color=COLORS[num%4])\n",
    "[axes[i].legend(keys[q:q+4]) for i, q in enumerate(range(0,len(keys), 4))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1628175727558,
     "user": {
      "displayName": "Nir Weingarten",
      "photoUrl": "",
      "userId": "08827275300476553234"
     },
     "user_tz": -180
    },
    "id": "ZJZQoUFJ1BZK"
   },
   "outputs": [],
   "source": [
    "# env = gym.make('CartPole-v0')\n",
    "env.env.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_S6emoRviPYW"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "TRAJ_LEN = 300\n",
    "SCHED_GAMMA = 0.98\n",
    "BETA = 1e-3\n",
    "PRINT_INTERVAL = 500\n",
    "LOG_INTERVAL = 0\n",
    "SCHED_INTERVAL = 1000\n",
    "\n",
    "gaussian_models = ['GaussianActorCritic', 'GaussianConvActorCritic']\n",
    "lrs = [1e-3, 3e-4, 1e-5]\n",
    "num_discretes = [100, 500, 10]\n",
    "discounts = [0.8,0.9,0.99]\n",
    "traj_lengths = [5,10,50,200,500]\n",
    "\n",
    "discrete_agents = {}\n",
    "\n",
    "for model_name in discrete_models:\n",
    "    for num_discrete in num_discretes:\n",
    "        for discount in discounts:\n",
    "            for traj in traj_lengths:\n",
    "                for lr in lrs:\n",
    "                    agent_name = '{}_{}_{}_{}_{}'.format(model_name, num_discrete, discount, traj, lr)\n",
    "                    print('#'*20, agent_name, '#'*20)    \n",
    "                    start = time.time()\n",
    "                    envs = [utils.EnvWrapper('LunarLanderContinuous-v2', utils.ObsType['BOX2D'], \n",
    "                                            utils.ActionType['DISCRETIZIED'], \n",
    "                                            5000, num_discrete=num_discrete) for _ in range(2)]\n",
    "                    env_gen = utils.AsyncEnvGen(envs, 1)\n",
    "                    model = getattr(models, model_name)(envs[0].obs_size, envs[0].num_actions, num_discrete=num_discrete)\n",
    "                    save_path = './model.pkl'\n",
    "                    log_path = './log.log'\n",
    "                    agent = A2CAgent(model, save_path, log_path)\n",
    "\n",
    "                    try:\n",
    "                        agent.train(EPOCHS, traj, env_gen, lr,\n",
    "                                            discount, SCHED_GAMMA, BETA,\n",
    "                                            PRINT_INTERVAL, LOG_INTERVAL, SCHED_INTERVAL)\n",
    "                    finally:\n",
    "                        end = time.time()\n",
    "                        utils.kill_process(env_gen)\n",
    "                        gaussian_models[agent_name] = agent\n",
    "                    print('----- training took {:.3f} minutes -----\\n'.format((end - start)/60))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOFixL5don+TutAyupHIZt3",
   "collapsed_sections": [],
   "mount_file_id": "1_L_aTBQTbRKLoLP9nOJcGM5qORKDQVVI",
   "name": "lunar_lander_tune_hyperparams_discrete.ipynb",
   "provenance": [
    {
     "file_id": "1bZwRsmDmMELA7Xbss_QQTxi0lh_600s5",
     "timestamp": 1628164431471
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
